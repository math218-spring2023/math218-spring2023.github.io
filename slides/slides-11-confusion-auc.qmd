---
title: "Untitled"
format: revealjs
editor: visual
editor_options: 
  chunk_output_type: console
---

# Housekeeping

```{r}
#| message: false
library(tidyverse)
library(vegan)
data(mite)
data(mite.env)
mite_dat_cat <- mite.env %>%
  add_column(abundance = mite$LRUG) %>%
  mutate(present = ifelse(abundance > 0, 1, 0)) %>%
  select(-abundance)
```

# Model assessment

## Model assessment in classification

-   In the case of binary response, it is common to create a **confusion matrix**, from which we can obtain the misclassification rate and other rates of interest

```{r fig.align = "center", fig.width=6, fig.height=6}
knitr::include_graphics("figs/confusion_matrix.png")
```

-   `FP` = "false positive", `FN` = "false negative", `TP` = "true positive, `TN` ="true negative

    -   "Success" class is the same as "positive"

-   Can calculate the overall **error/misclassification rate**: the proportion of observations that we misclassified

    -   Misclassification rate = $\frac{\text{FP} + \text{FN}}{\text{TP} + \text{FP} + \text{FN} + \text{TN}} = \frac{\text{FP} + \text{FN}}{n}$

## Example

-   10 observations, with true class and predicted class

```{r}
true <- c(0, 1, 1, 0, 1, 1, 0, 0, 1, 1)
pred <- c(0, 1, 0, 1, 1, 1, 0, 0, 1, 0)
data.frame( pred = pred, true = true) %>%
  knitr::kable()
```

-   Make a confusion matrix!

## Mite data

I fit a logistic regression to predict heart `present` using predictors `Topo` and `WatrCont`, and obtained the following confusion matrix of the train data:

```{r}
mite_log <- glm(present ~ Topo + WatrCont, data = mite_dat_cat,
                family = "binomial")

mite_preds <- predict(mite_log, type = "response")
mite_df <- data.frame(true = mite_dat_cat$present, prob = mite_preds) %>%
  mutate(pred = ifelse(prob > 0.5, 1, 0))
n <- nrow(mite_dat_cat); n0 <- sum(mite_dat_cat$present==0); n1 <- n - n0

mite_df%>%
  dplyr::select(pred, true) %>%
  table() %>%
  knitr::kable() %>%
  kableExtra::add_header_above(header = c("Pred" =1, "True" = 2)) 

fpr <- round(mite_df %>% filter(true == 0, pred==1) %>% nrow() / n0 , 3)
fnr <- round(mite_df %>% filter(true == 1, pred==0) %>% nrow() / n1 , 3)
```

-   What is the misclassification rate?

    -   Misclassification rate: (`r mite_df %>% filter(true == 1, pred==0) %>% nrow()` + `r mite_df %>% filter(true == 0, pred==1) %>% nrow()`)/`r n` = `r round((mite_df %>% filter(true == 1, pred==0) %>% nrow() + mite_df %>% filter(true == 0, pred==1) %>% nrow())/ n,3)`

## Types of errors

-   **False positive rate** (FPR): fraction of negative observations incorrectly classified as positive

    -   Number of failures/negatives in data = $\text{FP} + \text{TN}$

    -   $\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}$

-   **False negative rate** (FNR): fraction of positive observations incorrectly classified as negative example

    -   Number of success/positives in data = $\text{FN} + \text{TP}$

    -   $\text{FNR} = \frac{\text{FN}}{\text{FN} + \text{TP}}$

## Mite data errors

```{r}
mite_df%>%
  dplyr::select(pred, true) %>%
  table() %>%
  knitr::kable() %>%
  kableExtra::add_header_above(header = c("Pred" =1, "True" = 2)) 
```

-   What is the FNR? What is the FPR?

    -   FNR: `r mite_df %>% filter(true == 1, pred==0) %>% nrow()`/`r n1` = `r fpr`

    -   FPR: `r mite_df %>% filter(true == 0, pred==1) %>% nrow()`/`r n0` = `r fnr`

## Threshold

-   Is a false positive or a false negative worse? Depends on the context!

-   The previous confusion matrix was produced by classifying an observation as present if $\hat{p}(x)=\widehat{\text{Pr}}(\text{present = 1} | \text{Topo, WatrCont}) \geq 0.5$

    -   Here, 0.5 is the *threshold* for assigning an observation to the "present" class

    -   Can change threshold to any value in $[0,1]$, which will affect error rates

## Varying threshold

```{r lda_threshold, fig.align="center", fig.width=8, fig.height=5}
th <- seq(0, 1, 0.025)
TT <- length(th)
err_mat <- matrix(NA, nrow = TT, ncol = 3)
for(t in 1:length(th)){
  fpr <- mite_df %>%
    mutate(pred = ifelse(prob >= th[t], 1, 0 )) %>%
    filter(true == 0, pred == 1)  %>%
    nrow() / n0
  fnr <- mite_df %>%
    mutate(pred = ifelse(prob >= th[t], 1, 0 )) %>%
    filter(true == 1, pred == 0)  %>%
    nrow() / n1
  
  oe <- mite_df %>%
    mutate(pred = ifelse(prob >= th[t], 1, 0 )) %>%
    filter(true != pred)  %>%
    nrow() / n
  err_mat[t,] <- c(fpr, fnr, oe)
}

as.data.frame(err_mat) %>%
  add_column(threshold = th) %>%
  rename("False positive" = 1, "False negative" = 2, "Misclassification\n error" = 3) %>%
  pivot_longer(cols = 1:3, names_to = "type", values_to = "error_rate") %>%
  ggplot(., aes(x = threshold, y = error_rate, col = type))+
  geom_path() +
  labs(y = "Error rate")+
  theme(legend.title = element_blank(), text = element_text(size = 20))
```

-   Overall error rate minimized at threshold near 0.50

-   How to decide a threshold rate? Is there a way to obtain a "threshold-free" version of model performance?

## ROC Curve

-   The **ROC curve** is a measure of performance for binary classification at various thresholds

    -   ROC is a probability curve, and the **Area under the curve (AUC)** tells us how good the model is at distinguishing between/separating the two classes

-   The ROC curve simultaneously plots the true positive rate TPR on the y-axis against the false positive rate FPR on the x-axis

-   An excellent model has AUC near 1 (i.e. near perfect separability), whereas a terrible model has AUC near 0 (always predicts the wrong class)

    -   When AUC = 0.5, the model has no ability to separate classes

```{r fig.align="center", fig.height=4, fig.width=4}
library(pROC)
mite_roc <- roc(response = mite_df$true, predictor= as.numeric(mite_df$pred)-1, plot = F)
auc <- round(mite_roc$auc, 3)
ggroc(mite_roc, legacy.axes = T)+
  ggtitle(paste0('ROC Curve ', '(AUC = ', auc, ')')) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(x = "FPR (1 - specificity)", y = "TPR (sensitivity)")
```
