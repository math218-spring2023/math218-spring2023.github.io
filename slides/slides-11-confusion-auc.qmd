---
title: "Model Assessment and KNN Classification"
date: "April 6, 2023"
title-slide-attributes:
    data-background-image: "figs/title/palmer_penguins.png"
    data-background-opacity: "0.3"
format: 
  revealjs:
    theme: custom.scss
    transition: none
    incremental: true
editor: visual
editor_options: 
  chunk_output_type: console
draft: false
---

# Housekeeping

-   We will have an implementation assignment this week (most likely begin in class tomorrow)

-   Still need to hear about project partners!

::: footer
[PalmerPenguins](https://allisonhorst.github.io/palmerpenguins/articles/intro.html)
:::

```{r}
#| message: false
library(tidyverse)
library(vegan)
data(mite)
data(mite.env)
presence_dat <- mite.env %>%
  add_column(abundance = mite$LRUG) %>%
  mutate(present = ifelse(abundance > 0, 1, 0)) %>%
  select(-abundance)
source("../math218_fns.R")
plot_theme <- theme(text = element_text(size = 16))

```

# Model assessment

## Model assessment in classification {.scrollable}

In the case of binary response, it is common to create a **confusion matrix**, from which we can obtain the misclassification rate and other rates of interest

```{r fig.align = "center", fig.width=4, fig.height=4}
knitr::include_graphics("figs/confusion_matrix.png")
```

-   `FP` = "false positive", `FN` = "false negative", `TP` = "true positive, `TN` ="true negative

    -   "Success" class is the same as "positive" is the same as "1"

-   Can calculate the overall **error/misclassification rate**: the proportion of observations that we misclassified

    -   Misclassification rate = $\frac{\text{FP} + \text{FN}}{\text{TP} + \text{FP} + \text{FN} + \text{TN}} = \frac{\text{FP} + \text{FN}}{n}$

## Example

::: columns
::: {.column width="50%"}
```{r}
true <- c(0, 1, 1, 0, 1, 1, 0, 0, 1, 1)
pred <- c(0, 1, 0, 1, 1, 1, 0, 0, 1, 0)
data.frame( pred = pred, true = true) %>%
  knitr::kable()
```
:::

::: {.column width="50%"}
-   10 observations, with true class and predicted class

-   Make a confusion matrix!

::: fragment
```{r}
data.frame( pred = pred, true = true) %>%
  table() %>%
  knitr::kable() %>%
  kableExtra::add_header_above(header = c("Pred" =1, "True" = 2)) 
```
:::
:::
:::

## Types of errors

-   **False positive rate** (FPR): fraction of negative observations incorrectly classified as positive

    -   Number of failures/negatives in data = $\text{FP} + \text{TN}$

    -   $\text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}$

-   **False negative rate** (FNR): fraction of positive observations incorrectly classified as negative example

    -   Number of success/positives in data = $\text{FN} + \text{TP}$

    -   $\text{FNR} = \frac{\text{FN}}{\text{FN} + \text{TP}}$

## Mite data errors

I fit a logistic regression to predict heart `present` using predictors `Topo` and `WatrCont`, and obtained the following confusion matrix of the train data:

```{r echo = T}
mite_log <- glm(present ~ Topo + WatrCont, data = presence_dat,family = "binomial")
```

```{r}
mite_preds <- predict(mite_log, type = "response")
mite_df <- data.frame(true = presence_dat$present, prob = mite_preds) %>%
  mutate(pred = ifelse(prob > 0.5, 1, 0))
n <- nrow(presence_dat); n0 <- sum(presence_dat$present==0); n1 <- n - n0

fpr <- round(mite_df %>% filter(true == 0, pred==1) %>% nrow() / n0 , 3)
fnr <- round(mite_df %>% filter(true == 1, pred==0) %>% nrow() / n1 , 3)
```

```{r}
mite_df%>%
  dplyr::select(pred, true) %>%
  table() %>%
  knitr::kable() %>%
  kableExtra::add_header_above(header = c("Pred" =1, "True" = 2)) 
```

-   What is the misclassification rate?

    -   Misclassification rate: (`r mite_df %>% filter(true == 1, pred==0) %>% nrow()` + `r mite_df %>% filter(true == 0, pred==1) %>% nrow()`)/`r n` = `r round((mite_df %>% filter(true == 1, pred==0) %>% nrow() + mite_df %>% filter(true == 0, pred==1) %>% nrow())/ n,3)`

-   What is the FNR? What is the FPR?

    -   FNR: `r mite_df %>% filter(true == 1, pred==0) %>% nrow()`/`r n1` = `r fpr`

    -   FPR: `r mite_df %>% filter(true == 0, pred==1) %>% nrow()`/`r n0` = `r fnr`

## Threshold

-   Is a false positive or a false negative worse? Depends on the context!

-   The previous confusion matrix was produced by classifying an observation as present if $\hat{p}(x)=\widehat{\text{Pr}}(\text{present = 1} | \text{Topo, WatrCont}) \geq 0.5$

    -   Here, 0.5 is the *threshold* for assigning an observation to the "present" class

    -   Can change threshold to any value in $[0,1]$, which will affect resulting error rates

## Varying threshold

```{r lda_threshold, fig.align="center", fig.width=8, fig.height=5}
th <- seq(0, 1, 0.025)
TT <- length(th)
err_mat <- matrix(NA, nrow = TT, ncol = 3)
for(t in 1:length(th)){
  fpr <- mite_df %>%
    mutate(pred = ifelse(prob >= th[t], 1, 0 )) %>%
    filter(true == 0, pred == 1)  %>%
    nrow() / n0
  fnr <- mite_df %>%
    mutate(pred = ifelse(prob >= th[t], 1, 0 )) %>%
    filter(true == 1, pred == 0)  %>%
    nrow() / n1
  
  oe <- mite_df %>%
    mutate(pred = ifelse(prob >= th[t], 1, 0 )) %>%
    filter(true != pred)  %>%
    nrow() / n
  err_mat[t,] <- c(fpr, fnr, oe)
}

as.data.frame(err_mat) %>%
  add_column(threshold = th) %>%
  rename("False positive" = 1, "False negative" = 2, "Misclassification\n error" = 3) %>%
  pivot_longer(cols = 1:3, names_to = "type", values_to = "error_rate") %>%
  ggplot(., aes(x = threshold, y = error_rate, col = type))+
  geom_path() +
  labs(y = "Error rate")+
  theme(legend.title = element_blank(), text = element_text(size = 20))
```

-   Overall error rate minimized at threshold near 0.50

-   How to decide a threshold rate? Is there a way to obtain a "threshold-free" version of model performance?

## ROC Curve

-   The **ROC curve** is a measure of performance for binary classification at various thresholds

    -   ROC is a probability curve, and the **Area under the curve (AUC)** tells us how good the model is at distinguishing between/separating the two classes

-   The ROC curve simultaneously plots the true positive rate TPR on the y-axis against the false positive rate FPR on the x-axis

-   An excellent model has AUC near 1 (i.e. near perfect separability), whereas a terrible model has AUC near 0 (always predicts the wrong class)

    -   When AUC = 0.5, the model has no ability to separate classes

## ROC Curve (cont.)

ROC and AUC for the training data:

```{r fig.align="center", fig.height=7, fig.width=7}
library(pROC)
mite_roc <- roc(response = mite_df$true, predictor= as.numeric(mite_df$pred)-1, plot = F)
auc <- round(mite_roc$auc, 3)
ggroc(mite_roc, legacy.axes = T)+
  ggtitle(paste0('ROC Curve ', '(AUC = ', auc, ')')) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  labs(x = "FPR (1 - specificity)", y = "TPR (sensitivity)")
```

-   How do you think we did?

# KNN Classification

## KNN Classification

-   While logistic regression is nice (and still frequently used), the basic model only accommodates binary responses. What if we have a response with more than two classes?

-   We will now see how we can use KNN for classification

    -   *Side note*: when people refer to KNN, they almost always refer to KNN classification

-   Finding the neighbor sets proceeds exactly the same as in KNN regression! The difference lies in how we predict $\hat{y}$

## Example: mite data {.scrollable}

-   This is a similar plot to that from KNN regression slides, where now points (plotted in standardized predictor space) are colored by `present` status instead of `abundance`.

-   Two test points, which we'd like to classify using KNN with $K = 3$

```{r}
set.seed(6)
n <- nrow(presence_dat)
test_ids <- sample(1:n, 2)
presence_dat <- presence_dat %>%
  mutate(present = factor(present))
train_x <- presence_dat[-test_ids, c("SubsDens", "WatrCont")]
train_y <-  presence_dat$present[-test_ids]
test_x <- presence_dat[test_ids,  c("SubsDens", "WatrCont")]
test_y <-  presence_dat$present[test_ids]
K <- 3

my_scale <- function(df, mean_vec, sd_vec){
  ret <- df
  for(i in 1:ncol(df)){
    ret[,i] <- (df[,i] - mean_vec[i])/sd_vec[i]
  }
  return(ret)
}
train_means <- colMeans(train_x)
train_sds <- apply(train_x, 2, sd)
train_x_scaled <- my_scale(train_x, train_means, train_sds)
test_x_scaled <-  my_scale(test_x, train_means, train_sds)

train_dat <- data.frame(train_x_scaled) %>%
  mutate(present = train_y ) 
test_dat <-  data.frame(test_x_scaled) %>%
  mutate(present = test_y ) 

nbs_ls <- list()
for(i in 1:nrow(test_dat)){
  nbs_ls[[i]] <- data.frame(d = .dist(test_x_scaled[i,], train_x_scaled)) %>%
    mutate(id = row_number()) %>%
    arrange(d) %>%
    slice(1:K) 
}
```

::: r-stack
::: fragment
```{r}
#| fig-width: 7
#| fig-height: 7
p3 <- ggplot()+
  geom_point(data = train_dat, aes(x = SubsDens, y = WatrCont, fill = present), col = "black", size = 3, pch = 21) +
  scale_color_viridis_d( ) +
  plot_theme+
  theme(legend.position = "bottom") +
  ggtitle("Scaled train data")
p3
```
:::

::: fragment
```{r}
#| fig-width: 7
#| fig-height: 7
p3 + 
  geom_text(data = test_dat %>%
               mutate(test_pt = row_number()), 
                      mapping = aes(x = SubsDens, y = WatrCont,label = test_pt), size = 4)+
  labs(subtitle = "and test points")
```
:::
:::

## How would we classify?

::: columns
::: {.column width="60%"}
```{r}
#| fig-width: 7
#| fig-height: 7
p3 + 
  geom_text(data = test_dat %>%
               mutate(test_pt = row_number()), 
                      mapping = aes(x = SubsDens, y = WatrCont,label = test_pt))+
  geom_path(data = circleFun(center = as.numeric(test_dat[1,1:2]),
                             diameter = nbs_ls[[1]][K,1]*2-0.05),
            mapping = aes(x = x, y = y), linetype = "dashed") +
  geom_path(data = circleFun(center = as.numeric(test_dat[2,1:2]),
                             diameter = nbs_ls[[2]][K,1]*2-0.05),
            mapping = aes(x = x, y = y), linetype = "dashed") +
  labs(subtitle= "with neighbor sets for test points")
```
:::

::: {.column width="40%"}
::: {style="color: maroon"}
Discuss: which class labels would you predict for test points 1 and 2, and why?
:::

-   Estimated conditional class probabilities $\hat{p}_{ij}(\mathbf{x}_{i})$ are obtained via simple "majority vote":
    -   $\hat{p}_{1, \text{present}}(\mathbf{x}_{1}) = \frac{3}{3} = 1$ and $\hat{p}_{1, \text{absent}}(\mathbf{x}_{1}) = \frac{0}{3} = 0$
    -   $\hat{p}_{2, \text{present}}(\mathbf{x}_{2}) = \frac{1}{3}$ and $\hat{p}_{2, \text{absent}}(\mathbf{x}_{2}) = \frac{2}{3}$
:::
:::

## Beyond two classes

-   KNN classification is easily extended to more than two classes!

-   We still follow the majority vote approach: simply predict the class that has the highest representation in the neighbor set

-   [`palmerPenguins`](https://allisonhorst.github.io/palmerpenguins/) data: size measurements for adult foraging Ad√©lie, Chinstrap, and Gentoo penguins

-   We will classify penguin species by size measurements!

## KNN classification: different K

```{r}
library(palmerpenguins)
penguins_clean <- penguins %>%
  select(-island, -year) %>%
  na.omit() %>%
  mutate_if(is.numeric, scale)

```

```{r}
test_df <- data.frame(bill_length_mm = 0, bill_depth_mm = 0.3)

p_penguin <- ggplot() +
  geom_point(penguins_clean, mapping = aes(x = bill_length_mm, y = bill_depth_mm, col = species), size = 2.5) +
  geom_point(data = test_df, mapping = aes(x = bill_length_mm, y = bill_depth_mm), pch = 4, size = 4) +
  geom_path(data = circleFun(center = as.numeric(test_df[1,1:2]),
                             diameter = 0.5),
            mapping = aes(x = x, y = y), linetype = "dashed")+
   geom_path(data = circleFun(center = as.numeric(test_df[1,1:2]),
                             diameter = 0.78),
            mapping = aes(x = x, y = y), linetype = "dashed", col = "purple")+
  ggtitle("Neighbor sets for test observation",
          "K = 9 (purple) and K = 5 (black)") +
  plot_theme+
  theme(legend.position = "bottom")

p_zoom <- ggplot() +
  geom_point(penguins_clean, mapping = aes(x = bill_length_mm, y = bill_depth_mm,fill = species), col = "black", pch = 21, size = 6) +
  geom_point(data = test_df, mapping = aes(x = bill_length_mm, y = bill_depth_mm), pch = 4, size = 5) +
  geom_path(data = circleFun(center = as.numeric(test_df[1,1:2]),
                             diameter = 0.5),
            mapping = aes(x = x, y = y), linetype = "dashed")+
   geom_path(data = circleFun(center = as.numeric(test_df[1,1:2]),
                             diameter = 0.78),
            mapping = aes(x = x, y = y), linetype = "dashed", col = "purple") +
  xlim(c(-0.4, 0.4)) +
  ylim(c(-0.1, 0.7)) +
  ggtitle("Neighbor sets for test observation (zoomed)",
          "K = 9 (purple) and K = 5 (black)") +
  plot_theme +
  theme(legend.position = "bottom")
```

::: columns
::: {.column width="50%"}
```{r}
#| fig-width: 7
#| fig-height: 7
p_penguin
```
:::

::: {.column width="50%"}
```{r}
#| fig-width: 7
#| fig-height: 7
p_zoom
```
:::
:::

-   What class would you predict for the test point when $K = 9$?
-   What class would you predict for the test point when $K = 5$?

## Handling ties

-   An issue we may encounter in KNN classification is a *tie*

    -   It is possible that no single class is a majority!

-   ::: {style="color: maroon"}
    Discuss: how would you handle ties in the neighbor set?
    :::
