---
title: "Bootstrap"
date: "March 9, 2023"
title-slide-attributes:
    data-background-image: "figs/title/validation.png"
    data-background-size: contain
    data-background-opacity: "0.3"
format: 
  revealjs:
    theme: custom.scss
    transition: none
    incremental: true
editor: visual
editor_options: 
  chunk_output_type: console
draft: false
---

# Housekeeping

-   Second deliverable for KNN regression due to Canvas tonight 11:59pm!

-   No class next Friday

```{r packabundances, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, messabundance = F)

library(tidyverse)
library(broom)
library(ggfortify)
library(ISLR)
library(vegan)
source("../math218_fns.R")
plot_theme <- theme(text = element_text(size = 16))
data(mite)
data(mite.env)
mite_dat <- mite.env %>%
  add_column(abundance = mite$LRUG)

```

::: footer
[Validation short film](https://www.youtube.com/watch?v=Cbk980jV7Ao)
:::

## Resampling

-   Economically use a collected dataset by repeatedly drawing samples from the same training dataset and fitting a model of interest on each sample

    -   Obtain additional information about the fitted model

-   Two methods: cross-validation and the bootstrap

-   These slides will focus on the**bootstrap**


# The Bootstrap

## The Bootstrap

-   The **bootstrap** is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method

-   Example: can be used to estimate the standard errors of the $\beta$ coefficients in linear regression

-   One goal of statistics: learn about a population.

    -   Usually, population is not available, so must make inference from sample data

-   Bootstrapping operates by *resampling* this sample data to create many simulated samples

## The Bootstrap (cont.)

-   Bootstrapping resamples the original dataset **with replacement**

-   If the original datset has $n$ observations, then each bootstrap/resampled dataset also has $n$ observations

    -   Each observation has equal probability of being included in resampled dataset
    -   Can select an observation more than once for a resampled dataset

## Example

-   Suppose a study on adult daily caffeine consumption (mg) collects 4 data points: 110, 130, 150, 200. I want to learn about the average consumption in adults.

-   Create my first bootstrap sample:

```{r echo = T}
dat <- c(110, 130, 150, 200)
n <- length(dat)

samp1 <- sample(x = dat, size = n, replace = T)
samp1
```

-   Obtain our first estimate for $\mu$, the population mean daily caffeine consumption in adults: $\hat{\mu}_{1} = `r mean(samp1)`$

## Example (cont.)

-   Take second sample:

```{r echo = T}
samp2 <- sample(x = dat, size = n, replace = T)
samp2
```

-   $\hat{\mu}_{2} = `r mean(samp2)`$

-   Repeat this process thousands of times!

-   ...

```{r}
B <- 1000
samps <- t(replicate(B,  sample(x = dat, size = n, replace = T)))
mu_ests <- rowMeans(samps)
```

-   After `r B` bootstrap samples, we end up with `r B` estimates for $\mu$

::: fragment
```{r fig.align="center", fig.height=4, fig.width=4}
ggplot(data.frame(mu = mu_ests), aes(x = mu)) +
  geom_histogram(bins = 10)

```
:::

-   Mean over all estimates is $\hat{\mu} = `r mean(mu_ests)`$

-   Approximate 95% confidence interval for the mean are the 5% and 95% quantiles of the `r B` mean estimates: (`r quantile(mu_ests, c(0.025))`, `r quantile(mu_ests, c(0.975))`)

    -   Called a *bootstrap percentile* confidence interval

## Bootstrap: pros and cons

-   Real world vs bootstrap world

-   Pros:

    -   No assumptions about distribution of your data
    -   Very general method that allows estimating sampling distribution of almost any statistic!
    -   Cost-effective

-   Cons:

    -   In more complex scenarios, figuring out appropriate way to bootstrap may require thought
    -   Can fail in some situations
    -   Relies quite heavily on the original sample
