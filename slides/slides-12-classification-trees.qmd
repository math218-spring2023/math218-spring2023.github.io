---
title: "Classification Trees"
date: "April 18, 2023"
title-slide-attributes:
    data-background-image: "figs/title/palmer_penguins.png"
    data-background-opacity: "0.3"
format: 
  revealjs:
    theme: custom.scss
    transition: none
    incremental: true
editor: visual
editor_options: 
  chunk_output_type: console
draft: true
---

# Housekeeping

-   

::: footer
[PalmerPenguins](https://allisonhorst.github.io/palmerpenguins/articles/intro.html)
:::

```{r}
#| message: false
library(tidyverse)
library(palmerpenguins)
data(penguins)
source("../math218_fns.R")
plot_theme <- theme(text = element_text(size = 16))
library(tree)
library(randomForest)

```

# Classification Trees

## Classification trees

-   Interpretation of the tree is the same as for regression tree!

-   Will end up with paths to regions $R_{1}, R_{2}, \ldots, R_{T}$ where $T$ is the number of terminal nodes or leaves

-   What do you think we will predict for $\hat{y}$ now in a classification task?

    -   Recall: in regression trees, the prediction $\hat{y}$ for an observation that falls into a given region $R_{m}$ is the average of the training responses in that $R_{m}$

## Building the tree

-   Just like in regression trees, we will use recursive binary splitting to grow the tree

-   Top-down, greedy approach that makes the "best" split at that moment in time

-   In regression trees, we used residual sum of squares to quantify "best", but we cannot use that here!

-   What might we use instead to quantify "best" to decide each split?

-   Consider the fraction of training observations in the region that belong to the most common class:

$$E = 1 - \max_{k}(\hat{p}_{mk})$$

where $\hat{p}_{mk}$ is the proportion of training observations in the region $R_{m}$ that are from class $k$

-   Convince yourself that smaller $E$ would correspond to a "good" split!

-   Unfortunately, this error is not sufficiently sensitive to tree-growing

## Gini Index

-   The **Gini index** is a measure of the total *variance* across the $K$ classes

$$G_{m} = \sum_{k=1}^{K} \hat{p}_{mk} (1 - \hat{p}_{mk})$$

-   $G_{m}$ is small if all the $\hat{p}_{mk}$'s are close to zero or one

-   For this reason, Gini index is referred to as a measure of node *purity*

-   A small $G_{m}$ indicates that the node contains most of its observations from a single class

## Gini index: example

$$G_{m} = \sum_{k=1}^{K} \hat{p}_{mk} (1 - \hat{p}_{mk})$$

-   Example: 3 classes and 9 observations in three regions

```{r}
data.frame(Region1 = c(rep("A", 9), rep("B", 0), rep("C", 0)),
           Region2 = c(rep("A", 2), rep("B", 2), rep("C", 5)),
           Region3 = c(rep("A", 3), rep("B", 3), rep("C", 3))) %>%
  print.data.frame()
```

-   In these three regions, what are the Gini indices $G_{1}, G_{2}, G_{3}$?

    -   $G_{1} = 1(1-1) + 0(1-0) + 0(1-0) = 0$
    -   $G_{2} = \frac{2}{9}(1-\frac{2}{9}) +\frac{2}{9}(1-\frac{2}{9}) + \frac{5}{9}(1-\frac{5}{9}) \approx 0.60$
    -   $G_{3} = \frac{1}{3}(1-\frac{1}{3}) + \frac{1}{3}(1-\frac{1}{3})+\frac{1}{3}(1-\frac{1}{3}) = \frac{2}{3} \approx 0.67$

## Entropy

-   Alternative to Gini index is **cross-entropy**:

$$D_{m} = -\sum_{k=1}^{K} \hat{p}_{mk} \log\hat{p}_{mk}$$

-   Very similar to Gini index, so cross-entropy is also a measure of node purity

-   Same example: 3 classes and 9 observations in three regions

```{r}
data.frame(Region1 = c(rep("A", 9), rep("B", 0), rep("C", 0)),
           Region2 = c(rep("A", 2), rep("B", 2), rep("C", 5)),
           Region3 = c(rep("A", 3), rep("B", 3), rep("C", 3))) %>%
  print.data.frame()
```

-   In these three regions, what are the cross-entropies $D_{1}, D_{2}, D_{3}$?

    -   $D_{1} = -(1\log(1) + 0\log(0) + 0\log(0)) = 0$
    -   $D_{2} = -(\frac{2}{9}\log(\frac{2}{9}) +\frac{2}{9}\log(\frac{2}{9}) + \frac{5}{9}\log(\frac{5}{9})) \approx 1$
    -   $D_{3} = \frac{1}{3}(1-\frac{1}{3}) + \frac{1}{3}(1-\frac{1}{3})+\frac{1}{3}(1-\frac{1}{3}) = \frac{2}{3} \approx 1.1$

```{r echo = F, eval = F}
set.seed(45) # 45
n <- nrow(penguins)
train_ids <- sample(1:n, 0.7*n)
train_dat <- penguins[train_ids,]
test_dat <- penguins[-train_ids,]

penguins_full <- tree(species ~ bill_length_mm  + bill_depth_mm +
                        flipper_length_mm + sex,
                      data = train_dat,
                      control = tree.control(nobs = nrow(train_dat), minsize = 2))
plot(penguins_full)
text(penguins_full, pretty = 0, cex = 0.5)
title("Full tree")

cv_tree <- cv.tree(penguins_full, FUN = prune.misclass)
data.frame(size = cv_tree$size, dev = cv_tree$dev) %>%
  ggplot(., aes(x = size, y = dev)) +
  geom_point() + 
  geom_line()+
  scale_x_continuous(breaks  = cv_tree$size)
best_size <- min(cv_tree$size[which(cv_tree$dev == min(cv_tree$dev))])
prune_penguins <- prune.misclass(penguins_full, best = best_size)
plot(prune_penguins)
text(prune_penguins, pretty = 0, cex = 1)
title("Pruned tree")

predict(prune_penguins, newdata = test_dat)

predict(prune_penguins, newdata = test_dat, type = "class")
```

```{r eval = F, echo = F}
set.seed(3)
mushrooms <- read.csv("~/Downloads/mushrooms.csv")
mushrooms <- mushrooms %>%
  mutate_if(is.character, factor)
n <- nrow(mushrooms)
train_ids <- sample(1:n, 0.9*n)
train_dat <- mushrooms[train_ids,]
test_dat <- mushrooms[-train_ids,]
mushrooms_tree <- tree(class ~ cap.color + odor + veil.type + habitat +  spore.print.color, data = train_dat,
                       split = "gini",
                       control = tree.control(nobs = nrow(train_dat), minsize = 2))
plot(mushrooms_tree)
text(mushrooms_tree, pretty = 0)

cv_tree <- cv.tree(mushrooms_tree, FUN = prune.misclass)
data.frame(size = cv_tree$size, dev = cv_tree$dev) %>%
  ggplot(., aes(x = size, y = dev)) +
  geom_point() + 
  geom_line()+
  scale_x_continuous(breaks  = cv_tree$size)
best_size <- min(cv_tree$size[which(cv_tree$dev == min(cv_tree$dev))])
prune_mushrooms <- prune.misclass(mushrooms_tree, best = best_size)
tree_preds <- predict(prune_mushrooms, newdata = test_dat, type = "class")

library(randomForest)


mushroom_bag <- randomForest(class ~ cap.color + odor + veil.type + habitat +  spore.print.color, data = mushrooms, ntree = 100,
             mtry = 5)
bag_preds <- mushroom_bag$predicted[-train_ids]

mushroom_rf <- randomForest(class ~ cap.color + odor + veil.type + habitat +  spore.print.color, data = mushrooms, ntree = 100)
rf_preds <- mushroom_rf$predicted[-train_ids]

table(tree_preds, test_dat$class)
table(bag_preds, test_dat$class)
table(rf_preds, test_dat$class)
```

# Example

## Obesity data

We have [data](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6710633/) on obesity levels based on eating habits and physical condition in individuals from Colombia, Peru and Mexico (although some of this data is actually synthetic (i.e. fake)!)

```{r}
obesity <- read.csv("data/obesity.csv")
obesity <- obesity %>%
  mutate_if(is.character, factor)
obesity %>%
  count(class)
```

-   Variables include: gender, age, smoke status, family history, amount of physical activity, etc.

-   Can we predict the obesity status based on these variables using decision trees? What variables might be important for classifying the status?

## Analysis

-   Split data into 80% train, 20% test

-   Fit three different models to the data and obtained predictions for the test data:

    1.  Pruned classification tree fit on the 80% train

    2.  Bagged classification trees fit on all the data, but predictions obtained from OOB samples (B = 100 trees)

    3.  Random forest classification fit on all the data, but predictions obtained from OOB samples (B = 100 trees)

## Classification Tree {.scrollable}

```{r}
set.seed(1)
n <- nrow(obesity)
train_ids <- sample(1:n, 0.8*n)
train_dat <- obesity[train_ids,]
test_dat <- obesity[-train_ids,]
```

```{r echo = T}
#| code-line-numbers: "3|5"
obesity_tree <- tree(class ~ ., data = train_dat,
                       control = tree.control(nobs = nrow(train_dat), minsize = 2))
cv_tree <- cv.tree(obesity_tree, FUN = prune.misclass)
best_size <- min(cv_tree$size[which(cv_tree$dev == min(cv_tree$dev))])
prune_obesity <- prune.misclass(obesity_tree, best = best_size)
```

::: fragment
```{r}
plot(prune_obesity)
text(prune_obesity, pretty = 0, cex = 0.6)
```
:::

## Classification Tree: predictions

```{r echo = T, eval = F}
head(predict(prune_obesity, newdata = test_dat))
```

```{r echo = F, eval = T}
round(head(predict(prune_obesity, newdata = test_dat)), 6)
```

-   What does `predict()` output by default?

::: fragment
```{r echo = T}
head(predict(prune_obesity, newdata = test_dat, type = "class"))
```
:::

-   What's our misclassification rate?

::: fragment
```{r echo = F}
tree_preds <- predict(prune_obesity, newdata = test_dat, type = "class")
print(paste0("Misclass rate: ", round(mean(tree_preds != test_dat$class),3)))
```
:::

## Bagged classification trees

```{r echo = T}
obesity_bag <- randomForest(class ~ ., data = obesity, ntree = 100,
             mtry = ncol(obesity) - 1)
varImpPlot(obesity_bag)
```

## Aggregating predictions

-   How does a bagged model predict a single class across $B$ different trees?

    -   i.e. What does it mean to aggregate multiple labels?

::: fragment
```{r}
bag_preds <- obesity_bag$predicted[-train_ids]
print(paste0("Misclass rate: ", round(mean(bag_preds != test_dat$class),3)))
```
:::

## Random forests

For random forests, we typically set the number of predictors to consider at each split to be $\approx \sqrt{p}$, where $p$ is the total number of available predictors

::: fragment
```{r}
m <- round(sqrt(ncol(obesity)-1))
obesity_rf <- randomForest(class ~ ., data = obesity, ntree = 100,
                           mtry = m)
varImpPlot(obesity_rf)

rf_preds <- obesity_rf$predicted[-train_ids]
print(paste0("Misclass rate: ", round(mean(rf_preds != test_dat$class),3)))
```
:::

## Comparing all models

Which model would we prefer?

```{r}
data.frame(model = c("Pruned tree", "Bagged", "Random Forest"), error_rate = c(mean(tree_preds != test_dat$class),
             mean(bag_preds != test_dat$class),
             mean(rf_preds != test_dat$class)))
```
