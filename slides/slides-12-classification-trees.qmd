---
title: "Classification Trees"
date: "April 18, 2023"
title-slide-attributes:
    data-background-image: "figs/title/gini.png"
    data-background-opacity: "0.3"
format: 
  revealjs:
    theme: custom.scss
    transition: none
    incremental: true
editor: visual
editor_options: 
  chunk_output_type: console
draft: true
---

# Housekeeping

-   Lab 05 due this Thursday!
-   Project proposals due this Sunday to Canvas!

::: footer
[Image from Wikipedia](https://en.wikipedia.org/wiki/Gini_coefficient)
:::

```{r}
#| message: false
library(tidyverse)
library(palmerpenguins)
data(penguins)
source("../math218_fns.R")
plot_theme <- theme(text = element_text(size = 16))
library(tree)
library(randomForest)

```

# Classification Trees

## Classification trees

-   Interpretation of the tree is the same as for regression tree!

-   Will end up with paths to regions $R_{1}, R_{2}, \ldots, R_{T}$ where $T$ is the number of terminal nodes or leaves

-   What do you think we will predict for $\hat{y}$ now in a classification task?

    -   Recall: in regression trees, the prediction $\hat{y}$ for an observation that falls into a given region $R_{m}$ is the average of the training responses in that $R_{m}$

## Building the tree

-   Just like in regression trees, we will use recursive binary splitting to grow the tree

-   Top-down, greedy approach that makes the "best" split at that moment in time

-   In regression trees, we used residual sum of squares to quantify "best", but we cannot use that here!

-   What might we use instead to quantify "best" to decide each split?

-   Consider the fraction of training observations in the region that belong to the most common class: $$E = 1 - \max_{k}(\hat{p}_{mk}),$$ where $\hat{p}_{mk}$ is the proportion of training observations in the region $R_{m}$ that are from class $k$

-   Does smaller or larger $E$ correspond to a "good" split?

-   Unfortunately, this error is not sufficiently sensitive to tree-growing

## Gini Index

::: columns
::: {.column width="50%"}
The **Gini index** is a measure of the total *variance* across the $K$ classes

$$G_{m} = \sum_{k=1}^{K} \hat{p}_{mk} (1 - \hat{p}_{mk})$$

-   $G_{m}$ is small if all the $\hat{p}_{mk}$'s are close to zero or one

-   For this reason, Gini index is referred to as a measure of node *purity*

-   A small $G_{m}$ indicates that the node contains most of its observations from a single class
:::

::: {.column width="50%"}
-   Example: 3 classes and 9 observations in three regions

::: fragment
```{r}
data.frame(Region1 = c(rep("A", 9), rep("B", 0), rep("C", 0)),
           Region2 = c(rep("A", 2), rep("B", 2), rep("C", 5)),
           Region3 = c(rep("A", 3), rep("B", 3), rep("C", 3))) %>%
  print.data.frame()
```
:::

-   In these three regions, what are the Gini indices $G_{1}, G_{2}, G_{3}$?

    -   $G_{1} = 1(1-1) + 0(1-0) + 0(1-0) = 0$
    -   $G_{2} = \frac{2}{9}(1-\frac{2}{9}) +\frac{2}{9}(1-\frac{2}{9}) + \frac{5}{9}(1-\frac{5}{9}) \approx 0.60$
    -   $G_{3} = \frac{1}{3}(1-\frac{1}{3}) + \frac{1}{3}(1-\frac{1}{3})+\frac{1}{3}(1-\frac{1}{3}) = \frac{2}{3} \approx 0.67$
:::
:::

## Entropy

::: columns
::: {.column width="55%"}
Alternative to Gini index is **cross-entropy**:

$$D_{m} = -\sum_{k=1}^{K} \hat{p}_{mk} \log\hat{p}_{mk}$$

-   Very similar to Gini index, so cross-entropy is also a measure of node purity
:::

::: {.column width="45%"}
-   In these three regions, what are the cross-entropies $D_{1}, D_{2}, D_{3}$?

::: fragment
```{r}
data.frame(Region1 = c(rep("A", 9), rep("B", 0), rep("C", 0)),
           Region2 = c(rep("A", 2), rep("B", 2), rep("C", 5)),
           Region3 = c(rep("A", 3), rep("B", 3), rep("C", 3))) %>%
  print.data.frame()
```
:::

-   $D_{1} = -(1\log(1) + 0\log(0) + 0\log(0)) = 0$
-   $D_{2} = -(\frac{2}{9}\log(\frac{2}{9}) +\frac{2}{9}\log(\frac{2}{9}) + \frac{5}{9}\log(\frac{5}{9})) \approx 1$
-   $D_{3} = \frac{1}{3}(1-\frac{1}{3}) + \frac{1}{3}(1-\frac{1}{3})+\frac{1}{3}(1-\frac{1}{3}) = \frac{2}{3} \approx 1.1$
:::
:::

```{r echo = F, eval = F}
set.seed(45) # 45
n <- nrow(penguins)
train_ids <- sample(1:n, 0.7*n)
train_dat <- penguins[train_ids,]
test_dat <- penguins[-train_ids,]

penguins_full <- tree(species ~ bill_length_mm  + bill_depth_mm +
                        flipper_length_mm + sex,
                      data = train_dat,
                      control = tree.control(nobs = nrow(train_dat), minsize = 2))
plot(penguins_full)
text(penguins_full, pretty = 0, cex = 0.5)
title("Full tree")

cv_tree <- cv.tree(penguins_full, FUN = prune.misclass)
data.frame(size = cv_tree$size, dev = cv_tree$dev) %>%
  ggplot(., aes(x = size, y = dev)) +
  geom_point() + 
  geom_line()+
  scale_x_continuous(breaks  = cv_tree$size)
best_size <- min(cv_tree$size[which(cv_tree$dev == min(cv_tree$dev))])
prune_penguins <- prune.misclass(penguins_full, best = best_size)
plot(prune_penguins)
text(prune_penguins, pretty = 0, cex = 1)
title("Pruned tree")

predict(prune_penguins, newdata = test_dat)

predict(prune_penguins, newdata = test_dat, type = "class")
```

```{r eval = F, echo = F}
set.seed(3)
mushrooms <- read.csv("~/Downloads/mushrooms.csv")
mushrooms <- mushrooms %>%
  mutate_if(is.character, factor)
n <- nrow(mushrooms)
train_ids <- sample(1:n, 0.9*n)
train_dat <- mushrooms[train_ids,]
test_dat <- mushrooms[-train_ids,]
mushrooms_tree <- tree(class ~ cap.color + odor + veil.type + habitat +  spore.print.color, data = train_dat,
                       split = "gini",
                       control = tree.control(nobs = nrow(train_dat), minsize = 2))
plot(mushrooms_tree)
text(mushrooms_tree, pretty = 0)

cv_tree <- cv.tree(mushrooms_tree, FUN = prune.misclass)
data.frame(size = cv_tree$size, dev = cv_tree$dev) %>%
  ggplot(., aes(x = size, y = dev)) +
  geom_point() + 
  geom_line()+
  scale_x_continuous(breaks  = cv_tree$size)
best_size <- min(cv_tree$size[which(cv_tree$dev == min(cv_tree$dev))])
prune_mushrooms <- prune.misclass(mushrooms_tree, best = best_size)
tree_preds <- predict(prune_mushrooms, newdata = test_dat, type = "class")

library(randomForest)


mushroom_bag <- randomForest(class ~ cap.color + odor + veil.type + habitat +  spore.print.color, data = mushrooms, ntree = 100,
             mtry = 5)
bag_preds <- mushroom_bag$predicted[-train_ids]

mushroom_rf <- randomForest(class ~ cap.color + odor + veil.type + habitat +  spore.print.color, data = mushrooms, ntree = 100)
rf_preds <- mushroom_rf$predicted[-train_ids]

table(tree_preds, test_dat$class)
table(bag_preds, test_dat$class)
table(rf_preds, test_dat$class)
```

## Exercise

Work through building a mini classification tree!
