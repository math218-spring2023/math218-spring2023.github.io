{
  "hash": "babc21325a84d10511ac527e602f9693",
  "result": {
    "markdown": "---\ntitle: \"What is statistical learning?\"\ndate: \"February 16, 2023\"\ntitle-slide-attributes:\n    data-background-image: \"figs/01-intro/mite.png\"\n    data-background-size: contain\n    data-background-opacity: \"0.3\"\nformat: \n  revealjs:\n    theme: custom.scss\n    transition: none\n    incremental: true\neditor: visual\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n# Housekeeping\n\n-   2/21/23: Reminder that Lab 01 is due to Canvas this Thursday at 11:59pm\n\n    -   Office hours tomorrow 3-4pm, and by appointment via Calendly\n\n::: footer\nImage on title slide: https://www.chaosofdelight.org/all-about-mites-oribatida\n:::\n\n# Introduction\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n## What is statistical learning?\n\n-   Set of tools used to understand data\n\n    -   Supervised and unsupervised methods\n\n-   Use data and build appropriate functions (models) to try and perform inference and make predictions\n\n-   ::: important\n    Data-centered approach\n    :::\n\n-   Categories of statistical learning problems\n\n    -   Classification\n    -   Learning relationships\n    -   Prediction\n\n## Supervised Learning\n\n-   Notation: let $i = 1,\\ldots, n$ index the observation\n\n-   For each observation $i$, we have:\n\n    -   Response (outcome): $y_{i}$\n    -   Vector of $p$ predictors (covariates): $\\mathbf{x}_{i} = (x_{i1}, x_{i2}, \\ldots, x_{ip})'$\n\n-   **Regression**: the $y_{i}$ are quantitative (e.g. height, price)\n\n-   **Classification**: the $y_{i}$ are categorical (e.g. education level, diagnosis)\n\n-   Goal: relate response $y_{i}$ to the various predictors\n\n## Objectives in Supervised Learning\n\n1.  **Explanatory**: understand which predictors affect the response, and how\n2.  **Prediction**: accurately predict unobserved cases for new measurements of predictors\n3.  **Assessment**: quantify the quality of our predictions and inference\n\n<!-- ## Unsupervised Learning -->\n\n<!-- -   We only observe the $\\mathbf{x}_{i}$, but no associated response $y_{i}$ -->\n\n<!-- -   \"Unsupervised\" because there is no response variable guiding the analysis! -->\n\n<!-- -   Objective may not be as clearly defined -->\n\n<!-- -   Difficult to assess how well your are doing -->\n\n## Let's look at some real data!\n\n-   Oribatid mite data: abundance data of 35 oribatid mite species observed at 70 sampling locations irregularly spaced within a study area of 2.6 × 10 m collected on the territory of the Station de biologie des Laurentides of Université de Montréal, Québec, Canada in June 1989\n-   Variables measured at each location:\n    -   Substrate density (quantitative)\n\n    -   Water content (quantitative)\n\n    -   Microtopography (binary categorical)\n\n    -   Shrub density (ordinal categorical, three levels)\n\n    -   Substrate type (nominal categorical, seven levels)\n\n## Sampling map\n\n![](figs/01-intro/mites_map.jpeg){fig-align=\"center\"}\n\n## Data {.scrollable}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# from the vegan library\ndata(\"mite\")\ndata(\"mite.env\")\nnames(mite)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"Brachy\"   \"PHTH\"     \"HPAV\"     \"RARD\"     \"SSTR\"     \"Protopl\" \n [7] \"MEGR\"     \"MPRO\"     \"TVIE\"     \"HMIN\"     \"HMIN2\"    \"NPRA\"    \n[13] \"TVEL\"     \"ONOV\"     \"SUCT\"     \"LCIL\"     \"Oribatl1\" \"Ceratoz1\"\n[19] \"PWIL\"     \"Galumna1\" \"Stgncrs2\" \"HRUF\"     \"Trhypch1\" \"PPEL\"    \n[25] \"NCOR\"     \"SLAT\"     \"FSET\"     \"Lepidzts\" \"Eupelops\" \"Miniglmn\"\n[31] \"LRUG\"     \"PLAG2\"    \"Ceratoz3\" \"Oppiminu\" \"Trimalc2\"\n```\n:::\n\n```{.r .cell-code}\n# Focus on just the LRUG mite abundances\nmite_dat <- mite.env %>%\n  add_column(abundance = mite$LRUG)\nhead(mite_dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  SubsDens WatrCont Substrate Shrub    Topo abundance\n1    39.18   350.15   Sphagn1   Few Hummock         0\n2    54.99   434.81    Litter   Few Hummock         0\n3    46.07   371.72 Interface   Few Hummock         0\n4    48.19   360.50   Sphagn1   Few Hummock         0\n5    23.55   204.13   Sphagn1   Few Hummock         0\n6    57.32   311.55   Sphagn1   Few Hummock         0\n```\n:::\n:::\n\n\n## EDA {.scrollable}\n\n(scroll for more content)\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](slides-01-what-is_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=1875}\n:::\n\n::: {.cell-output-display}\n![](slides-01-what-is_files/figure-revealjs/unnamed-chunk-3-2.png){fig-align='center' width=1875}\n:::\n\n::: {.cell-output-display}\n![](slides-01-what-is_files/figure-revealjs/unnamed-chunk-3-3.png){fig-align='center' width=1875}\n:::\n:::\n\n\n# Statistical learning tasks\n\n## Model building\n\n-   **Goal:** predict `LRUG` abundance using these variables\n\n-   Maybe `LRUG` $\\approx f($ `SubsDens` + `WatrCont`$)$?\n\n-   If so, how would we represent these variables using our notation? i.e., what are $y_{i}$ and $x_{i}$?\n\n-   Then our model can be written as $y_{i} = f(x_{i}) + \\epsilon_{i}$ where $\\epsilon_{i}$ represents random measurement error\n\n-   ::: important\n    What does this equation mean?\n    :::\n\n## Why care about f?\n\n-   Model (dropping the indices): $Y = f(X) + \\epsilon$\n\n-   The function $f(X)$ represents the systematic information that $X$ tells us about $Y$.\n\n-   If $f$ is \"good\", then we can make reliable predictions of $Y$ at new points $X = x$\n\n-   If $f$ is \"good\", then we can identify which components of $X$ are important for explaining $Y$\n\n    -   Depending on $f$, we may be able to learn how each component of $X$ affects $Y$\n\n## Why care about f?\n\n-   We assume that $f$ is fixed but unknown\n-   Goal of statistical learning: how to obtain an estimate $\\hat{f}$ of the true $f$?\n    -   Sub-goals: **prediction** and **inference**\n-   The sub-goal may affect our choice of $\\hat{f}$\n\n## Prediction\n\n-   We have a set of inputs or predictors $x_{i}$, and we want to predict a corresponding $y_{i}$. Assume the true model is $y_{i} = f(x_{i}) + \\epsilon_{i}$, but don't know $f$\n\n-   Assuming the error $\\epsilon_{i}$ is 0 on average, we can obtain predictions of $y_{i}$ as $$\\hat{y}_{i} = \\hat{f}(x_{i})$$\n\n    -   Then, if we know the true $y_{i}$, we can evaluate the accuracy of the prediction $\\hat{y}_{i}$\n\n-   Generally, $y_{i} \\neq \\hat{y}_{i}$. Why?\n\n    1.  $\\hat{f}$ will not be perfect estimate of $f$\n    2.  $y_{i}$ is a function of $\\epsilon_{i}$, which cannot be predicted using $x_{i}$\n\n## Types of error\n\n-   Model: $y_{i} = f(x_{i}) + \\epsilon_{i}$\n\n-   ::: important\n    Irreducible error: $\\epsilon_{i}$\n    :::\n\n    -   Even if we knew $f$ perfectly, there is still some inherent variability\n    -   $\\epsilon_{i}$ may also contain unmeasured variables that are not available to us\n\n-   ::: important\n    Reducible error: how far $\\hat{f}$ is from the true $f$\n    :::\n\n## Prediction errors\n\n-   Ways to quantify error\n    -   Difference/error = $y_{i} - \\hat{y}_{i}$\n    -   Absolute error = $|y_{i} - \\hat{y}_{i}|$\n    -   Squared error = $(y_{i} - \\hat{y}_{i})^2$\n-   Intuitively, larger error indicates worse prediction\n-   Question: are there scenarios where we might prefer one error over another?\n\n## Prediction errors\n\n-   Given $\\hat{f}$ and $x_{i}$, we can obtain a prediction $\\hat{y}_{i} = \\hat{f}(x_{i})$ for $y_{i}$\n\n-   Mean-squared prediction error: \\begin{align*}\n    \\mathsf{E}[(y_{i} - \\hat{y}_{i})^2] &= \\mathsf{E}[( f(x_{i}) + \\epsilon_{i} - \\hat{f}(x_{i}))^2] \\\\\n    &= \\underbrace{[f(x_{i}) - \\hat{f}(x_{i})]^2}_\\text{reducible} + \\underbrace{\\text{Var}(\\epsilon_{i})}_\\text{irreducible}\n    \\end{align*}\n\n-   We cannot do much to decrease the irreducible error\n\n-   But we *can* potentially minimize the reducible error by choosing better $\\hat{f}$!\n\n## Inference\n\n-   We are often interested in learning how $Y$ and the $X_{1}, \\ldots, X_{p}$ are related or associated\n-   In this mindset, we want to estimate $f$ to learn the relationships, rather than obtain a $\\hat{Y}$\n\n## Prediction vs Inference\n\n-   Prediction: estimate $\\hat{f}$ for the purpose of $\\hat{Y}$ and $Y$.\n\n-   Inference: estimate $\\hat{f}$ for the purpose of $X$ and $Y$\n\n-   Some problems will call for prediction, inference, or both\n\n    -   To what extent is `LRUG` abundance associated with `microtopography`?\n\n    -   Given a specific land profile, how many `LRUG` mites would we expect there to be?\n\n# Assessment\n\n## Assessing model accuracy\n\n-   No single method or choice of $\\hat{f}$ is superior over all possible data sets\n\n-   Prediction accuracy vs. interpretability\n\n    -   More restrictive models may be easier to interpret (better for inference)\n\n    -   Good fit vs. over-fit (or under-fit)\n\n-   A simpler model is often preferred over a very complex one\n\n## Assessing model accuracy\n\n-   How can we know how well a chosen $\\hat{f}$ is performing?\n\n-   In regression setting, we often use **mean squared error (MSE)** or **root MSE (RMSE)**\n\n    -   $\\text{MSE}=\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{f}(x_{i}))^2$\n\n    -   $\\text{RMSE}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{f}(x_{i}))^2}$\n\n-   MSE (and RMSE) will be small if predictions $\\hat{y}_{i} = \\hat{f}(x_{i})$ are very close to true $y_{i}$\n\n-   Question: why might we prefer reporting RMSE over MSE?\n\n## Training vs. test data\n\n-   In practice, we split our data into **training** and **test** sets\n\n    -   Training set is used to fit the model\n    -   Test set is used to assess model fit\n\n-   We are often most interested in accuracy of our predictions when applying the method to *previously unseen* data. Why?\n\n-   We can compute the MSE for the training and test data respectively...but we typically focus more attention to **test MSE**\n\n## Example 1\n\nI generated some fake data and fit three models that differ in flexibility. In this example, the generated data (points) follow a curve-y shape.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](slides-01-what-is_files/figure-revealjs/plot_mse1-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n## Example 2\n\nIn this example, the generated data (points) look more linear.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](slides-01-what-is_files/figure-revealjs/plot_mse2-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n## Bias-Variance trade-off\n\n-   As model flexibility increases, the training MSE will decrease but test MSE may not.\n\n-   Flexible models may **overfit** the data, which leads to low train MSE and high test MSE\n\n    -   The supposed patterns in train data do not exist in test data\n\n-   Let us consider a test observation $(x_{0}, y_{0})$.\n\n-   The expected test MSE for given $x_{0}$ can be decomposed as follows:\n\n    -   $\\mathsf{E}[(y_{0} - \\hat{f}(x_{0}))^2] = \\text{Var}(\\hat{f}(x_{0})) + [\\text{Bias}(\\hat{f}(x_{0}))]^2 + \\text{Var}(\\epsilon)$\n\n    -   $\\text{Bias}(\\hat{f}(x_{0})) = \\mathsf{E}[\\hat{f}(x_{0})] - \\hat{f}(x_{0})$\n\n\n::: {.cell layout-align=\"center\" hash='slides-01-what-is_cache/revealjs/bv1_58d0d8febd7604224ec83e50fca75411'}\n\n:::\n\n::: {.cell layout-align=\"center\" hash='slides-01-what-is_cache/revealjs/bv2_69977d9da7c2de3c9cb53fd9755243de'}\n\n:::\n\n\n## Bias-Variance trade-off (cont.)\n\n::: r-stack\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](slides-01-what-is_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=2400}\n:::\n:::\n\n\n::: fragment\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](slides-01-what-is_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=2400}\n:::\n:::\n\n:::\n\n::: fragment\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](slides-01-what-is_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=2400}\n:::\n:::\n\n:::\n:::\n\n<!-- ## Classification -->\n\n<!-- -   Up until now, we have focused on quantitative responses $y_{i}$ -->\n\n<!-- -   What happens when $y_{i}$ is qualitative? Examples include: -->\n\n<!--     -   Medical diagnosis: $\\mathcal{C} = \\{\\text{yes}, \\text{no}\\}$ -->\n\n<!--     -   Education level: $\\mathcal{C} = \\{\\text{high school}, \\text{college}, \\text{graduate}\\}$ -->\n\n<!-- -   Each category in $\\mathcal{C}$ is also known as a *label* -->\n\n<!-- -   In this setting, we want our model to be **classifier**, i.e. given predictors $X$, predict a label from the pool of all possible categories $\\mathcal{C}$ -->\n\n<!-- ## Classification -->\n\n<!-- -   We will still have to estimate $f$ -->\n\n<!-- -   $\\hat{y}_{i}$ is the predicted class label for observation $i$ using estimate $\\hat{f}$ -->\n\n<!-- -   How to assess model accuracy? Error is more intuitive: we make an error if we predict the incorrect label, and no error otherwise -->\n\n<!-- -   This can be represented using an *indicator* variable or function. $\\mathbf{I}(y_{i} = \\hat{y}_{i})$: -->\n\n<!--     -   $$\\mathbf{I}(y_{i} = \\hat{y}_{i}) = \\begin{cases} 1 & \\text{ if } y_{i} = \\hat{y}_{i}\\\\ 0 & \\text{ if } y_{i} \\neq \\hat{y}_{i} \\end{cases}$$ -->\n\n<!-- ## Classification error rate -->\n\n<!-- -   To quantify accuracy of estimated classifier $\\hat{f}$, can calculate the *error rate*, which is the proportion of mistakes we make in labeling: -->\n\n<!--     -   $$\\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{I}(y_{i} \\neq \\hat{y}_{i})$$ -->\n\n<!-- -   Small error rate is preferred -->\n\n<!-- -   As with MSE, can calculate the error rate for train and test data sets -->\n\n<!-- ## Classifiers -->\n\n<!-- -   How do we choose which label to predict for a given observation? -->\n\n<!-- -   Assume we have a total of $J$ possible labels in $\\mathcal{C}$ -->\n\n<!-- -   For a given observation $i$, can calculate the following probability for each possible label $j$: $$p_{ij}(x_{i}) = Pr(y_{i} = j | X = x_{i})$$ -->\n\n<!-- -   These probabilities are called **conditional class probabilities** at $x_{i}$ -->\n\n<!-- ## Bayes optimal classifier -->\n\n<!-- -   The **Bayes optimal** classifier will assign/predict the label which has the largest conditional class probability -->\n\n<!--     -   It can be shown that the *test* error rate $\\frac{1}{n_{test}} \\sum_{i=1}^{n_{test}} \\mathbf{I}(y_{i} \\neq \\hat{y}_{i})$ is minimized when using the Bayes optimal classifier -->\n\n<!-- -   For example, consider a binary problem with levels \"yes\" and \"no\". -->\n\n<!-- -   For observation $i$, if $Pr(y_{i} = \\text{yes} | X = x_{i}) > 0.5$, then $\\hat{y}_{i} =$ \"yes\". -->\n\n<!-- -   The $x_{i}$ where $Pr(y_{i} = \\text{yes} | X = x_{i}) = Pr(y_{i} = \\text{no} | X = x_{i})= 0.5$ is called the *Bayes decision boundary* -->\n\n<!-- ## Example -->\n\n<!-- ```{r fig.align = \"center\", fig.height=7.5, fig.width=7.5} -->\n\n<!-- set.seed(1) -->\n\n<!-- n <- 100 -->\n\n<!-- lb <- -2; ub = 2 -->\n\n<!-- x <- cbind(rtruncnorm(n, a = lb, b = ub), runif(n, lb, ub)) -->\n\n<!-- beta <- c(1, 0.5) -->\n\n<!-- true_probs <- pnorm(x%*%beta - 0.5*x[,2]^2) -->\n\n<!-- labels <- purrr::rbernoulli(n, true_probs)*1 -->\n\n<!-- points_df <- data.frame(x) %>% -->\n\n<!--          mutate(class = factor(labels)) -->\n\n<!-- x_grid <- expand.grid(x = seq(lb,ub,0.1), y = seq(lb, ub, 0.1)) -->\n\n<!-- probs <- pnorm(as.matrix(x_grid)%*% beta - 0.5*x_grid[,2]^2) -->\n\n<!-- plot_df_bayes <- data.frame(probs, X1=x_grid[,1], X2 = x_grid[,2]) %>% -->\n\n<!--   mutate(class = case_when(probs <= 0.50 ~ 0, -->\n\n<!--                            probs > 0.5 ~ 1)) %>% -->\n\n<!--   mutate(class = factor(class)) -->\n\n<!-- ggplot() + -->\n\n<!--   geom_point(data = points_df, aes(x = X1, y = X2, col = class), size = 2) + -->\n\n<!--   geom_point(data = plot_df_bayes, aes(x = X1, y = X2, col = class), size = 0.05)+ -->\n\n<!--   geom_contour(data = plot_df_bayes, aes(x = X1, y=X2, z = as.numeric(class == 0)), -->\n\n<!--                breaks = 0.1, -->\n\n<!--                col = \"black\")+ -->\n\n<!--   theme_bw()+ -->\n\n<!--   guides(col = \"none\")+ -->\n\n<!--   theme(text = element_text(size = 16)) -->\n\n<!-- ``` -->\n\n<!-- ## Classification in practice -->\n\n<!-- -   Bayes classifier is \"gold standard\" -->\n\n<!-- -   In practice, we cannot compute the $p_{ij}(x_{i})$ exactly because we do know the conditional distribution of $y$ given $x$ -->\n\n<!-- -   Instead, we need to estimate these $p_{ij}(x_{i})$ -->\n\n<!-- -   Almost all of our choices of $\\hat{f}$ will output these estimates -->\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}