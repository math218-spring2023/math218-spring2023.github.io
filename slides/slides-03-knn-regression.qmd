---
title: "KNN Regression"
date: "February 28, 2023"
title-slide-attributes:
    data-background-image: "figs/title/animal_crossing.png"
    data-background-size: contain
    data-background-opacity: "0.3"
format: 
  revealjs:
    theme: custom.scss
    transition: none
    incremental: true
editor: visual
editor_options: 
  chunk_output_type: console
draft: false
---

# Housekeeping

-   Lab 02 due Thursday to Canvas at 11:59pm

-   Office hours today from 3-4pm

```{r packabundances, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, messabundance = F)

library(tidyverse)
library(broom)
library(ggfortify)
library(ISLR)
library(vegan)
plot_theme <- theme(text = element_text(size = 16))
data(mite)
data(mite.env)
mite_dat <- mite.env %>%
  add_column(abundance = mite$LRUG)
get_rmse <- function(y_pred, true){
  sqrt(mean((y_pred-true)^2))
}
circleFun <- function(center = c(0,0),diameter = 1, npoints = 100){
    r = diameter / 2
    tt <- seq(0,2*pi,length.out = npoints)
    xx <- center[1] + r * cos(tt)
    yy <- center[2] + r * sin(tt)
    return(data.frame(x = xx, y = yy))
}
```

# K-nearest neighbors

(Warning: there will be a *lot* of $K$'s in this course!)

## K-nearest neighbors

-   K-nearest neighbors (KNN) is a nonparametric supervised learning method

-   Intuitive and simple

-   Relies on the assumption: observations with similar predictors will have similar responses

-   Works for both regression and classification (we will start with regression)

## Algorithm (in words)

-   Choose a positive integer $K$, and have your data split into a train set and test set

-   For a given test observation with predictor $x_{0}$:

    -   Identify the $K$ points in the train data that are **closest** (in predictor space) to $x_{0}$. Call this set of neighbors $\mathcal{N}_{0}$.

    -   Predict $\hat{y}_{0}$ to be the average of the responses in the neighbor set, i.e. $$\hat{y}_{0} = \frac{1}{K} \sum_{i \in \mathcal{N}_{0}} y_{i}$$

## Example

```{r}
test_pt <- c(6.8, 2.15)
```

-   On the next slide, you will see plot with a bunch of colored points

-   Each point is plotted in predictor space $(x1, x2)$, and is colored according to the value of its response $y$

-   I have a new point ($\color{blue}{*}$) and its covariates/predictors, but I need to make a prediction for its response

## Example (cont.) {.scrollable}

::: r-stack
```{r fig.width=7, fig.height=6}
set.seed(1)
n <- 20
x1 <- runif(n, 5, 7)
x2 <- runif(n, 2, 4)
y <- x1 + x2 + rnorm(n, 0, 0.5)
df <- data.frame(x1,x2, y) %>%
  mutate(y = round(y, 3))
p <- ggplot(df, aes(x = x1, y=x2, col = y)) +
  geom_point(size = 3) +
  viridis::scale_color_viridis(option = "plasma") +
  plot_theme
p
```

::: fragment
```{r fig.width=7, fig.height=6}
p +
  geom_point(aes(x =  test_pt[1], y = test_pt[2]), col = "blue", pch = 8, size = 3) 
```
:::

::: fragment
```{r fig.width=7, fig.height=6}
p +
  geom_text(aes(label = y), nudge_y = 0.05) +
  geom_point(aes(x = test_pt[1], y = test_pt[2]), col = "blue", pch = 8, size = 3) 

```
:::
:::

```{r}
.dist <- function(x0, x){
   n <- nrow(x)
   dists <- rep(NA, n)
   for(i in 1:n){
     dists[i] <- sqrt(sum((x0 - x[i,])^2))
   }
   dists
}
dists <- .dist(test_pt, df[,1:2])
nb_ids <- data.frame(d = dists) %>%
   mutate(id = row_number()) %>%
   arrange(d) %>%
   pull(id)
K <- 3
nbs <- nb_ids[1:K]

```

-   Using $K= `r K`$, predicted $\hat{y}_{0} = \frac{1}{`r K`}(`r round(y[nbs[1]], 3)` + `r round(y[nbs[2]], 3)` + `r round(y[nbs[3]], 3)`) = `r round(mean(y[nbs]), 3)`$

```{r}
K <- 4
nbs <- nb_ids[1:K]
```

-   Using $K= `r K`$, predicted $\hat{y}_{0} = \frac{1}{`r K`}(`r round(y[nbs[1]], 3)` + `r round(y[nbs[2]], 3)` + `r round(y[nbs[3]], 3)` + `r round(y[nbs[4]], 3)`) = `r round(mean(y[nbs]), 3)`$

## Considerations

1.  How do we determine who the neighbors $\mathcal{N}_{0}$ should be? On the previous slide, it may have seemed intuitive.

    -   i.e. how do we quantify "closeness"?

2.  Which $K$ should we use?

# Consideration 1: Closeness

-   We will quantify closeness using distance metrics

## Euclidean distance

-   One of the most common ways to measure distance between points is with **Euclidean distance**

-   On a number line (one-dimension), the distance between two points $a$ and $b$ is simply the absolute value of their difference

    -   Let $d(a,b)$ denote the Euclidean distance between $a$ and \$b\$. Then in 1-D, $d(a,b) = |a-b|$.

-   In 2-D (think lon-lat coordinate system), the two points are $\mathbf{a} = (a_{1}, a_{2})$ and $\mathbf{b} = (b_{1}, b_{2})$ with Euclidean distance $$d(\mathbf{a}, \mathbf{b}) = \sqrt{(a_{1} - b_{1}) ^2 + (a_{2} - b_{2})^2}$$

-   **Important**: the "two-dimensions" refers to the number of coordinates in each point, not the fact that we are calculating a distance between two points

## Euclidean distance (cont.)

-   Generalizing to $p$ dimensions: if $\mathbf{a} = (a_{1}, a_{2}, \ldots, a_{p})$ and $\mathbf{b} = (b_{1}, b_{2}, \ldots, b_{p})$, then $$d(\mathbf{a}, \mathbf{b}) = \sqrt{(a_{1} - b_{1}) ^2 + (a_{2} - b_{2})^2 + \ldots (a_{p} - b_{p})^2} = \sqrt{\sum_{j=1}^{p}(a_{j} - b_{j})^2}$$

-   E.g. let $\mathbf{a} = (1, 0, 3)$ and $\mathbf{b} = (-1, 2, 2)$. What is $d(\mathbf{a}, \mathbf{b})$?

## Euclidean distance

-   Another common distance metric is **Manhattan distance**

    -   Named after the grid-system of Manhattan's roads

-   The Manhattan distance between two points $\mathbf{a}$ and $\mathbf{b}$ in $p$-dimensions is $$d_{m}(\mathbf{a}, \mathbf{b}) = |a_{1} - b_{1}| + |a_{2}  - b_{2}| + \ldots +|a_{p}  - b_{p}| = \sum_{j=1}^{p} |a_{j}  - b_{j}|$$

-   E.g. let $\mathbf{a} = (1, 0, 3)$ and $\mathbf{b} = (-1, 2, 2)$. What is $d_{m}(\mathbf{a}, \mathbf{b})$?

## Distance in KNN

-   We want to find the closest neighbor(s) in the train set to a given test point $\mathbf{x}_{0}$, such that we can make a prediction $\hat{y}_{0}$.

-   ::: important
    Important: what would the points $\mathbf{a}$ and $\mathbf{b}$ be in KNN?
    :::

# Consideration 2: K

## How much does K matter?

-   It can matter a lot!

-   As we saw previously, you will get different predicted $\hat{y}_{0}$ for different choices of $K$ in KNN regression

-   Discuss:

    -   What does $K=1$ mean? Do you think $K = 1$ is a good choice?

    -   Generally, do you think it's better to choose a small $K$ or big $K$?

# Example: mite data

## Mite data: preparation {.nonincremental}

-   The following code divides my data into train and test sets.

-   ::: important
    Make sure you understand what each line of code is doing! If you don't, please ask!
    :::

::: fragment
```{r}
#| echo: true
#| code-line-numbers: true
set.seed(6)
n <- nrow(mite_dat)
test_ids <- sample(1:n, 2)
train_x <- mite_dat[-test_ids, c("SubsDens", "WatrCont")]
train_y <-  mite_dat$abundance[-test_ids]
test_x <- mite_dat[test_ids,  c("SubsDens", "WatrCont")]
test_y <-  mite_dat$abundance[test_ids]
head(train_x)
```
:::

```{r}
K <- 3
nbs_ls <- list()
for(i in 1:nrow(test_x)){
  nbs_ls[[i]] <- data.frame(d = .dist(test_x[i,], train_x)) %>%
    mutate(id = row_number()) %>%
    arrange(d) %>%
    slice(1:K) 
}


```

## Mite data: KNN

::: r-stack
::: fragment
```{r}
#| fig-width: 7
#| fig-height: 7
train_dat <- data.frame(train_x) %>%
  mutate(abundance = train_y)
p2 <- ggplot()+
  geom_point(data = train_dat, aes(x = SubsDens, y = WatrCont, col = abundance), size = 3) +
  scale_color_viridis_c(option = "plasma") +
  plot_theme + 
  theme(legend.position = "bottom")+
  ggtitle("Train data")
p2
```
:::

::: fragment
```{r}
#| fig-width: 7
#| fig-height: 7

test_dat <- data.frame(test_x) %>%
  mutate(abundance = test_y)
p2 + 
  geom_text(data = test_dat %>%
               mutate(test_pt = row_number()), 
                      mapping = aes(x = SubsDens, y = WatrCont,label = test_pt), size = 4)+
  labs(subtitle = "and test points")
```
:::
:::

## Mite data: KNN results {.scrollable}

::: columns
::: {.column width="60%"}
-   Running KNN with $K = 3$ and using Euclidean distance, I identify the following neighbor sets for each test point:

::: fragment
```{r}
#| fig-width: 7
#| fig-height: 7
p2 + 
  geom_text(data = test_dat %>%
               mutate(test_pt = row_number()), 
                      mapping = aes(x = SubsDens, y = WatrCont,label = test_pt))+
  geom_path(data = circleFun(center = as.numeric(test_dat[1,1:2]),
                             diameter = nbs_ls[[1]][K,1]*2),
            mapping = aes(x = x, y = y), linetype = "dashed") +
  geom_path(data = circleFun(center = as.numeric(test_dat[2,1:2]),
                             diameter = nbs_ls[[2]][K,1]*2),
            mapping = aes(x = x, y = y), linetype = "dashed") +
  labs(subtitle= "with neighbor sets for test points")
```
:::

```{r}
summary_df <- train_dat %>%
  mutate(id = row_number()) %>%
  inner_join(., do.call(rbind,nbs_ls) %>%
               mutate(test_pt = rep(1:length(test_ids), each = K)),
             by = "id") %>%
  group_by(test_pt) %>%
  summarise(y_hat = mean(abundance)) %>%
  add_column(y_true = test_dat$abundance )
rmse <- get_rmse(summary_df$y_hat, summary_df$y_true) 
```
:::

::: {.column width="40%"}
-   Predicted abundance $\hat{y}$ and true abundance $y$ for both test points, for a test RMSE of `r round(rmse, 3)`.

::: fragment
```{r}
summary_df
```
:::

-   ::: important
    Discuss: it seems like we did poorly for the first test observation. Does its neighbor set "make sense"?
    :::
:::
:::

## Standardizing predictors {.scrollable}

We will **standardize** our predictors, meaning that each predictor $X_{j}$ will be transformed to have mean 0 and standard deviation 1:

$$X_{j}^{\text{std}} = \frac{X_{j} - \bar{X_{j}}}{\sigma_{X_{j}}},$$

where $X_{j}$ is the vector of the $j$-th predictor, $\bar{X}_{j}$ is the average of $X_{j}$, and $\sigma_{X_{j}}$ is its standard deviation.

::: fragment
```{r}
#| echo: true
scale(train_x$SubsDens)

# confirming we have mean 0 and sd 1
scaled_SubsDens <- scale(train_x$SubsDens)
mean(scaled_SubsDens)
sd(scaled_SubsDens)
```
:::

## Standardizing multiple variables

::: columns
::: {.column width="40%"}
::: fragment
```{r}
#| echo: true
train_x_scaled <- train_x %>%
  mutate_if(is.numeric, scale)
head(train_x_scaled)
```
:::
:::

::: {.column width="60%"}
::: fragment
```{r}
#| echo: true
#| code-line-numbers: "1-3"
train_x_scaled <- train_x
train_x_scaled$SubsDens <- scale(train_x$SubsDens)
train_x_scaled$WatrCont <- scale(train_x$WatrCont)
head(train_x_scaled)
```
:::
:::
:::

## Standardizing the test data

-   We should use the same statistics from the training data to scale the test data

    -   i.e. to standardize the $j$-th predictor of the *test* data, we should use the mean and standard deviation of the $j$-th predictor from the *training* data

-   ::: important
    Discuss: why not scale the predictors first, and then split into train/test sets?
    :::

```{r}
train_x_scaled <- train_x %>%
  mutate_if(is.numeric, scale)
train_means <- colMeans(train_x)
train_sds <- apply(train_x, 2, sd)
test_x_scaled <- test_x
for(j in 1:ncol(test_x_scaled)){
  test_x_scaled[,j] <- (test_x_scaled[,j] - train_means[j])/train_sds[j]
}
```

::: fragment
```{r}
#| echo: true
# note: I am not providing you the code for how I scaled my test observations!
test_x_scaled
```
:::

```{r}
train_dat <- data.frame(train_x_scaled) %>%
  mutate(abundance = train_y ) 
test_dat <-  data.frame(test_x_scaled) %>%
  mutate(abundance = test_y ) 
```

-   Important:
    -   I do not scale the response variable
    -   I scale after splitting into train/test

## Scaled mite data

::: r-stack
::: fragment
```{r}
#| fig-width: 7
#| fig-height: 7
nbs_ls <- list()
for(i in 1:nrow(test_dat)){
  nbs_ls[[i]] <- data.frame(d = .dist(test_x_scaled[i,], train_x_scaled)) %>%
    mutate(id = row_number()) %>%
    arrange(d) %>%
    slice(1:K) 
}

p3 <- ggplot()+
  geom_point(data = train_dat, aes(x = SubsDens, y = WatrCont, col = abundance), size = 3) +
  scale_color_viridis_c(option = "plasma") +
  plot_theme+
  theme(legend.position = "bottom") +
  ggtitle("Scaled train data")

p3
```
:::

::: fragment
```{r}
#| fig-width: 7
#| fig-height: 7
p3 + 
  geom_text(data = test_dat %>%
               mutate(test_pt = row_number()), 
                      mapping = aes(x = SubsDens, y = WatrCont,label = test_pt), size = 4)+
  labs(subtitle = "and test points")
```
:::
:::

## Scaled mite data: KNN results {.scrollable}

::: columns
::: {.column width="60%"}
::: fragment
```{r}
#| fig-width: 7
#| fig-height: 7
p3 + 
  geom_text(data = test_dat %>%
               mutate(test_pt = row_number()), 
                      mapping = aes(x = SubsDens, y = WatrCont,label = test_pt))+
  geom_path(data = circleFun(center = as.numeric(test_dat[1,1:2]),
                             diameter = nbs_ls[[1]][K,1]*2),
            mapping = aes(x = x, y = y), linetype = "dashed") +
  geom_path(data = circleFun(center = as.numeric(test_dat[2,1:2]),
                             diameter = nbs_ls[[2]][K,1]*2),
            mapping = aes(x = x, y = y), linetype = "dashed") +
  labs(subtitle= "with neighbor sets for test points")
```
:::

```{r}
summary_df <- train_dat %>%
  mutate(id = row_number()) %>%
  inner_join(., do.call(rbind,nbs_ls) %>%
               mutate(test_pt = rep(1:length(test_ids), each = K)),
             by = "id") %>%
  group_by(test_pt) %>%
  summarise(y_hat = mean(abundance)) %>%
  add_column(y_true = test_dat$abundance )
rmse <- get_rmse(summary_df$y_hat, summary_df$y_true) 
```
:::

::: {.column width="40%"}
-   Predicted abundance $\hat{y}$ and true abundance $y$ for both test points, for a test RMSE of `r round(rmse, 3)`.

::: fragment
```{r}
summary_df
```
:::

-   Note how this RMSE compares to when we fit on original scale!

    -   Even though we do slightly worse predicting test point 2, we improve a lot on test point 1
:::
:::

# Summary

-   Discuss: what do you think are some pros and cons of KNN regression?

    -   You might want to compare to linear regression
    
- Key point: in KNN, you should consider **scaling** your predictors if they are on very different scales
  
- Looking forward:

  - You will implement KNN this week in small groups!
  
  -   We will learn later on in the semester how we might pick $K$

  -   What about categorical predictors?
