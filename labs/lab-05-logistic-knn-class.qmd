---
title: "Lab 05: Logistic Regression + KNN Classification"
description: "Rain in WaggaWagga"
editor: visual
callout-appearance: default
draft: false
editor_options: 
  chunk_output_type: console
execute: 
  echo: false
  eval: true
---

```{r packages,message = F}
library(tidyverse)
library(class)
source("../math218_fns.R")

```

## Introduction

We will try to predict if it rained the next day in Wagga Wagga, Australia given certain environmental and climate conditions the previous day. Our data, obtained from [Kaggle](https://www.kaggle.com/datasets/jsphyg/weather-dataset-rattle-package), consist of measurements of these variables on Fridays from 2009-2016.

```{r message = F}
library(sp)
library(ozmaps)
library(sf)
points <- cbind(147.3655, -35.1026)
df <- data.frame(x = points[1], y = points[2], name = "Wagga Wagga")
ww <- SpatialPointsDataFrame(coords =  points, data = df,
              proj4string=CRS("+proj=longlat +ellips=WGS84"))
ww <- st_as_sf(ww)
ww <- st_transform(ww, crs = "+proj=lcc +lon_0=135 +lat_0=-30 +lat_1=-10 +lat_2=-45 +datum=WGS84")

sf_oz <- ozmap_data("states")
ggplot(sf_oz ) + 
  geom_sf(aes(fill = NAME)) + 
  coord_sf(crs = "+proj=lcc +lon_0=135 +lat_0=-30 +lat_1=-10 +lat_2=-45 +datum=WGS84") +
  ggsflabel::geom_sf_label_repel(data = ww, aes(label = name),
                           force = 5000,  nudge_y = 14,seed = 10) +
  labs(x = "", y = "")
```

The variables are as follows:

-   `Evaporation`: The so-called Class A pan evaporation (mm) in the 24 hours to 9am
-   `Sunshine`: The number of hours of bright sunshine in the day
-   `WindSpeed3pm`: Wind speed (km/hr) averaged over 10 minutes prior to 3pm
-   `Humidity3pm`: Humidity (percent) at 3pm
-   `Pressure3pm`: Atmospheric pressure (hpa) reduced to mean sea level at 3pm
-   `Cloud3pm`: Fraction of sky obscured by cloud (in "oktas": eighths) at 3pm
-   `Temp3pm`: Temperature (degrees C) at 3pm
-   `RainTomorrow`: "yes" or "no" for whether it rained the next day

## Data

First, load in your data located in the `weatherWaggaWagga.csv` file. We will consider `RainTomorrow = "yes"` to be the success class.

::: {style="color: maroon"}
Pick one quantitative variable and create side-by-side boxplots that display the distribution of that variable for each level of `RainTomorrow`. Also display a table of the total number of successes and failures in the data. Interpret your plot, and comment on the table.
:::

```{r fig.width=4, fig.height=4}
sub_dat2 <- read.csv("data/weatherWaggaWagga.csv")
sub_dat2 <- sub_dat2 %>%
  mutate(RainTomorrow = factor(RainTomorrow, levels = c("No", "Yes")))
```

```{r eda, eval = F}
sub_dat2 %>%
  ggplot(., aes(x = RainTomorrow, y = Pressure3pm)) +
  geom_boxplot()
sub_dat2 %>%
  count(RainTomorrow)
```

## Part 1: validation set approach

We will compare the performance of logistic regression with that of KNN classification using a validation set approach.

### Test/train split

```{r}
seed <- 41
```

::: {style="color: maroon"}
First split your data into an 80% train and 20% test set using a seed of `r seed`.
:::

```{r}
n <- nrow(sub_dat2)
set.seed(seed)
train_ids <- sample(1:n, 0.8*n)
train_dat <- sub_dat2[train_ids,]
test_dat <- sub_dat2[-train_ids,]

# sub_dat <- weatherAUS %>% 
#   filter(Location == "WaggaWagga") %>%
#   na.omit() %>%
#   select(-Rainfall) %>%
#   filter(wday(Date) == 5) 
#   

# sub_dat2 <- sub_dat %>%
#   select("Evaporation", "Sunshine",  "WindSpeed3pm", "Humidity3pm",
#          "Pressure3pm", "Cloud3pm", "Temp3pm",  "RainTomorrow") %>%

```

### Logistic regression

#### Fit the model

::: {style="color: maroon"}
Fit a logistic regression to the training data for `RainTomorrow` using all the remaining variables in the dataset. Display a summary of the model, and interpret the coefficient for the variable you chose to visualize in the EDA section. How, if at all, is the probability of it raining tomorrow in Wagga Wagga associated with that predictor?
:::

::: callout-note
Hypothesis tests of the form $H_{0}: \ \beta_{j} =0$ vs $H_{a}: \ \beta_{j} \neq 0$ can be formulated for the coefficients in logistic regression just as they are in linear regression!
:::

```{r eval = F}
mod <- glm(RainTomorrow ~. ,data= train_dat, family = "binomial")
summary(mod)
```

#### Predict

Now, obtain predicted *labels* for the test data using a threshold probability of 0.5. Importantly, because the data are in `"yes"/"no"` and not 1/0, your predictions should also be in terms of `"yes"/"no"`.

::: {style="color: maroon"}
Then create a confusion matrix for the test set. Using code that is as reproducible as possible, obtain and report the misclassification rate, false negative rate, and false positive rate for the data. This can be achieved using either tidyverse or base `R`.

Comment on which is larger: your FPR or your FNR. Do these values make sense to you given the data?
:::

```{r eval = F}
log_preds <- c("No", "Yes")[1+1*(predict(mod, newdata = test_dat, type = "response") >= 0.5)]
table(log_preds, test_dat$RainTomorrow)
mean(log_preds != test_dat$RainTomorrow)
mean( (log_preds != test_dat$RainTomorrow)[test_dat$RainTomorrow == "Yes"])
mean( (log_preds != test_dat$RainTomorrow)[test_dat$RainTomorrow == "No"])
```

### KNN Classification

Now we will fit a KNN model to predict `RainTomorrow` using `K = 20` neighbors. Set a seed of `r seed` in case there are ties. As our predictors are on completely different scales, first properly standardize your train and test data sets before fitting the model. You may either use your own implementation of KNN classification, or you may use the `knn()` function from the `class` library. Paste any functions you may need in the chunk labeled `functions` at the top of the document.

::: {style="color: maroon"}
Under this model, create a confusion matrix for the test set. Using reproducible code, obtain and report the misclassification rate, false negative rate, and false positive rate for the data.

How do your rates and the predicted labels themselves compare to those obtained under the logistic regression model?
:::

```{r eval = F}
set.seed(seed)
K <- 20

train_x <- train_dat %>%
  select(-RainTomorrow) 
mean_vec <- colMeans(train_x); sd_vec <- apply(train_x, 2, sd)
train_x_std <- my_scale(train_x, mean_vec, sd_vec)
test_x <- test_dat %>%
  select(-RainTomorrow) 
test_x_std <- my_scale(test_x, mean_vec, sd_vec)
# 
# knn_preds <- knn(train_x,test_x,  train_dat$RainTomorrow, K)
# table(knn_preds, test_dat$RainTomorrow)
# mean(knn_preds != test_dat$RainTomorrow)
# mean( (knn_preds != test_dat$RainTomorrow)[test_dat$RainTomorrow == "Yes"])

knn_preds_std <- knn(train_x_std,test_x_std,  train_dat$RainTomorrow, K)
table(knn_preds_std, test_dat$RainTomorrow)
mean(knn_preds_std != test_dat$RainTomorrow)
mean( (knn_preds_std != test_dat$RainTomorrow)[test_dat$RainTomorrow == "Yes"])
mean( (knn_preds_std != test_dat$RainTomorrow)[test_dat$RainTomorrow == "No"])

```

## Part 2: stratified k-fold CV

At this point, we know that k-fold CV is a better approach to estimating the test error compared to a validation set approach. For this lab, we will implement *stratified* k-fold CV. In stratified k-fold CV, the proportions of each label in *each* fold should be representative of the proportion of each label in the *entire* data set.

### Create folds

::: {style="color: maroon"}
Here, create your list of fold indices for stratified k-fold CV using 10 folds of roughly equal size. Set a seed of `r seed` again. "Roughly equal" means that either all 10 folds are the same size, or 9 folds have the same size and the tenth fold has a little less.

Your code should be as reproducible as possible! That means it should not be specific to this specific data or number of folds. This is the most difficult part of the lab, so think carefully about what you want to do! It could be helpful to write down your thoughts.
:::

```{r eval = F}
# stratified k-fold CV
n_folds <- 10

set.seed(seed)
success_ids <- which(sub_dat2$RainTomorrow == "Yes")
failure_ids <- (1:n)[-success_ids]

success_fold_size <- ceiling(length(success_ids) / n_folds)
failure_fold_size <- ceiling(length(failure_ids) / n_folds)
rand_success_ids <- sample(success_ids)
rand_failure_ids <-  sample(failure_ids)
fold_ids <- list()
for(i in 1:n_folds){
  to_add <- c()
  if(length(rand_failure_ids) < failure_fold_size |length(rand_success_ids) < success_fold_size ){
      fold_ids[[i]] <- c(rand_success_ids, rand_failure_ids)
  }  else{
      fold_ids[[i]] <- c(rand_success_ids[1:success_fold_size], 
                         rand_failure_ids[1:failure_fold_size])
  }
  rand_success_ids <- rand_success_ids[-(1:success_fold_size)]
  rand_failure_ids <- rand_failure_ids[-(1:failure_fold_size)]
}
```

### Logistic regression

Now, fit the logistic regression model again, but now using the stratified 10-fold CV approach to estimate the test error.

::: {style="color: maroon"}
Report the estimated test misclassification, false negative, and false positive rates under this model.
:::

```{r eval = F}
set.seed(seed)
misclass_matrix_strat <- fnr_matrix_strat <- fpr_matrix_strat <- matrix(NA, nrow = n_folds, ncol = 3)
for(i in 1:n_folds){
  err_vec <- fnr_vec <- fpr_vec <-  rep(NA, 3)
  test_ids <- fold_ids[[i]]
  train_dat <- sub_dat2[-test_ids,]
  test_dat <- sub_dat2[test_ids,]
  mod <- glm(RainTomorrow ~ . , data= train_dat, family = "binomial")
  log_preds <- c("No", "Yes")[1+1*(predict(mod, newdata = test_dat, type = "response") >= 0.5)]
  err_vec[1] <- mean(log_preds != test_dat$RainTomorrow)
  
  fnr_vec[1] <- mean( (log_preds != test_dat$RainTomorrow)[test_dat$RainTomorrow == "Yes"])
  
  fpr_vec[1] <- mean( (log_preds != test_dat$RainTomorrow)[test_dat$RainTomorrow == "No"])

  train_x <- train_dat %>%
    select(-RainTomorrow) 
  mean_vec <- colMeans(train_x); sd_vec <- apply(train_x, 2, sd)
  train_x_std <- my_scale(train_x, mean_vec, sd_vec)
  test_x <- test_dat %>%
    select(-RainTomorrow) 
  test_x_std <- my_scale(test_x, mean_vec, sd_vec)
  
  # knn_preds <- knn(train_x,test_x,  train_dat$RainTomorrow, K)
  # err_vec[2] <- mean(knn_preds != test_dat$RainTomorrow)
  # fnr_vec[2] <- mean( (knn_preds != test_dat$RainTomorrow)[test_dat$RainTomorrow == "Yes"])
  knn_preds_std <- knn(train_x_std,test_x_std,  train_dat$RainTomorrow, K)
  err_vec[3] <- mean(knn_preds_std != test_dat$RainTomorrow)
  fnr_vec[3] <-mean( (knn_preds_std != test_dat$RainTomorrow)[test_dat$RainTomorrow == "Yes"])
  fpr_vec[3] <-mean( (knn_preds_std != test_dat$RainTomorrow)[test_dat$RainTomorrow == "No"])
  misclass_matrix_strat[i, ] <- err_vec
  fnr_matrix_strat[i,] <- fnr_vec
  fpr_matrix_strat[i,] <- fpr_vec
}
colMeans(misclass_matrix_strat)[1]
colMeans(fnr_matrix_strat)[1]
colMeans(fpr_matrix_strat)[1]

```

### KNN classification

Once again using `K = 20` neighbors and 10 folds, fit a KNN classification model with stratified k-fold CV using standardized predictors. Set a seed of `r seed` again.

::: {style="color: maroon"}
Report the estimated test misclassification, false negative, and false positive rates under this model. How do the prediction performances of your two models compare based on the estimated test error using stratified 10-fold CV?
:::

```{r eval = F}
colMeans(misclass_matrix_strat)[3]
colMeans(fnr_matrix_strat)[3]
colMeans(fpr_matrix_strat)[3]
```

## Part 3: Comprehension

1.  I also obtained estimates of the test error rates when performing regular 10-fold CV (i.e. non-stratified). The following shows my results:

```{r echo = F}
set.seed(seed)
K <- 20
# k-fold CV
n_folds <- 10
fold_size <- n / n_folds
rand_ids <- sample(1:n)
fold_ids <- list()
for(i in 1:n_folds){
  fold_ids[[i]] <- rand_ids[1:fold_size]
  rand_ids <- rand_ids[-(1:fold_size)]
}

misclass_matrix <-  fnr_matrix <- fpr_matrix <- matrix(NA, nrow = n_folds, ncol = 3)
for(i in 1:n_folds){
  err_vec <- fnr_vec <- fpr_vec <- rep(NA, 3)
  test_ids <- fold_ids[[i]]
  train_dat <- sub_dat2[-test_ids,]
  test_dat <- sub_dat2[test_ids,]
  mod <- glm(RainTomorrow ~ . , data= train_dat, family = "binomial")
  log_preds <- c("No", "Yes")[1+1*(predict(mod, newdata = test_dat, type = "response") >= 0.5)]
  err_vec[1] <- mean(log_preds != test_dat$RainTomorrow)
  fnr_vec[1] <- mean( (log_preds != test_dat$RainTomorrow)[test_dat$RainTomorrow == "Yes"])
  fpr_vec[1] <- mean( (log_preds != test_dat$RainTomorrow)[test_dat$RainTomorrow == "No"])
  train_x <- train_dat %>%
    select(-RainTomorrow) 
  mean_vec <- colMeans(train_x); sd_vec <- apply(train_x, 2, sd)
  train_x_std <- my_scale(train_x, mean_vec, sd_vec)
  test_x <- test_dat %>%
    select(-RainTomorrow) 
  test_x_std <- my_scale(test_x, mean_vec, sd_vec)
  
  # knn_preds <- knn(train_x,test_x,  train_dat$RainTomorrow, K)
  # err_vec[2] <- mean(knn_preds != test_dat$RainTomorrow)
  # fnr_vec[2] <- mean( (knn_preds != test_dat$RainTomorrow)[test_dat$RainTomorrow == "Yes"])
  
  knn_preds_std <- knn(train_x_std,test_x_std,  train_dat$RainTomorrow, K)
  err_vec[3] <- mean(knn_preds_std != test_dat$RainTomorrow)
  fnr_vec[3] <- mean( (knn_preds_std != test_dat$RainTomorrow)[test_dat$RainTomorrow == "Yes"])
  fpr_vec[3] <- mean( (knn_preds_std != test_dat$RainTomorrow)[test_dat$RainTomorrow == "No"])
  misclass_matrix[i, ] <- err_vec
  fnr_matrix[i,] <- fnr_vec
  fpr_matrix[i,] <- fpr_vec
}
# colMeans(misclass_matrix)
# colMeans(fnr_matrix)
tab <- as.table(cbind(colMeans(misclass_matrix), colMeans(fnr_matrix), colMeans(fpr_matrix))[-2,])
tab <- round(tab, 4)
rownames(tab) <- c("Logistic", "KNN")
colnames(tab) <- c("Misclass. rate", "FNR", "FPR")
kableExtra::kable(tab) %>%
  kableExtra::kable_styling()
```

Based on these results, what comparisons (if any) can you make about the performance of the models fit here and the models fit in Part 1 or Part 2? What is the correct way to explain why the results I obtained here are different than the rates you obtained via stratified k-fold CV (apart from the simple fact that we used different fold ids)?

2.  Compare the magnitudes of your estimated misclassification test error rates when using the validation set approach vs the stratified k-fold CV approach. Does this make sense to you? Why or why not?
3.  In this problem, do you think a false positive or a false negative rate is worse? Based on your answer, should you increase or decrease the threshold? Why?

## Submission

When you're finished, knit to PDF one last time and upload the PDF to Canvas. Commit and push your code back to GitHub one last time.
