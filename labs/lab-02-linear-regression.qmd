---
title: "Lab 02: Linear regression"
description: "Insurance costs"
categories: "Lab assignment"
draft: true
---

# Introduction

The data can be found in the `insurance.csv` file in the `data` folder of this project. Load in the data using the following code:

```{r eval = F}
insurance <- read.csv("data/insurance.csv")
```

https://www.kaggle.com/datasets/mirichoi0218/insurance

# Part 0

## Define functions

In an R chunk, create a function that takes in two vectors: one of true values and one of corresponding predicted values. The function should return the root mean squared error (RMSE).

## Data preparation

Split your data into a 90% train set and a 10% test set. Be sure to set a seed for reproducibility.

# Part 1

## EDA

Using the full data set, create some plots to examine the relationships between the response variable and some of the possible explanatory variables. Also create an EDA plot for the distribution of the response variable. Do you see any support for including an interaction term? If so, interaction(s) between which variables?

## Fitting the model

Use the **training** data to fit three linear regression models: a model that only includes main effects, a model that includes an interaction between `age` and `smoke` status, and a model that includes an interaction between `bmi` and `smoke` status.

## Interpretation

For each of your models, interpret how smoke status is associated with insurance charges.

## Predictions

Using the `predict()` function, obtain predictions for both of your models using the **test** data. You should have three vectors of predictions.

Then, obtain the RMSE for each model. Interpret one of the RMSEs, and determine which model performs better at predictions.

# Part 2

You may have noticed in Part 1 that the distribution of our response is heavily right-skewed. When this happens, it is often common to fit a linear regression on a log transformation of the response variable. That is, instead of $y \approx \beta_{0} + \beta_{1}x_{1} + \ldots + \beta_{p} x_{p}$, our regression is $\log(y) \approx \beta_{0} + \beta_{1}x_{1} + \ldots + \beta_{p} x_{p}$.

Repeat all of components in Part 1 here, but now treating the log of the insurance charges as our response. Note: to take the log of a value `x` in `R`, simply use the function `log(x)`.

# Part 3

## Comparisons

In Part 1, you determined a linear regression model for `charge` that performed the best predictions in terms of RMSE. In Part 2, you determined a linear regression model for `log(charge)` that performed the best predictions in terms of RMSE. Based on your findings, answer the following questions:

1.  Do the two models you selected share the same sets of covariates?
2.  Among all the models, which one do you prefer and why? If you do not or cannot make a preference, explain why not.

## Prediction uncertainty

I fit a linear regression model for `log(charges)` using the training data (for the purposes of this exercise, the exact model does not matter), and obtained predictions for the test data.

In the following plot, the points shown are the true `charge` values for each observation, plotted in ascending order of the predicted value. You will notice vertical colored error bars. These represent 95% *confidence* intervals and *prediction* intervals for each observation in the test data. If a bar is colored red, that means that the interval does not capture the true `charge` value. *Note*: the points are exactly the same between the two panels; the differences lie in the error bars.

![](figs/intervals.png)

Now that I've describe what the plot is showing, **interpret the findings**. That is, discuss the differences and similarities between the two panels. What are they showing us? Why do they look so different? Is there any pattern with respect to when an interval does not capture the true value?

# Submission

TBD
