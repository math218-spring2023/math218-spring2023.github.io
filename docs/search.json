[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Date\nContent & Materials\nDue dates\n\n\n\n\n\n\nWEEK 1\n\n\n\n\nT 2/14\nWelcome!\n\nClass introduction and intro to base R\n\n\n\n\nR 2/16\nWhat is statistical learning?\n\n\n\nF 2/17\nLab day\n\nMore advanced coding\nLab 01: Roulette\n\n\n\n\n\n\nWEEK 2\n\n\n\n\nT 2/21\n\n\n\n\nR 2/23"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 218: Spring 2023",
    "section": "",
    "text": "Warner 104, TRF 8:40-9:25AM Eastern\nProfessor Becky Tang\n\nOffice: Warner 214\nOffice hours: TBD\n\n\nVisit the schedule page to see the current course schedule, lecture notes, and due dates.\nVisit the assignments page to see the current list of assignments."
  },
  {
    "objectID": "index.html#materials",
    "href": "index.html#materials",
    "title": "Math 218: Spring 2023",
    "section": "Materials",
    "text": "Materials\nThere is no required textbook for this course.\nYou should have a fully-charged laptop, tablet with keyboard, or comparable device to every lecture and lab session.\nYou should also have R and RStudio installed on your machine. If you do not have either, please follow these instructions."
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Math 218: Spring 2023",
    "section": "Syllabus",
    "text": "Syllabus\nThe course syllabus can be found here. Please note that the once the semester begins, the schedule on the syllabus may not be up-to-date. Refer to the schedule page to see the current course schedule."
  },
  {
    "objectID": "slides-01-what-is.html#version-control",
    "href": "slides-01-what-is.html#version-control",
    "title": "What is statistical learning?",
    "section": "Version control",
    "text": "Version control\n\nGit is a version control system – like “Track Changes” features from Microsoft Word.\nGitHub is the home for your Git-based projects on the internet (like DropBox but much better).\nThere are a lot of Git commands and very few people know them all. 99% of the time you will use git to add, commit, push, and pull.\nWe will be using the GitHub Desktop application to perform the Git commands in a beginner-friendly way. Our local project in RStudio will be sent to GitHub using the application."
  },
  {
    "objectID": "slides-01-what-is.html#installation",
    "href": "slides-01-what-is.html#installation",
    "title": "What is statistical learning?",
    "section": "Installation",
    "text": "Installation\n\nAccept the e-mailed GitHub invitation to join our course organization. If you did not receive an invitation, that means I do not have your GitHub username!\nCreate a folder on your desktop called “Math218”\nGo to https://desktop.github.com/ and download the GitHub Desktop application.\n\nContinue following the instructions found here"
  },
  {
    "objectID": "slides-01-what-is.html#what-is-statistical-learning",
    "href": "slides-01-what-is.html#what-is-statistical-learning",
    "title": "What is statistical learning?",
    "section": "What is statistical learning?",
    "text": "What is statistical learning?\n\nSet of tools used to understand data\n\nSupervised and unsupervised methods\n\nUse data and build appropriate functions (models) to try and perform inference and make predictions\n\nData-centered approach\n\nCategories of statistical learning problems\n\nClassification\nLearning relationships\nPrediction"
  },
  {
    "objectID": "slides-01-what-is.html#supervised-learning",
    "href": "slides-01-what-is.html#supervised-learning",
    "title": "What is statistical learning?",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nNotation: let \\(i = 1,\\ldots, n\\) index the observation\nFor each observation \\(i\\), we have:\n\nOutcome/response: \\(y_{i}\\)\nVector of \\(p\\) predictors/covariates: \\(\\mathbf{x}_{i} = (x_{i1}, x_{i2}, \\ldots, x_{ip})'\\)\n\nRegression: the \\(y_{i}\\) are quantitative (e.g. height, price)\nClassification: the \\(y_{i}\\) are categorical/qualitative (e.g. education level, diagnosis)\nGoal: relate response \\(y_{i}\\) to the various predictors"
  },
  {
    "objectID": "slides-01-what-is.html#objectives-in-supervised-learning",
    "href": "slides-01-what-is.html#objectives-in-supervised-learning",
    "title": "What is statistical learning?",
    "section": "Objectives in Supervised Learning",
    "text": "Objectives in Supervised Learning\n\nExplanatory: understand which predictors affect the response, and how\nPrediction: accurately predict unobserved cases for new measurements of predictors\nAssessment: quantify the quality of our predictions and inference"
  },
  {
    "objectID": "slides-01-what-is.html#unsupervised-learning",
    "href": "slides-01-what-is.html#unsupervised-learning",
    "title": "What is statistical learning?",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n\nWe only observe the \\(\\mathbf{x}_{i}\\), but no associated response \\(y_{i}\\)\n“Unsupervised” because there is no response variable guiding the analysis!\nObjective may not be as clearly defined\nDifficult to assess how well your are doing"
  },
  {
    "objectID": "slides-01-what-is.html#lets-look-at-some-real-data",
    "href": "slides-01-what-is.html#lets-look-at-some-real-data",
    "title": "What is statistical learning?",
    "section": "Let’s look at some real data!",
    "text": "Let’s look at some real data!\n\nOribatid mite data: abundance data of 35 oribatid mite species observed at 70 sampling locations irregularly spaced within a study area of 2.6 × 10 m collected on the territory of the Station de biologie des Laurentides of Université de Montréal, Québec, Canada in June 1989\nVariables measured at each location:\n\nSubstrate density (quantitative)\nWater content (quantitative)\nMicrotopography (binary categorical)\nShrub density (ordinal categorical, three levels)\nSubstrate type (nominal categorical, seven levels)"
  },
  {
    "objectID": "slides-01-what-is.html#section",
    "href": "slides-01-what-is.html#section",
    "title": "What is statistical learning?",
    "section": "",
    "text": "# from the vegan library\ndata(\"mite\")\nmite %>%\n  head()\n\n  Brachy PHTH HPAV RARD SSTR Protopl MEGR MPRO TVIE HMIN HMIN2 NPRA TVEL ONOV\n1     17    5    5    3    2       1    4    2    2    1     4    1   17    4\n2      2    7   16    0    6       0    4    2    0    0     1    3   21   27\n3      4    3    1    1    2       0    3    0    0    0     6    3   20   17\n4     23    7   10    2    2       0    4    0    1    2    10    0   18   47\n5      5    8   13    9    0      13    0    0    0    3    14    3   32   43\n6     19    7    5    9    3       2    3    0    0   20    16    2   13   38\n  SUCT LCIL Oribatl1 Ceratoz1 PWIL Galumna1 Stgncrs2 HRUF Trhypch1 PPEL NCOR\n1    9   50        3        1    1        8        0    0        0    0    0\n2   12  138        6        0    1        3        9    1        1    1    2\n3   10   89        3        0    2        1        8    0        3    0    2\n4   17  108       10        1    0        1        2    1        2    1    3\n5   27    5        1        0    5        2        1    0        1    0    0\n6   39    3        5        0    1        1        8    0        4    0    1\n  SLAT FSET Lepidzts Eupelops Miniglmn LRUG PLAG2 Ceratoz3 Oppiminu Trimalc2\n1    0    0        0        0        0    0     0        0        0        0\n2    2    2        1        0        0    0     0        0        0        0\n3    0    8        0        0        0    0     0        0        0        0\n4    2   12        0        0        0    0     0        0        0        0\n5    0   12        2        0        0    0     0        0        0        0\n6    0   10        0        0        0    0     0        0        0        0"
  },
  {
    "objectID": "slides-01-what-is.html#section-1",
    "href": "slides-01-what-is.html#section-1",
    "title": "What is statistical learning?",
    "section": "",
    "text": "# from the vegan library\ndata(\"mite.env\")\nhead(mite.env)\n\n  SubsDens WatrCont Substrate Shrub    Topo\n1    39.18   350.15   Sphagn1   Few Hummock\n2    54.99   434.81    Litter   Few Hummock\n3    46.07   371.72 Interface   Few Hummock\n4    48.19   360.50   Sphagn1   Few Hummock\n5    23.55   204.13   Sphagn1   Few Hummock\n6    57.32   311.55   Sphagn1   Few Hummock\n\nmite_dat <- mite.env %>%\n  add_column(abundance = mite$LRUG)"
  },
  {
    "objectID": "slides-01-what-is.html#eda",
    "href": "slides-01-what-is.html#eda",
    "title": "What is statistical learning?",
    "section": "EDA",
    "text": "EDA\n(scroll for more content)"
  },
  {
    "objectID": "slides-01-what-is.html#section-2",
    "href": "slides-01-what-is.html#section-2",
    "title": "What is statistical learning?",
    "section": "",
    "text": "::: important Goal: ::: can we predict SUCT abundance using these variables?"
  },
  {
    "objectID": "slides-01-what-is.html#model-building",
    "href": "slides-01-what-is.html#model-building",
    "title": "What is statistical learning?",
    "section": "Model building",
    "text": "Model building\n\nGoal: predict LRUG abundance using these variables\nMaybe LRUG \\(\\approx f(\\) SubsDens + WatrCont\\()\\)?\nIf so, how would we represent these variables using our notation? i.e., what are \\(y_{i}\\) and \\(x_{i}\\)?\nThen our model can be written as \\(y_{i} = f(x_{i}) + \\epsilon_{i}\\) where \\(\\epsilon_{i}\\) represents random measurement error\n\nWhat does this equation mean?"
  },
  {
    "objectID": "slides-01-what-is.html#why-care-about-f",
    "href": "slides-01-what-is.html#why-care-about-f",
    "title": "What is statistical learning?",
    "section": "Why care about f?",
    "text": "Why care about f?\n\nModel (dropping the indices): \\(Y = f(X) + \\epsilon\\)\nThe function \\(f(X)\\) represents the systematic information that \\(X\\) tells us about \\(Y\\).\nIf \\(f\\) is “good”, then we can make reliable predictions of \\(Y\\) at new points \\(X = x\\)\nIf \\(f\\) is “good”, then we can identify which components of \\(X\\) are important to explaining \\(Y\\)\n\nDepending on \\(f\\), we may be able to learn how each component \\(X_{j}\\) of \\(X\\) affects \\(Y\\)"
  },
  {
    "objectID": "slides-01-what-is.html#why-care-about-f-1",
    "href": "slides-01-what-is.html#why-care-about-f-1",
    "title": "What is statistical learning?",
    "section": "Why care about f?",
    "text": "Why care about f?\n\nWe assume that \\(f\\) is fixed but unknown\nGoal of statistical learning: how to estimate \\(f\\)?\n\nSub-goals: prediction and inference\n\nThe sub-goal may affect the kind of \\(f\\) we choose"
  },
  {
    "objectID": "slides-01-what-is.html#prediction",
    "href": "slides-01-what-is.html#prediction",
    "title": "What is statistical learning?",
    "section": "Prediction",
    "text": "Prediction\n\nWe have a set of inputs or predictors \\(X\\), and we want to predict a corresponding \\(Y\\)\nAssuming the error \\(\\epsilon\\) is 0 on average, we can obtain predictions of \\(Y\\) as \\[\\hat{Y} = \\hat{f}(X)\\]\nIf we knew the true \\(Y\\), we could evaluate the accuracy of the prediction \\(\\hat{Y}\\)\nGenerally, \\(Y \\neq \\hat{Y}\\). Why?\n\n\\(\\hat{f}\\) will not be perfect estimate of \\(f\\)\n\\(Y\\) is a function of \\(\\epsilon\\), which cannot be predicted using \\(X\\)"
  },
  {
    "objectID": "slides-01-what-is.html#types-of-error",
    "href": "slides-01-what-is.html#types-of-error",
    "title": "What is statistical learning?",
    "section": "Types of error",
    "text": "Types of error\n\nModel: \\(Y = f(X) + \\epsilon\\)\nIrreducible error: \\(\\epsilon\\)\n\nEven if we knew \\(f\\) perfectly, there is still some inherent variability\n\\(\\epsilon\\) may also contained unmeasured variables that are not available to us\n\nReducible error: how far \\(\\hat{f}\\) is from the true \\(f\\)"
  },
  {
    "objectID": "slides-01-what-is.html#prediction-errors",
    "href": "slides-01-what-is.html#prediction-errors",
    "title": "What is statistical learning?",
    "section": "Prediction errors",
    "text": "Prediction errors\n\nWays to quantify error\n\nDifference/error = \\(Y - \\hat{Y}\\)\nAbsolute error = \\(|Y - \\hat{Y}|\\)\nSquared error = \\((Y - \\hat{Y})^2\\)\n\nIntuitively, large error indicates worse prediction"
  },
  {
    "objectID": "slides-01-what-is.html#prediction-errors-1",
    "href": "slides-01-what-is.html#prediction-errors-1",
    "title": "What is statistical learning?",
    "section": "Prediction errors",
    "text": "Prediction errors\n\nGiven \\(\\hat{f}\\) and \\(X\\), we can obtain a prediction \\(\\hat{Y} = \\hat{f}(X)\\) for \\(Y\\)\nMean-squared prediction error: \\[\\begin{align*}\n\\mathsf{E}[(Y - \\hat{Y})^2] &= \\mathsf{E}[( f(X) + \\epsilon - \\hat{f}(X))^2] \\\\\n&= \\underbrace{[f(X) - \\hat{f}(X)]^2}_\\text{reducible} + \\underbrace{\\text{Var}(\\epsilon)}_\\text{irreducible}\n\\end{align*}\\]\nWe cannot do much to decrease the irreducible error\nBut we can potentially minimize the reducible error by choosing better \\(\\hat{f}\\)!"
  },
  {
    "objectID": "slides-01-what-is.html#inference",
    "href": "slides-01-what-is.html#inference",
    "title": "What is statistical learning?",
    "section": "Inference",
    "text": "Inference\n\nWe are often interested in learning how \\(Y\\) and the \\(X_{1}, \\ldots, X_{p}\\) are related or associated\nIn this mindset, we want to estimate \\(f\\) to learn the relationships, rather than obtain a \\(\\hat{Y}\\)"
  },
  {
    "objectID": "slides-01-what-is.html#prediction-vs-inference",
    "href": "slides-01-what-is.html#prediction-vs-inference",
    "title": "What is statistical learning?",
    "section": "Prediction vs Inference",
    "text": "Prediction vs Inference\n\nIn the prediction setting, estimate \\(\\hat{f}\\) for the purpose of \\(\\hat{Y}\\) and \\(Y\\).\nIn the inference setting, estimate \\(\\hat{f}\\) for the purpose of \\(X\\) and \\(Y\\)\nSome problems will call for prediction, inference, or both\n\nTo what extent is LRUG abundance associated with microtopography?\nGiven a specific land profile, how many LRUG mites would we expect there to be?"
  },
  {
    "objectID": "slides-01-what-is.html#assessing-model-accuracy",
    "href": "slides-01-what-is.html#assessing-model-accuracy",
    "title": "What is statistical learning?",
    "section": "Assessing model accuracy",
    "text": "Assessing model accuracy\n\nNo single method or choice of \\(f\\) is superior over all possible data sets\nPrediction accuracy vs. interpretability\n\nMore restrictive models may be easier to interpret (better for inference)\nGood fit vs. over- or under-fit\n\nParsimony vs. black box\n\nA simpler model is often preferred over a very complex one"
  },
  {
    "objectID": "slides-01-what-is.html#assessing-model-accuracy-1",
    "href": "slides-01-what-is.html#assessing-model-accuracy-1",
    "title": "What is statistical learning?",
    "section": "Assessing model accuracy",
    "text": "Assessing model accuracy\n\nHow can we know how well a chosen \\(\\hat{f}\\) is performing?\nIn regression setting, we often use mean squared error (MSE)\n\n\\(\\text{MSE}=\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{f}(x_{i}))^2\\)\n\nMSE will be small if predictions \\(\\hat{y}_{i} = {f}(x_{i})\\) are very close to the true \\(y_{i}\\)"
  },
  {
    "objectID": "slides-01-what-is.html#training-vs.-test-data",
    "href": "slides-01-what-is.html#training-vs.-test-data",
    "title": "What is statistical learning?",
    "section": "Training vs. test data",
    "text": "Training vs. test data\n\nIn practice, we split our data into training and test sets\n\nTraining set is used to fit the model\nTest set is used to assess model fit\n\nWe are often most interested in accuracy of our predictions when applying the method to previously unseen data. Why?\nWe can compute the MSE for the training and test data respectively"
  },
  {
    "objectID": "slides-01-what-is.html#example-1",
    "href": "slides-01-what-is.html#example-1",
    "title": "What is statistical learning?",
    "section": "Example 1",
    "text": "Example 1"
  },
  {
    "objectID": "slides-01-what-is.html#example-2",
    "href": "slides-01-what-is.html#example-2",
    "title": "What is statistical learning?",
    "section": "Example 2",
    "text": "Example 2"
  },
  {
    "objectID": "slides-01-what-is.html#bias-variance-trade-off",
    "href": "slides-01-what-is.html#bias-variance-trade-off",
    "title": "What is statistical learning?",
    "section": "Bias-Variance trade-off",
    "text": "Bias-Variance trade-off\n\nAs model flexibility increases, the training MSE will decrease but test MSE may not.\nFlexible models may overfit the data, which leads to low train MSE and high test MSE\n\nThe supposed patterns in train data do not exist in test data\n\nLet us consider a test observation \\((x_{0}, y_{0})\\).\nThe expected test MSE for given \\(x_{0}\\) can be decomposed as follows:\n\n\\(\\mathsf{E}[(y_{0} - \\hat{f}(x_{0}))^2] = \\text{Var}(\\hat{f}(x_{0})) + [\\text{Bias}(\\hat{f}(x_{0}))]^2 + \\text{Var}(\\epsilon)\\)\n\\(\\text{Bias}(\\hat{f}(x_{0})) = \\mathsf{E}[\\hat{f}(x_{0})] - \\hat{f}(x_{0})\\)"
  },
  {
    "objectID": "slides-01-what-is.html#bias-variance-trade-off-cont.",
    "href": "slides-01-what-is.html#bias-variance-trade-off-cont.",
    "title": "What is statistical learning?",
    "section": "Bias-Variance trade-off (cont.)",
    "text": "Bias-Variance trade-off (cont.)"
  },
  {
    "objectID": "slides-01-what-is.html#classification",
    "href": "slides-01-what-is.html#classification",
    "title": "What is statistical learning?",
    "section": "Classification",
    "text": "Classification\n\nUp until now, we have focused on quantitative responses \\(y_{i}\\)\nWhat happens when \\(y_{i}\\) is qualitative? Examples include:\n\nMedical diagnosis: \\(\\mathcal{C} = \\{\\text{yes}, \\text{no}\\}\\)\nEducation level: \\(\\mathcal{C} = \\{\\text{high school}, \\text{college}, \\text{graduate}\\}\\)\n\nEach category in \\(\\mathcal{C}\\) is also known as a label\nIn this setting, we want our model to be classifier, i.e. given predictors \\(X\\), predict a label from the pool of all possible categories \\(\\mathcal{C}\\)"
  },
  {
    "objectID": "slides-01-what-is.html#classification-1",
    "href": "slides-01-what-is.html#classification-1",
    "title": "What is statistical learning?",
    "section": "Classification",
    "text": "Classification\n\nWe will still have to estimate \\(f\\)\n\\(\\hat{y}_{i}\\) is the predicted class label for observation \\(i\\) using estimate \\(\\hat{f}\\)\nHow to assess model accuracy? Error is more intuitive: we make an error if we predict the incorrect label, and no error otherwise\nThis can be represented using an indicator variable or function. \\(\\mathbf{I}(y_{i} = \\hat{y}_{i})\\):\n\n\\[\\mathbf{I}(y_{i} = \\hat{y}_{i}) = \\begin{cases} 1 & \\text{ if } y_{i} = \\hat{y}_{i}\\\\ 0 & \\text{ if } y_{i} \\neq \\hat{y}_{i} \\end{cases}\\]"
  },
  {
    "objectID": "slides-01-what-is.html#classification-error-rate",
    "href": "slides-01-what-is.html#classification-error-rate",
    "title": "What is statistical learning?",
    "section": "Classification error rate",
    "text": "Classification error rate\n\nTo quantify accuracy of estimated classifier \\(\\hat{f}\\), can calculate the error rate, which is the proportion of mistakes we make in labeling:\n\n\\[\\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{I}(y_{i} \\neq \\hat{y}_{i})\\]\n\nSmall error rate is preferred\nAs with MSE, can calculate the error rate for train and test data sets"
  },
  {
    "objectID": "slides-01-what-is.html#classifiers",
    "href": "slides-01-what-is.html#classifiers",
    "title": "What is statistical learning?",
    "section": "Classifiers",
    "text": "Classifiers\n\nHow do we choose which label to predict for a given observation?\nAssume we have a total of \\(J\\) possible labels in \\(\\mathcal{C}\\)\nFor a given observation \\(i\\), can calculate the following probability for each possible label \\(j\\): \\[p_{ij}(x_{i}) = Pr(y_{i} = j | X = x_{i})\\]\nThese probabilities are called conditional class probabilities at \\(x_{i}\\)"
  },
  {
    "objectID": "slides-01-what-is.html#bayes-optimal-classifier",
    "href": "slides-01-what-is.html#bayes-optimal-classifier",
    "title": "What is statistical learning?",
    "section": "Bayes optimal classifier",
    "text": "Bayes optimal classifier\n\nThe Bayes optimal classifier will assign/predict the label which has the largest conditional class probability\n\nIt can be shown that the test error rate \\(\\frac{1}{n_{test}} \\sum_{i=1}^{n_{test}} \\mathbf{I}(y_{i} \\neq \\hat{y}_{i})\\) is minimized when using the Bayes optimal classifier\n\nFor example, consider a binary problem with levels “yes” and “no”.\nFor observation \\(i\\), if \\(Pr(y_{i} = \\text{yes} | X = x_{i}) > 0.5\\), then \\(\\hat{y}_{i} =\\) “yes”.\nThe \\(x_{i}\\) where \\(Pr(y_{i} = \\text{yes} | X = x_{i}) = Pr(y_{i} = \\text{no} | X = x_{i})= 0.5\\) is called the Bayes decision boundary"
  },
  {
    "objectID": "slides-01-what-is.html#example",
    "href": "slides-01-what-is.html#example",
    "title": "What is statistical learning?",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "slides-01-what-is.html#classification-2",
    "href": "slides-01-what-is.html#classification-2",
    "title": "What is statistical learning?",
    "section": "Classification",
    "text": "Classification\n\nBayes classifier is “gold standard”\nIn practice, we cannot compute \\(p_{ij}(x_{i}) = Pr(y_{i} = j | X = x_{i})\\) because we do know the conditional distribution of \\(y\\) given \\(x\\)\nInstead, we need to estimate these \\(p_{ij}(x_{i})\\)\nAlmost all of our choices of \\(\\hat{f}\\) will output these estimates"
  },
  {
    "objectID": "slides-01-what-is.html#classification-in-practice",
    "href": "slides-01-what-is.html#classification-in-practice",
    "title": "What is statistical learning?",
    "section": "Classification in practice",
    "text": "Classification in practice\n\nBayes classifier is “gold standard”\nIn practice, we cannot compute the \\(p_{ij}(x_{i})\\) exactly because we do know the conditional distribution of \\(y\\) given \\(x\\)\nInstead, we need to estimate these \\(p_{ij}(x_{i})\\)\nAlmost all of our choices of \\(\\hat{f}\\) will output these estimates"
  },
  {
    "objectID": "slides-01-what-is.html#sampling-map",
    "href": "slides-01-what-is.html#sampling-map",
    "title": "What is statistical learning?",
    "section": "Sampling map",
    "text": "Sampling map"
  }
]