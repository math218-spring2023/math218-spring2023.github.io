[
  {
    "objectID": "labs/lab-01-roulette.html",
    "href": "labs/lab-01-roulette.html",
    "title": "Lab 01: Roulette",
    "section": "",
    "text": "We will simulate the casino game of American roulette! On an American roulette wheel, there are 38 equally-sized spaces that differ in color and number value:\n\nTwo spaces are green, and are labelled 0 and 00\nThe remaining spaces alternate red and black and take on the values 1-36\n\nThere is no particular order to the numbers\n\n\nThe game is simple: a ball is dropped and spins along the roulette until it settles in one of the 38 spaces. Each space has an equal chance of being landed on.\n\n\n\n\n\nDiffering wagers or bets can be made on where the ball will end up landing. For example, it is common to bet on a color. If you “bet on black”, you think the ball will land in a black-colored space. Or you might “bet on events”, which means you think the ball will land in a space with an even number greater than 0.\nYou wager some money when you make a bet. For example, if I bet $5 and I win, I will get my original $5 back and also gain $5 from the house. So I net $5. If I lose, then the house takes my $5 and I net -$5. In summary:\n\nIf I win, I net the amount that I bet\nIf I lose, I net the negative amount that I bet\n\nIn either case, your net gain will be either positive or negative.\nBecause this is a casino game, we know that the house always wins. So, the purpose of this assignment is to simulate/demonstrate that the player will always lose money in the long run."
  },
  {
    "objectID": "labs/lab-01-roulette.html#assignment",
    "href": "labs/lab-01-roulette.html#assignment",
    "title": "Lab 01: Roulette",
    "section": "Assignment",
    "text": "Assignment\nImagine that you have unlimited funds and will play roulette n number of times. You can choose any integer value of n greater than 200, but be sure to store it as a variable for reproducibility. You will always bet on red and wager $5. For each one of the n rounds, keep track of the net gain.\nAt the end of your gambling, obtain and report the sum and average of your net earnings.\nOptional: make a plot of a running total of your net gains (i.e. the x-axis represents the iteration/round of betting, and the y-axis represents the total gains up to and including that round).\nPlease clone the Lab 01 GitHub repository and work in the lab-01-roulette.Rmd file provided for you.\n\nDetails\nFor simplicity, you can assume that the red spaces take on the values (1, 2, …, 18) and the black spaces take on the values (19, 20, …, 36). You can decide how you would like to represent the two green spaces.\nHere is some structure that might help you design your code: you will need to create a for loop where on every iteration you should:\n\n“Spin” the roulette\nEvaluate how the outcome of your spin compares to your bet\nMake note of the net gain on that iteration\n\n\n\nConcepts used\n\nfor loops\nConditional statements\nComparing R objects\nCreating and modifying vectors\nWorking with GitHub: try to regularly commit your changes and push them back to GitHub!"
  },
  {
    "objectID": "labs/lab-01-roulette.html#submission-details",
    "href": "labs/lab-01-roulette.html#submission-details",
    "title": "Lab 01: Roulette",
    "section": "Submission details",
    "text": "Submission details\nKnit, commit, and push your project back to github.com using the GitHub Desktop client.\nSubmit your knitted PDF onto Canvas. You may need to install the tinytex package to knit to PDF."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Intro to base R\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nImportant coding skills\n\n\n\nFeb 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCreating functions\n\n\n\nFeb 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nLinear regression\n\n\n\nFeb 23, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "assignments.html#labs",
    "href": "assignments.html#labs",
    "title": "Assignments",
    "section": "Labs",
    "text": "Labs\n\n\n\n\n\n\n\nLab 01: Roulette\n\n\nCoding roulette\n\n\n\n\n\n\n\n\n\n\nLab 02: Linear regression\n\n\nMoneyball\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "assignments.html#implementations",
    "href": "assignments.html#implementations",
    "title": "Assignments",
    "section": "Implementations",
    "text": "Implementations\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 218: Spring 2023",
    "section": "",
    "text": "Warner 104, TRF 8:40-9:30AM Eastern\nProfessor Becky Tang\n\nOffice: Warner 214\nOffice hours: Tuesdays 3-4pm\nSchedule 1:1 meetings via Calendly\n\n\nVisit the schedule page to see the current course schedule, lecture notes, and due dates.\nVisit the assignments page to see the current list of assignments."
  },
  {
    "objectID": "index.html#materials",
    "href": "index.html#materials",
    "title": "Math 218: Spring 2023",
    "section": "Materials",
    "text": "Materials\nThere is no required textbook for this course.\nYou should have a fully-charged laptop, tablet with keyboard, or comparable device to every class.\nYou should also have R and RStudio installed on your machine. If you do not have either, please follow these instructions."
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "Math 218: Spring 2023",
    "section": "Syllabus",
    "text": "Syllabus\nThe course syllabus can be found here. Please note that the once the semester begins, the schedule on the syllabus may not be up-to-date. Refer to the schedule page to see the current course schedule."
  },
  {
    "objectID": "slides/slides-01-what-is.html#what-is-statistical-learning",
    "href": "slides/slides-01-what-is.html#what-is-statistical-learning",
    "title": "What is statistical learning?",
    "section": "What is statistical learning?",
    "text": "What is statistical learning?\n\nSet of tools used to understand data\n\nSupervised and unsupervised methods\n\nUse data and build appropriate functions (models) to try and perform inference and make predictions\n\nData-centered approach\n\nCategories of statistical learning problems\n\nClassification\nLearning relationships\nPrediction"
  },
  {
    "objectID": "slides/slides-01-what-is.html#supervised-learning",
    "href": "slides/slides-01-what-is.html#supervised-learning",
    "title": "What is statistical learning?",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n\nNotation: let \\(i = 1,\\ldots, n\\) index the observation\nFor each observation \\(i\\), we have:\n\nResponse (outcome): \\(y_{i}\\)\nVector of \\(p\\) predictors (covariates): \\(\\mathbf{x}_{i} = (x_{i1}, x_{i2}, \\ldots, x_{ip})'\\)\n\nRegression: the \\(y_{i}\\) are quantitative (e.g. height, price)\nClassification: the \\(y_{i}\\) are categorical (e.g. education level, diagnosis)\nGoal: relate response \\(y_{i}\\) to the various predictors"
  },
  {
    "objectID": "slides/slides-01-what-is.html#objectives-in-supervised-learning",
    "href": "slides/slides-01-what-is.html#objectives-in-supervised-learning",
    "title": "What is statistical learning?",
    "section": "Objectives in Supervised Learning",
    "text": "Objectives in Supervised Learning\n\nExplanatory: understand which predictors affect the response, and how\nPrediction: accurately predict unobserved cases for new measurements of predictors\nAssessment: quantify the quality of our predictions and inference"
  },
  {
    "objectID": "slides/slides-01-what-is.html#lets-look-at-some-real-data",
    "href": "slides/slides-01-what-is.html#lets-look-at-some-real-data",
    "title": "What is statistical learning?",
    "section": "Let’s look at some real data!",
    "text": "Let’s look at some real data!\n\nOribatid mite data: abundance data of 35 oribatid mite species observed at 70 sampling locations irregularly spaced within a study area of 2.6 × 10 m collected on the territory of the Station de biologie des Laurentides of Université de Montréal, Québec, Canada in June 1989\nVariables measured at each location:\n\nSubstrate density (quantitative)\nWater content (quantitative)\nMicrotopography (binary categorical)\nShrub density (ordinal categorical, three levels)\nSubstrate type (nominal categorical, seven levels)"
  },
  {
    "objectID": "slides/slides-01-what-is.html#sampling-map",
    "href": "slides/slides-01-what-is.html#sampling-map",
    "title": "What is statistical learning?",
    "section": "Sampling map",
    "text": "Sampling map"
  },
  {
    "objectID": "slides/slides-01-what-is.html#data",
    "href": "slides/slides-01-what-is.html#data",
    "title": "What is statistical learning?",
    "section": "Data",
    "text": "Data\n\n# from the vegan library\ndata(\"mite\")\ndata(\"mite.env\")\nnames(mite)\n\n [1] \"Brachy\"   \"PHTH\"     \"HPAV\"     \"RARD\"     \"SSTR\"     \"Protopl\" \n [7] \"MEGR\"     \"MPRO\"     \"TVIE\"     \"HMIN\"     \"HMIN2\"    \"NPRA\"    \n[13] \"TVEL\"     \"ONOV\"     \"SUCT\"     \"LCIL\"     \"Oribatl1\" \"Ceratoz1\"\n[19] \"PWIL\"     \"Galumna1\" \"Stgncrs2\" \"HRUF\"     \"Trhypch1\" \"PPEL\"    \n[25] \"NCOR\"     \"SLAT\"     \"FSET\"     \"Lepidzts\" \"Eupelops\" \"Miniglmn\"\n[31] \"LRUG\"     \"PLAG2\"    \"Ceratoz3\" \"Oppiminu\" \"Trimalc2\"\n\n# Focus on just the LRUG mite abundances\nmite_dat <- mite.env %>%\n  add_column(abundance = mite$LRUG)\nhead(mite_dat)\n\n  SubsDens WatrCont Substrate Shrub    Topo abundance\n1    39.18   350.15   Sphagn1   Few Hummock         0\n2    54.99   434.81    Litter   Few Hummock         0\n3    46.07   371.72 Interface   Few Hummock         0\n4    48.19   360.50   Sphagn1   Few Hummock         0\n5    23.55   204.13   Sphagn1   Few Hummock         0\n6    57.32   311.55   Sphagn1   Few Hummock         0"
  },
  {
    "objectID": "slides/slides-01-what-is.html#eda",
    "href": "slides/slides-01-what-is.html#eda",
    "title": "What is statistical learning?",
    "section": "EDA",
    "text": "EDA\n(scroll for more content)"
  },
  {
    "objectID": "slides/slides-01-what-is.html#model-building",
    "href": "slides/slides-01-what-is.html#model-building",
    "title": "What is statistical learning?",
    "section": "Model building",
    "text": "Model building\n\nGoal: predict LRUG abundance using these variables\nMaybe LRUG \\(\\approx f(\\) SubsDens + WatrCont\\()\\)?\nIf so, how would we represent these variables using our notation? i.e., what are \\(y_{i}\\) and \\(x_{i}\\)?\nThen our model can be written as \\(y_{i} = f(x_{i}) + \\epsilon_{i}\\) where \\(\\epsilon_{i}\\) represents random measurement error\n\nWhat does this equation mean?"
  },
  {
    "objectID": "slides/slides-01-what-is.html#why-care-about-f",
    "href": "slides/slides-01-what-is.html#why-care-about-f",
    "title": "What is statistical learning?",
    "section": "Why care about f?",
    "text": "Why care about f?\n\nModel (dropping the indices): \\(Y = f(X) + \\epsilon\\)\nThe function \\(f(X)\\) represents the systematic information that \\(X\\) tells us about \\(Y\\).\nIf \\(f\\) is “good”, then we can make reliable predictions of \\(Y\\) at new points \\(X = x\\)\nIf \\(f\\) is “good”, then we can identify which components of \\(X\\) are important for explaining \\(Y\\)\n\nDepending on \\(f\\), we may be able to learn how each component of \\(X\\) affects \\(Y\\)"
  },
  {
    "objectID": "slides/slides-01-what-is.html#why-care-about-f-1",
    "href": "slides/slides-01-what-is.html#why-care-about-f-1",
    "title": "What is statistical learning?",
    "section": "Why care about f?",
    "text": "Why care about f?\n\nWe assume that \\(f\\) is fixed but unknown\nGoal of statistical learning: how to obtain an estimate \\(\\hat{f}\\) of the true \\(f\\)?\n\nSub-goals: prediction and inference\n\nThe sub-goal may affect our choice of \\(\\hat{f}\\)"
  },
  {
    "objectID": "slides/slides-01-what-is.html#prediction",
    "href": "slides/slides-01-what-is.html#prediction",
    "title": "What is statistical learning?",
    "section": "Prediction",
    "text": "Prediction\n\nWe have a set of inputs or predictors \\(x_{i}\\), and we want to predict a corresponding \\(y_{i}\\). Assume the true model is \\(y_{i} = f(x_{i}) + \\epsilon_{i}\\), but don’t know \\(f\\)\nAssuming the error \\(\\epsilon_{i}\\) is 0 on average, we can obtain predictions of \\(y_{i}\\) as \\[\\hat{y}_{i} = \\hat{f}(x_{i})\\]\n\nThen, if we know the true \\(y_{i}\\), we can evaluate the accuracy of the prediction \\(\\hat{y}_{i}\\)\n\nGenerally, \\(y_{i} \\neq \\hat{y}_{i}\\). Why?\n\n\\(\\hat{f}\\) will not be perfect estimate of \\(f\\)\n\\(y_{i}\\) is a function of \\(\\epsilon_{i}\\), which cannot be predicted using \\(x_{i}\\)"
  },
  {
    "objectID": "slides/slides-01-what-is.html#types-of-error",
    "href": "slides/slides-01-what-is.html#types-of-error",
    "title": "What is statistical learning?",
    "section": "Types of error",
    "text": "Types of error\n\nModel: \\(y_{i} = f(x_{i}) + \\epsilon_{i}\\)\n\nIrreducible error: \\(\\epsilon_{i}\\)\n\n\nEven if we knew \\(f\\) perfectly, there is still some inherent variability\n\\(\\epsilon_{i}\\) may also contain unmeasured variables that are not available to us\n\n\nReducible error: how far \\(\\hat{f}\\) is from the true \\(f\\)"
  },
  {
    "objectID": "slides/slides-01-what-is.html#prediction-errors",
    "href": "slides/slides-01-what-is.html#prediction-errors",
    "title": "What is statistical learning?",
    "section": "Prediction errors",
    "text": "Prediction errors\n\nWays to quantify error\n\nDifference/error = \\(y_{i} - \\hat{y}_{i}\\)\nAbsolute error = \\(|y_{i} - \\hat{y}_{i}|\\)\nSquared error = \\((y_{i} - \\hat{y}_{i})^2\\)\n\nIntuitively, larger error indicates worse prediction\nQuestion: are there scenarios where we might prefer one error over another?"
  },
  {
    "objectID": "slides/slides-01-what-is.html#prediction-errors-1",
    "href": "slides/slides-01-what-is.html#prediction-errors-1",
    "title": "What is statistical learning?",
    "section": "Prediction errors",
    "text": "Prediction errors\n\nGiven \\(\\hat{f}\\) and \\(x_{i}\\), we can obtain a prediction \\(\\hat{y}_{i} = \\hat{f}(x_{i})\\) for \\(y_{i}\\)\nMean-squared prediction error: \\[\\begin{align*}\n\\mathsf{E}[(y_{i} - \\hat{y}_{i})^2] &= \\mathsf{E}[( f(x_{i}) + \\epsilon_{i} - \\hat{f}(x_{i}))^2] \\\\\n&= \\underbrace{[f(x_{i}) - \\hat{f}(x_{i})]^2}_\\text{reducible} + \\underbrace{\\text{Var}(\\epsilon_{i})}_\\text{irreducible}\n\\end{align*}\\]\nWe cannot do much to decrease the irreducible error\nBut we can potentially minimize the reducible error by choosing better \\(\\hat{f}\\)!"
  },
  {
    "objectID": "slides/slides-01-what-is.html#inference",
    "href": "slides/slides-01-what-is.html#inference",
    "title": "What is statistical learning?",
    "section": "Inference",
    "text": "Inference\n\nWe are often interested in learning how \\(Y\\) and the \\(X_{1}, \\ldots, X_{p}\\) are related or associated\nIn this mindset, we want to estimate \\(f\\) to learn the relationships, rather than obtain a \\(\\hat{Y}\\)"
  },
  {
    "objectID": "slides/slides-01-what-is.html#prediction-vs-inference",
    "href": "slides/slides-01-what-is.html#prediction-vs-inference",
    "title": "What is statistical learning?",
    "section": "Prediction vs Inference",
    "text": "Prediction vs Inference\n\nPrediction: estimate \\(\\hat{f}\\) for the purpose of \\(\\hat{Y}\\) and \\(Y\\).\nInference: estimate \\(\\hat{f}\\) for the purpose of \\(X\\) and \\(Y\\)\nSome problems will call for prediction, inference, or both\n\nTo what extent is LRUG abundance associated with microtopography?\nGiven a specific land profile, how many LRUG mites would we expect there to be?"
  },
  {
    "objectID": "slides/slides-01-what-is.html#assessing-model-accuracy",
    "href": "slides/slides-01-what-is.html#assessing-model-accuracy",
    "title": "What is statistical learning?",
    "section": "Assessing model accuracy",
    "text": "Assessing model accuracy\n\nNo single method or choice of \\(\\hat{f}\\) is superior over all possible data sets\nPrediction accuracy vs. interpretability\n\nMore restrictive models may be easier to interpret (better for inference)\nGood fit vs. over-fit (or under-fit)\n\nA simpler model is often preferred over a very complex one"
  },
  {
    "objectID": "slides/slides-01-what-is.html#assessing-model-accuracy-1",
    "href": "slides/slides-01-what-is.html#assessing-model-accuracy-1",
    "title": "What is statistical learning?",
    "section": "Assessing model accuracy",
    "text": "Assessing model accuracy\n\nHow can we know how well a chosen \\(\\hat{f}\\) is performing?\nIn regression setting, we often use mean squared error (MSE) or root MSE (RMSE)\n\n\\(\\text{MSE}=\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{f}(x_{i}))^2\\)\n\\(\\text{RMSE}=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{f}(x_{i}))^2}\\)\n\nMSE (and RMSE) will be small if predictions \\(\\hat{y}_{i} = \\hat{f}(x_{i})\\) are very close to true \\(y_{i}\\)\nQuestion: why might we prefer reporting RMSE over MSE?"
  },
  {
    "objectID": "slides/slides-01-what-is.html#training-vs.-test-data",
    "href": "slides/slides-01-what-is.html#training-vs.-test-data",
    "title": "What is statistical learning?",
    "section": "Training vs. test data",
    "text": "Training vs. test data\n\nIn practice, we split our data into training and test sets\n\nTraining set is used to fit the model\nTest set is used to assess model fit\n\nWe are often most interested in accuracy of our predictions when applying the method to previously unseen data. Why?\nWe can compute the MSE for the training and test data respectively…but we typically focus more attention to test MSE"
  },
  {
    "objectID": "slides/slides-01-what-is.html#example-1",
    "href": "slides/slides-01-what-is.html#example-1",
    "title": "What is statistical learning?",
    "section": "Example 1",
    "text": "Example 1\nI generated some fake data and fit three models that differ in flexibility. In this example, the generated data (points) follow a curve-y shape."
  },
  {
    "objectID": "slides/slides-01-what-is.html#example-2",
    "href": "slides/slides-01-what-is.html#example-2",
    "title": "What is statistical learning?",
    "section": "Example 2",
    "text": "Example 2\nIn this example, the generated data (points) look more linear."
  },
  {
    "objectID": "slides/slides-01-what-is.html#bias-variance-trade-off",
    "href": "slides/slides-01-what-is.html#bias-variance-trade-off",
    "title": "What is statistical learning?",
    "section": "Bias-Variance trade-off",
    "text": "Bias-Variance trade-off\n\nAs model flexibility increases, the training MSE will decrease but test MSE may not.\nFlexible models may overfit the data, which leads to low train MSE and high test MSE\n\nThe supposed patterns in train data do not exist in test data\n\nLet us consider a test observation \\((x_{0}, y_{0})\\).\nThe expected test MSE for given \\(x_{0}\\) can be decomposed as follows:\n\n\\(\\mathsf{E}[(y_{0} - \\hat{f}(x_{0}))^2] = \\text{Var}(\\hat{f}(x_{0})) + [\\text{Bias}(\\hat{f}(x_{0}))]^2 + \\text{Var}(\\epsilon)\\)\n\\(\\text{Bias}(\\hat{f}(x_{0})) = \\mathsf{E}[\\hat{f}(x_{0})] - \\hat{f}(x_{0})\\)"
  },
  {
    "objectID": "slides/slides-01-what-is.html#bias-variance-trade-off-cont.",
    "href": "slides/slides-01-what-is.html#bias-variance-trade-off-cont.",
    "title": "What is statistical learning?",
    "section": "Bias-Variance trade-off (cont.)",
    "text": "Bias-Variance trade-off (cont.)"
  },
  {
    "objectID": "slides/slides-00-welcome.html#about-me",
    "href": "slides/slides-00-welcome.html#about-me",
    "title": "Welcome to statistical learning!",
    "section": "About me",
    "text": "About me\n\nPhD in Statistical Science from Duke University, BA in Mathematics and Computer Science from Swarthmore College\nResearch interests: Bayesian hierarchical models for ecological applications\n\nDeveloping models for single species or community abundances\n\nOffice: Warner 214\n\nIf my door is open, come on in! Also feel free to e-mail me.\nOffice hours: Tuesdays 3-4pm and by appointment via Calendly\n\nCurrent hobbies: running, mushroom foraging\nAspirational hobbies/skills: fly fishing, driving stick shift"
  },
  {
    "objectID": "slides/slides-00-welcome.html#about-you",
    "href": "slides/slides-00-welcome.html#about-you",
    "title": "Welcome to statistical learning!",
    "section": "About you",
    "text": "About you\n\nIntroduce yourself using any or all of the following (the first is mandatory):\n\nName\nYear\nMajor/minor\nHobbies\nHow do you take your coffee?"
  },
  {
    "objectID": "slides/slides-00-welcome.html#about-the-course",
    "href": "slides/slides-00-welcome.html#about-the-course",
    "title": "Welcome to statistical learning!",
    "section": "About the course",
    "text": "About the course\n\nCourse website: https://math218-spring2023.github.io/ (please bookmark!)\nLearn various models for regression and classification tasks (more on this next lecture)\n\nLinear and logistic regression, KNN, decision trees + variants, K-means and hierarchical clustering"
  },
  {
    "objectID": "slides/slides-00-welcome.html#necessary-background",
    "href": "slides/slides-00-welcome.html#necessary-background",
    "title": "Welcome to statistical learning!",
    "section": "Necessary background",
    "text": "Necessary background\n\n\n\n\nI assume you have taken Math 118 prior to this course, and are comfortable with tidyverse and RStudio. There is a large emphasis on computing.\n\nI also assume you are comfortable with knitting. In this course, I ask that you knit to PDF.\n\nWe will learn how to code in base R, and by the end of the course you should feel comfortable switching between base R and tidyverse.\nWe will focus more on applications and developing intuition. The goal is to begin developing a toolbox of methods that you may use in future analyses."
  },
  {
    "objectID": "slides/slides-00-welcome.html#class-meetings",
    "href": "slides/slides-00-welcome.html#class-meetings",
    "title": "Welcome to statistical learning!",
    "section": "Class Meetings",
    "text": "Class Meetings\n\n\nLecture\n\nFocus on concepts behind statistical learning techniques\nInteractive lecture that includes examples and hands-on exercises\nBring fully-charged laptop to every lecture\n\nPlease let me know if you do not have access to a laptop\n\n\nLab\n\nTypically occurs on Fridays\nFocus on computing using functions provided in R packages\nApply concepts from lecture to case study scenarios\n\nImplementation\n\nSome days will be focused on implementing (i.e. coding by hand) methods discussed in lecture\nComplete in small groups"
  },
  {
    "objectID": "slides/slides-00-welcome.html#major-assessments",
    "href": "slides/slides-00-welcome.html#major-assessments",
    "title": "Welcome to statistical learning!",
    "section": "Major assessments",
    "text": "Major assessments\n\n\nOne midterm with two components:\n\nComputational component (take-home)\nOral component\n\nFinal project\n\nGroups of 3-4 students (tentatively)\nPresentations during last two days of class*\nNO sit-down final"
  },
  {
    "objectID": "slides/slides-00-welcome.html#grading",
    "href": "slides/slides-00-welcome.html#grading",
    "title": "Welcome to statistical learning!",
    "section": "Grading",
    "text": "Grading\nAssignments\n\nLabs (30%)\nImplementation deliverables (20%)\nMidterm (20%)\nFinal project (25%)\nParticipation (5%)"
  },
  {
    "objectID": "slides/slides-00-welcome.html#important-dates",
    "href": "slides/slides-00-welcome.html#important-dates",
    "title": "Welcome to statistical learning!",
    "section": "Important dates",
    "text": "Important dates\n\nFriday, 3/31: take-home midterm\nMonday, 4/3: oral midterm\n\nFriday, 4/14: Spring symposium (no class)\n\nMonday, 4/17: final day to drop classes :(\n\nFriday, 5/13 and Monday, 5/15*: project presentations"
  },
  {
    "objectID": "slides/slides-00-welcome.html#excused-absences",
    "href": "slides/slides-00-welcome.html#excused-absences",
    "title": "Welcome to statistical learning!",
    "section": "Excused Absences",
    "text": "Excused Absences\n\nStudents who miss a class due to a scheduled varsity trip, religious holiday, or short-term illness should fill out the respective form.\n\nThese excused absences do not excuse you from assigned work.\n\nIf you have a personal or family emergency or chronic health condition that affects your ability to participate in class, please contact your academic dean’s office.\nExam dates cannot be changed and no make-up exams will be given."
  },
  {
    "objectID": "slides/slides-00-welcome.html#late-work-and-regrade-requests",
    "href": "slides/slides-00-welcome.html#late-work-and-regrade-requests",
    "title": "Welcome to statistical learning!",
    "section": "Late Work and Regrade Requests",
    "text": "Late Work and Regrade Requests\n\nHomework assignments:\n\nAfter the assigned deadline, there is a 10% penalty for each day the assignment is late\nPlease communicate with me early if you will need a homework extension!\n\nLate work will not be accepted for the midterm or final project.\nRegrade requests must be submitted within one week of when the assignment is returned"
  },
  {
    "objectID": "slides/slides-00-welcome.html#academic-honesty-and-reusing-code",
    "href": "slides/slides-00-welcome.html#academic-honesty-and-reusing-code",
    "title": "Welcome to statistical learning!",
    "section": "Academic Honesty and Reusing Code",
    "text": "Academic Honesty and Reusing Code\n\nAll work for this class should be done in accordance with the Middlebury Honor code. Any violations will automatically result in a grade of 0 on the assignment and will be reported.\nUnless explicitly stated otherwise, you may make use of online resources (e.g. StackOverflow) for coding examples on assignments. If you directly use code from an outside source (or use it as inspiration), you must or explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nOn individual assignments, you may discuss the assignment with one another; however, you may not directly share code or write up with other students. This includes copy-and-paste sharing, as well as showing your screen with the code displayed to another student.\nOn team assignments, you may not directly share code or write up with another team. Unauthorized sharing of the code or write up will be considered a violation for all students involved.\nChatGPT most likely will not be useful in this class. However, if you use it on an assignment, please let me know in what capacity you used it by including a comment in your assignment."
  },
  {
    "objectID": "slides/slides-00-welcome.html#inclusion",
    "href": "slides/slides-00-welcome.html#inclusion",
    "title": "Welcome to statistical learning!",
    "section": "Inclusion",
    "text": "Inclusion\n\nIn this course, we will strive to create a learning environment that is welcoming to all students. If there is any aspect of the class that is not welcoming or accessible to you, please let me know immediately.\nAdditionally, if you are experiencing something outside of class that is affecting your performance in the course, please feel free to talk with me and/or your academic dean."
  },
  {
    "objectID": "slides/slides-00-welcome.html#create-a-github-account",
    "href": "slides/slides-00-welcome.html#create-a-github-account",
    "title": "Welcome to statistical learning!",
    "section": "Create a GitHub account",
    "text": "Create a GitHub account\n\nGo to https://github.com, and create an account (unless you already have one). After you create your account, click here and enter your GitHub username.\n\nTips for creating a username from Happy Git with R.\n\nIncorporate your actual name!\nReuse your username from other contexts if you can.\nShorter is better than longer; be as unique as possible in as few characters as possible.\nAvoid words laden with special meaning in programming, like NA."
  },
  {
    "objectID": "slides/slides-00-welcome.html#coding-exercise",
    "href": "slides/slides-00-welcome.html#coding-exercise",
    "title": "Welcome to statistical learning!",
    "section": "“Coding” exercise",
    "text": "“Coding” exercise\nLet’s create the following plot together:"
  },
  {
    "objectID": "slides/slides-00-welcome.html#data",
    "href": "slides/slides-00-welcome.html#data",
    "title": "Welcome to statistical learning!",
    "section": "Data",
    "text": "Data\n\ncat_lovers %>%\n  datatable(options = list(pageLength = 5))"
  },
  {
    "objectID": "slides/slides-00-welcome.html#playing-with-base-r",
    "href": "slides/slides-00-welcome.html#playing-with-base-r",
    "title": "Welcome to statistical learning!",
    "section": "Playing with base R",
    "text": "Playing with base R\n\nCreate a folder on your desktop called Math218\nOpen RStudio and create a new Rmarkdown document.\n\nWe will work through some coding exercises. The associated code can be found in “Live Code 01”"
  },
  {
    "objectID": "slides/slides-00-installation.html#version-control",
    "href": "slides/slides-00-installation.html#version-control",
    "title": "Installation",
    "section": "Version control",
    "text": "Version control\n\nGit is a version control system – like “Track Changes” features from Microsoft Word.\nGitHub is the home for your Git-based projects on the internet (like DropBox but much better).\nThere are a lot of Git commands and very few people know them all. 99% of the time you will use git to add, commit, push, and pull.\nWe will be using the GitHub Desktop application to perform the Git commands in a beginner-friendly way. Our local project in RStudio will be sent to GitHub using the application."
  },
  {
    "objectID": "slides/slides-00-installation.html#download-instructions",
    "href": "slides/slides-00-installation.html#download-instructions",
    "title": "Installation",
    "section": "Download Instructions",
    "text": "Download Instructions\n\nAccept the e-mailed GitHub invitation to join our course organization. If you did not receive an invitation, that means I do not have your GitHub username!\nGo to https://desktop.github.com/ and download the GitHub Desktop application. Mac users: check if you need to download for Apple silicon Mac.\nA zipped file will appear in your Downloads folder. Please unzip the file.\nA purple GitHub Desktop icon with the octocat icon will appear. I suggest dragging this to your dock.\n\n\n\nOpen GitHub Desktop. At some point, you might get the following pop-up:\n\n\nIf so, check the box and choose the white “Not Now” button"
  },
  {
    "objectID": "slides/slides-00-installation.html#syncing-your-github-account",
    "href": "slides/slides-00-installation.html#syncing-your-github-account",
    "title": "Installation",
    "section": "Syncing your GitHub Account",
    "text": "Syncing your GitHub Account\n\nNow we need to sync your GitHub Desktop application with your GitHub account\n\nMac users: GitHub Desktop -> Preference -> Accounts -> Sign-in\nWindows users: File -> Options -> Accounts -> Sign-in\n\nYou will be directed to github.com to sign-in. Enter your account information. Once authenticated, your GitHub Desktop client should be set up!"
  },
  {
    "objectID": "slides/slides-00-installation.html#cloning-a-github-repo",
    "href": "slides/slides-00-installation.html#cloning-a-github-repo",
    "title": "Installation",
    "section": "Cloning a GitHub repo",
    "text": "Cloning a GitHub repo\n\nIn this course, I will create your GitHub repositories (i.e. projects) for you. I have a master repo, and I distribute copies to each one of you that only you (and in the future, your group members) can access\nGo to the course organization on GitHub (either via github.com or by clicking on the octocat on the course website)\nFind the repo with the prefix lab-01-roulette-"
  },
  {
    "objectID": "slides/slides-00-installation.html#cloning-a-github-repo-cont.",
    "href": "slides/slides-00-installation.html#cloning-a-github-repo-cont.",
    "title": "Installation",
    "section": "Cloning a GitHub repo (cont.)",
    "text": "Cloning a GitHub repo (cont.)\n\nClick on the green Code button, and select the option Open with GitHub Desktop\n\nThe GitHub Desktop application will open up, with a white window that says “Clone a Repository”. Important: in the second line that says “Local Path”, there is a button that says Choose… Click on it, and select the Math 218 folder you created from this course. Then hit the blue Clone button."
  },
  {
    "objectID": "slides/slides-00-installation.html#committing-and-pushing-changes",
    "href": "slides/slides-00-installation.html#committing-and-pushing-changes",
    "title": "Installation",
    "section": "Committing and pushing changes",
    "text": "Committing and pushing changes\n\nWhen you work on a project locally (i.e. on your own machine), you will want to periodically “back-up” your changes in case something terrible happens to your laptop, or you need to share your progress with a team member\nThe process is done in stages:\n\nAdding your changes,\nCommiting your changes, and\nPushing your changes\n\nGitHub Desktop automatically does the add step for you, but you need to explicitly commit and push!"
  },
  {
    "objectID": "slides/slides-00-installation.html#practice",
    "href": "slides/slides-00-installation.html#practice",
    "title": "Installation",
    "section": "Practice",
    "text": "Practice\n\nOpen up the lab-01-roulette.Rmd file, and edit your name in the author section of the YAML, then knit the document.\n\n\n\nOpen GitHub Desktop and make sure the Current Repository is the project of interest.\n\nOn the left-hand side, you should see files you either edited or created, with a checked blue box. This is correct – GitHub Desktop has automatically done the add step for you.\n\nCommit: on the bottom left:\n\nType a brief comment in the small text-box next to your GitHub profile photo/graphic.\nPush the blue Commit to main button.\n\nPush: on the top right:\n\nThere should be a tab that says Push origin with an upward arrow with a number next to it. That means you are ready! Click the button to push your changes to origin.\nYou will know the push was successful if this tab returns to say Fetch origin"
  },
  {
    "objectID": "slides/slides-00-installation.html#checking-your-changes",
    "href": "slides/slides-00-installation.html#checking-your-changes",
    "title": "Installation",
    "section": "Checking your changes",
    "text": "Checking your changes\n\nGo to the corresponding repository on github.com\nCheck to see if the current version of lab-01-roulette.Rmd file has your updated author name. If it doesn’t, that means the push was not successful."
  },
  {
    "objectID": "live-code/live-code-01.html",
    "href": "live-code/live-code-01.html",
    "title": "Live code 01:",
    "section": "",
    "text": "This lab is intended to re-familiarize yourself with R and RStudio, as well as begin practicing to code in base R. You will need the tidyverse package."
  },
  {
    "objectID": "live-code/live-code-01.html#vectors",
    "href": "live-code/live-code-01.html#vectors",
    "title": "Live code 01:",
    "section": "Vectors",
    "text": "Vectors\nIn R, a vector is a data structure that holds or stores elements of the same type. Type may be numeric, integer, character, boolean, etc.\n\nThe c() function\nGenerally, we create vectors using the c() function and then save the vector into a variable. In the code below, I create a vector of three values (10, 11, and 12), and save the results into v:\n\nv <- c(10, 11, 12)\n\n\n\nThe : operator\nNow, sometimes it’s really useful to create a vector of consecutive numbers, for example, the values 1 through 10. Rather than type out every number explicitly and wrap it in c() , I can use the : operator, which looks like a:b where a and b are integers of your choosing. If a < b, R will then create a vector of numbers a, a+1, a+2,…, b-1, b .\n\nx <- 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWhat do you think happens if a > b? Try the following code for yourself:\n\ny <- 10:1\n\n\n\nThe rep() function\nOne function that I personally use a lot to create vectors is the rep(a,b) function, which takes in two argument. The first is the value a you wish to repeat, and the second argument is the number of times b you’d like to repeat it. How would we create a vector of 20 0’s? Think about it, and check:\n\n\nCode\nrep(0, 20)"
  },
  {
    "objectID": "live-code/live-code-01.html#matrices",
    "href": "live-code/live-code-01.html#matrices",
    "title": "Live code 01:",
    "section": "Matrices",
    "text": "Matrices\nMatrices are the 2D extension of the one-dimensional vectors. When a matrix has n rows and p columns, we denote its dimensions as n x p or “n by p”. We create matrices using the matrix() function. Because of the multiple dimensions, we need to specify the number of rows and the number of columns:\n\nmatrix(NA, nrow = 2, ncol = 3)\n\n     [,1] [,2] [,3]\n[1,]   NA   NA   NA\n[2,]   NA   NA   NA\n\n\nThis code above creates a 2 x 3 matrix of NA values. The first argument takes in the elements you want to fill the matrix with. This can either be a single value, or a single vector of values."
  },
  {
    "objectID": "live-code/live-code-01.html#data-frames",
    "href": "live-code/live-code-01.html#data-frames",
    "title": "Live code 01:",
    "section": "Data frames",
    "text": "Data frames\nWe will create a data frame called my_df here, which holds the two vectors we created before.\n\nmy_df <- data.frame(xvar = x, yvar = y)\n\nNow, if I wanted to only take the variable xvar from my_df, how would I do so using dplyr functions? Take a second to think about it, then check:\n\n\nCode\nmy_df %>%\n  select(xvar)\n\n\nWe will now use base R to access that xvar variable by using $ notation: <df>$<var_name> . If you do this yourself, you should notice that immediately after typing the $ , a menu pops up with all the variables contained in the data frame.\n\nmy_df$xvar\n\nNow, do you notice the difference between the two outputs?\n\nmy_df %>%\n  select(xvar)\n\n   xvar\n1     1\n2     2\n3     3\n4     4\n5     5\n6     6\n7     7\n8     8\n9     9\n10   10\n\nmy_df$xvar\n\n [1]  1  2  3  4  5  6  7  8  9 10"
  },
  {
    "objectID": "live-code/live-code-01.html#indexing",
    "href": "live-code/live-code-01.html#indexing",
    "title": "Live code 01:",
    "section": "Indexing",
    "text": "Indexing\nOne of the most useful tools we will use is indexing and index notation.\nAn index is essentially a numerical representation of an item’s position in a list or vector. It is typically an integer, starting from either 0 or 1 depending on the programming language. In R, our index positions always start at 1!\nFor example, in the word “learning”, the l is at index 1. Similarly, in our vector v of the numbers \\((10, 11, 12)\\), the value at index 1 is 10. We can confirm this with code:\n\nv[1]\n\n[1] 10\n\n\nNotice that we access the item held in index 1 using the square bracket notation [ ]\nNow, I can also replace or modify an element at a given index. I will still access the location using [ ], but now I will store/save the new value:\n\nv[1] <- 13\nv\n\n[1] 13 11 12\n\n\nWe can also modify multiple elements at once by passing in a vector of indices to modify, as well as a vector of new values:\n\nv[2:3] <- c(14, 15)\n\nWhat does v look like now?\nWe can also use indices to refer to elements or entire rows and columns of data frames! Unlike vectors, data frame are two-dimensional, i.e. there are both rows and columns. Thus, our index notation will need to adapt to accommodate this feature. We will still use [ ] notation, but now commas will be introduced:\n\nmy_df[1,2]\n\n[1] 10\n\n\nBased on my_df, what do you think the [1,2] means?\nNow, we already saw how to access a column of a data frame using the $ notation, but we can also use index notation. To access the first column, we would type:\n\nmy_df[,1]\n\nThe 1 after the comma tells R that we want to focus on column 1.\nAs I do not type anything before the comma, R reads this as: “since you did not want a specific row, you must want all the rows”.\nHow do you think we would access the third column? How about both the first and second row?"
  },
  {
    "objectID": "live-code/live-code-01.html#functions",
    "href": "live-code/live-code-01.html#functions",
    "title": "Live code 01:",
    "section": "Functions",
    "text": "Functions\nThere are a lot of simple functions in R that we will rely on. We already saw c() and rep(). Most of the functions we will use take in a vector or matrix of numeric values, and return either a single number or vector in return.\nThe function mean() takes in a vector, and returns the mean of the vector.\n\nmean(x)\n\n[1] 5.5\n\n\nThe function length() takes in a vector, and returns the number of elements in the vector:\n\nlength(x)\n\n[1] 10\n\n\nThe functions max() and min() return what you would expect them to!\nAn extremely useful function we will use is the which() function. Unlike the previous functions, which() does not take in a numeric vector. Rather, it takes a vector of boolean values (i.e. TRUE/FALSE values). Then, it returns the indices of the TRUE values in the vector. For example, an input might be:\n\ny == -5\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nThis is comparing each value in y to see if it equals -5 (recall the double equals sign check for equality). Notice that only one value evaluates to TRUE, specifically the element in index 6. Therefore, if we wrap the which() function around that argument, we should get 6 in return:\n\nwhich(y == -5)\n\ninteger(0)\n\n\nPersonally, I tend to read this line of code as a question: Which element(s) of y are exactly equal to -5?\nWe know that y only holds negative values. What do you think happens if we try to evaluate the following. Try it yourself!\n\nwhich(y == 0)\n\nIt’s also entirely possible that many values in the boolean vector are true, in which case the function would return multiple indices. For example, if I want to know which values in y are negative, I could code:\n\nwhich(y < 0)\n\ninteger(0)\n\n\n\nIndexing with boolean vectors\nAbove, we saw how to access elements held at specific indices of interest. We can also use boolean vectors to return values. Recall our vector v:\n\nv\n\n[1] 13 14 15\n\n\nI can index v by indexing using TRUE’s for each value that I want, and FALSE’s otherwise.\n\nv[c(F, T, F)]\n\n[1] 14"
  },
  {
    "objectID": "live-code/live-code-01.html#your-turn",
    "href": "live-code/live-code-01.html#your-turn",
    "title": "Live code 01:",
    "section": "Your turn!",
    "text": "Your turn!\nPlease complete the following exercises in order:\n\nCreate a vector called my_vec that holds the values 50 through 100.\nCreate a new vector called less60 where an element is TRUE if the corresponding element in my_vec is less than 60, and FALSE otherwise.\nConfirm that the length of your two vectors are the same.\nPass less60 into the function sum() function. Relate the value obtained to the elements of less60.\nModify my_vec such that the value at index 10 is 100.\nObtain the index of the maximum values of my_vec using functions described above.\nNow, pass my_vec into the which.max() function. Even though we haven’t seen it before, based on the function name, the name of the function is intuitive. Does the result from which.max() differ from what you obtained in Ex. 6? How so?\nCreate a 2 x 5 matrix of the values 1 through 10, where the first row holds the values 1-5, and the second row holds the values 6-10. Hint: look at the help file for matrix."
  },
  {
    "objectID": "live-code/live-code-02.html",
    "href": "live-code/live-code-02.html",
    "title": "Live code 02",
    "section": "",
    "text": "We will continue working in base R, and begin learning about conditional statements and for loops!"
  },
  {
    "objectID": "live-code/live-code-02.html#new-function",
    "href": "live-code/live-code-02.html#new-function",
    "title": "Live code 02",
    "section": "New function:",
    "text": "New function:\n\nThe sample(vec, m) function takes a random sample of size m from the vector vec . By default, we sample without replacement and each value in vec is equally likely. For example, I can draw one value between 1-5 at random (where each value as 1/5 chance of being sampled) as follows:\n\nsample(1:5, 1)\n\n[1] 1\n\nsample(1:5, 1)\n\n[1] 1\n\nsample(1:5, 1)\n\n[1] 3\n\n\n\nAs you see, running this code multiple times will lead to different values being sample-d!\nYou can sample with replacement or sample each value in vec with different probability by changing the arguments in the function call."
  },
  {
    "objectID": "live-code/live-code-02.html#conditional-statements",
    "href": "live-code/live-code-02.html#conditional-statements",
    "title": "Live code 02",
    "section": "Conditional statements",
    "text": "Conditional statements\nThus far, we have learned how to store values and relate different R objects. For example, we can obtain a boolean TRUE or FALSE value when we compare two objects as follows:\n\nx <- 3\nx <= 5\n\n[1] TRUE\n\n\nMost often, we want to use the results from these logical operators to change the behavior of our code. That is, if a certain condition is satisfied, we want our code to do something. Else, our code should do something else.\n\nif statements\nThe if statement takes in a condition. If the condition evaluates to TRUE, then the R code we associate with the if statement is executed. The syntax is as follows:\n\nif (condition){\n  code\n}\n\nNotice that the condition goes in parentheses ( ), and the relevant code goes within curly braces { }.\nFor example:\n\nif (x < 5){\n  print(\"x is less than 5\")\n}\n\nTry this yourself! Set x to be a number, then run this code. If you chose x to be greater than or equal to 5, then the condition evaluates to FALSE and so we do not run the code within the curly braces and nothing is printed.\n\n\nelse statements\nNow, maybe we want to a different block of code to run if the condition evaluates to FALSE. This is where the else statement comes in! Importantly, else statements always follows an if statement so there is no need to supply a conditional statement. The syntax is as follows:\n\nif (condition){\n  code associated with TRUE condition\n} else{\n  code associated with FALSE condition\n}\n\nTry modifying the if statement above to have a corresponding block of code that corrently prints when x is greater than or equal to 5."
  },
  {
    "objectID": "live-code/live-code-02.html#for-loops",
    "href": "live-code/live-code-02.html#for-loops",
    "title": "Live code 02",
    "section": "for loops",
    "text": "for loops\nIt is quite simple to perform repetitive tasks in R. If we want to execute the same operations over and over again, we will use a loop. If we want to repeat the operations for a specific number of times, we use a for loop.\nLet’s look at this code:\n\nfor(i in 1:5){\n  print(i)\n}\n\nThe for() code is telling R that we want to run a for loop, which means we want to repeat the code within the curly braces. How many times do we want to repeat? The code says we want to do this for every value in 1:5.\nThe confusing part is the index i, which is essentially a placeholder. Instead of i, we could use any character we’d like! However, people tend to use i for “iteration”. At the beginning, i is set to the first value in the vector 1:5, (i.e. i = 1 to begin with). All the code within the braces are executed with i = 1 being the state of the world. Once we reach the end of the code within the braces, we go back to the top and set i = 2. We continue to do this until the last value in 1:5, which would be 5.\n\nfor(i in 1:5){\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5"
  },
  {
    "objectID": "live-code/live-code-02.html#your-turn",
    "href": "live-code/live-code-02.html#your-turn",
    "title": "Live code 02",
    "section": "Your turn!",
    "text": "Your turn!\n\nWrite code that outputs the square root of a number. If the number is negative, then print out an informative statement instead. Note: the square root of a number can be obtained via the sqrt() function.\nWrite a for loop that calculates the factorial of a whole number of your choice. As a quick refresher, 5! (which we read as “5 factorial”) is equal to 5 x 4 x 3 x 2 x 1.\nObtain a vector y of 5 values by using the sample function, where the possible values to sample from are integers ranging between 1 and 5. Here, I want you to sample with replacement. Write a for loop that loops for 5 iterations and print the number of elements in y equal to the current iteration. If the current iteration value is not contained in y, please print out a useful statement for the user instead."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Unless otherwise stated, please submit assignments as a PDF to Canvas.\nNote: course schedule is subject to change.\n\n\n\n\nDate\nDescription\nClass materials\nAssignments\nDue date\n\n\n\n\n\n\n\nWEEK 1\n\n\n\n\n\nT 2/14\nWelcome!\nClass introduction, intro to base R\n\n\n\n\nR 2/16\nWhat is statistical learning?\nWhat is statistical learning?\n\n\n\n\nF 2/17\nLab day\nMore R coding, GitHub Desktop installation\nLab 01: Roulette\n2/23 at 11:59pm\n\n\n\n\n\nWEEK 2\n\n\n\n\n\nT 2/21\nWhat is statistical learning? (cont.)\nWhat is statistical learning?,\nLive code: Writing functions\n\n\n\n\nR 2/23\nLinear regression\nSlides: Linear regression\nLive code\n\n\n\n\nF 2/24\nLinear regression (cont.)\nLab day\n\nLab 02: Moneyball\n03/02 at 11:59pm\n\n\n\n\n\nWEEK 3\n\n\n\n\n\nT 2/28\nKNN regression\n\n\n\n\n\nR 3/2\nKNN regression (cont.)\n\n\n\n\n\nF 3/3\nLab day"
  },
  {
    "objectID": "live-code/live-code-03.html",
    "href": "live-code/live-code-03.html",
    "title": "Live code 03:",
    "section": "",
    "text": "Here we will learn how to write functions in R. Functions are extremely helpful for automating commons tasks that you might use often (e.g. computing the RMSE for a set of predictions). If you’re going to use the same block of code more than twice, you should consider writing a function!\nThere are three key steps to creating a new function:\n\nPicking a NAME for the function\nListing the INPUTS/ARGUMENTS to the function called function()\nPlacing the code you have developed in the BODY of the function (between the sets of curly braces { } that immediately follow function()\n\nMaking sure to return() the output from the function\n\n\n\n\nFor example, suppose that I want to create a function that takes in a matrix and returns the sums of the values within each column. (There is a function that already does this, but pretend there isn’t!) The function might look like this:\n\ncolumn_sums <- function(input_matrix){\n  n_cols <- ncol(input_matrix)\n  sums <- rep(NA, n_cols)\n  for(i in 1:n_cols){\n    sums[i] <- sum(input_matrix[,i])\n  }\n  return(sums)\n}\n\nThis looks scary, but let’s break it down.\n\nThe name of the function is column_sums.\nR knows I want to create a function because I use the function() function, where I specify that my function requires a single input that will be referred to as input_matrix.\nThe code within the body specifies how I will use input_matrix to calculate the column sums.\nI finish the function by return()-ing a vector.\n\nLet’s test this out: I will create a matrix of numbers, and then use/call my function as I normally would any R function:\n\nmy_mat <- matrix(1:10, nrow = 2, ncol = 5)\nmy_mat\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\ncolumn_sums(my_mat)\n\n[1]  3  7 11 15 19\n\n\nLet’s confirm this is the correct output with the pre-provided R function colSums():\n\ncolSums(my_mat)\n\n[1]  3  7 11 15 19\n\n\nHow might we code a function that calculates the squared error between two values? Try it yourself, then check here to see if your code generally agrees with mine:\n\n\nCode\nsquared_error <- function(x1, x2){\n  ret <- (x1 - x2)^2\n  return(ret)\n}\n\n\n\n\nCode\n# test your code: you should get 16 for passing in -2 and 2\nsquared_error(-2, 2)"
  },
  {
    "objectID": "live-code/live-code-03.html#your-turn",
    "href": "live-code/live-code-03.html#your-turn",
    "title": "Live code 03:",
    "section": "Your turn!",
    "text": "Your turn!\nFeel free to try any and all of the following:\n\nWrite a function that takes in a temperature in degrees Fahrenheit and returns the temperature in degrees Celsius. For reference, the conversion is \\((\\text{degreesF} - 32) * \\frac{5}{9}\\). Give your function an appropriate name.\n\nCheck to make sure your function works by passing in \\(32^\\circ\\) F. You should get 0 back!\n\nWrite a more complicated version of Exercise 1 where the function takes in two inputs: 1) a temperature (in either Fahrenheit or Celsius) and 2) a string or Boolean (your choice!) that denotes if you want to convert to Fahrenheit or Celsius. Your function should return the correct conversion based on the user’s inputs. Note: the conversion from Celsius to Fahrenheit is \\(\\text{degreesC} * \\frac{9}{5} + 32\\).\nWrit a function called get_rmse() that takes in two vectors as inputs: one of predictions, and one of true values. Your function should calculate and return the root mean squared error (see slides for equation).\nLast week we practiced coding for() loops by obtaining the factorial of a given positive integer. Create a function where the user specifies the integer they want to find the factorial of, and return the factorial."
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#linear-regression-1",
    "href": "slides/slides-02-linear-regression.html#linear-regression-1",
    "title": "Linear Regression",
    "section": "Linear regression",
    "text": "Linear regression\n\nA simple, widely used approach in supervised learning\nAssumes that the dependence of \\(Y\\) on the predictors \\(X_{1}, \\ldots, X_{p}\\) is linear"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#possible-questions-of-interest",
    "href": "slides/slides-02-linear-regression.html#possible-questions-of-interest",
    "title": "Linear Regression",
    "section": "Possible questions of interest",
    "text": "Possible questions of interest\n\nIs there a relationship between the abundance of LRUG mites and substrate density or water content in the soil where they are found?\n\nIf so, how are strong are these relationships?\n\nIs the relationship linear?\nHow accurately can we predict the abundance of these mites?"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#simple-linear-regression-1",
    "href": "slides/slides-02-linear-regression.html#simple-linear-regression-1",
    "title": "Linear Regression",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\n\nSimple linear regression (SLR)\n\n\nAssumes a linear model for a quantitative response \\(Y\\) using a single predictor \\(X\\)\n\\[Y = \\beta_{0} + \\beta_{1} X + \\epsilon,\\]\nwhere \\(\\beta_{0}, \\beta_{1}\\) are unknown coefficients (parameters) and \\(\\epsilon\\) is the error\n\n\\(\\beta_{0}\\) is commonly referred to as the intercept, and \\(\\beta_{1}\\) is the slope\nFor example: abundance = \\(\\beta_{0}\\) + \\(\\beta_{1}\\) watercont + \\(\\epsilon\\)"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#parameter-estimation",
    "href": "slides/slides-02-linear-regression.html#parameter-estimation",
    "title": "Linear Regression",
    "section": "Parameter estimation",
    "text": "Parameter estimation\n\nAssuming \\(n\\) observations, we have data of the form \\((x_{1}, y_{1}), (x_{2}, y_{2}), \\ldots, (x_{n}, y_{n})\\)\nAn SLR model says \\[\\begin{align*} y_{i} &= \\beta_{0} + \\beta_{1}x_{i}+ \\epsilon\\\\ &\\approx \\beta_{0} + \\beta_{1}x_{i}\\ , \\qquad \\text{ for all } i = 1,\\ldots, n \\end{align*}\\]\n\nIn this model, \\(f(x_{i}) = \\beta_{0} + \\beta_{1} x_{i}\\)\nNotice that the relationship between \\(x_{i}\\) and \\(y_{i}\\) is the same for all \\(i\\)\n\nIn practice, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are unknown, so we must estimate them\nGoal: obtain (good) estimates \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) that are as close to the true values as possible, such that \\(y_{i} \\approx \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\\)\n\nHow? Minimize the least squares criterion"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#least-squares",
    "href": "slides/slides-02-linear-regression.html#least-squares",
    "title": "Linear Regression",
    "section": "Least squares",
    "text": "Least squares\n\nLet \\(\\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i}\\) be predicted response for \\(i\\)-th observation with predictor \\(x_{i}\\)\nThe \\(i\\)-th residual \\(e_{i}\\) is defined as \\[e_{i} = y_{i} - \\hat{y}_{i}\\]\nDefine residual sum of squares (RSS) as \\[\\text{RSS} = e_{1}^{2} + e_{2}^{2} + \\ldots + e_{n}^{2} = \\sum_{i=1}^{n} e_{i}^2\\]"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#estimation-by-least-squares",
    "href": "slides/slides-02-linear-regression.html#estimation-by-least-squares",
    "title": "Linear Regression",
    "section": "Estimation by least squares",
    "text": "Estimation by least squares\n\\[\\text{RSS} = \\sum_{i=1}^{n} e_{i}^2 = \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^2 = \\sum_{i=1}^{n} (y_{i} - (\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i}))^2\\]\n\nLeast squares approach selects the pair \\((\\hat{\\beta}_{0}, \\hat{\\beta}_{1})\\) that minimize the RSS. Can be shown that the minimizing values are: \\[\\begin{align*}\n\\hat{\\beta}_{1} &= \\frac{\\sum_{i=1}^{n}(x_{i} - \\bar{x})(y_{i} - \\bar{y})}{\\sum_{i=1}^{n}(x_{i} - \\bar{x})^2}\\\\\n\\hat{\\beta}_{0} &= \\bar{y} - \\hat{\\beta}_{1} \\bar{x}\n\\end{align*}\\]\nwhere \\(\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_{i}\\) and \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_{i}\\)"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#mite-data",
    "href": "slides/slides-02-linear-regression.html#mite-data",
    "title": "Linear Regression",
    "section": "Mite data",
    "text": "Mite data\nLeast squares fit for abundance regressed on WaterCont, with residuals in orange.\n\nm1 <- lm(abundance ~ WatrCont, data = mite_dat)\n\n\nLet’s interpret this plot! Do you see anything strange or any patterns?"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#interpreting-coefficients-and-estimates",
    "href": "slides/slides-02-linear-regression.html#interpreting-coefficients-and-estimates",
    "title": "Linear Regression",
    "section": "Interpreting Coefficients and Estimates",
    "text": "Interpreting Coefficients and Estimates\n\\[Y = \\beta_{0} + \\beta_{1} X + \\epsilon\\]\n\n\\(\\beta_{0}\\) is the expected value of \\(Y\\) when \\(X = 0\\)\n\\(\\beta_{1}\\) is the average increase in \\(Y\\) for one-unit increase in \\(X\\)\n\\(\\epsilon\\) is error\nThis equation is the population regression line\nWhen using the least squares estimates for the coefficients, \\(\\hat{Y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X\\) is the least squares line"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#interpreting-coefficients-and-estimates-cont.",
    "href": "slides/slides-02-linear-regression.html#interpreting-coefficients-and-estimates-cont.",
    "title": "Linear Regression",
    "section": "Interpreting Coefficients and Estimates (cont.)",
    "text": "Interpreting Coefficients and Estimates (cont.)\n\nsummary(m1)\n\n\nCall:\nlm(formula = abundance ~ WatrCont, data = mite_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.525  -8.033  -4.088   4.493  47.937 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.63410    4.51171   0.141   0.8886  \nWatrCont     0.02385    0.01039   2.296   0.0248 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.29 on 68 degrees of freedom\nMultiple R-squared:  0.07194,   Adjusted R-squared:  0.05829 \nF-statistic: 5.271 on 1 and 68 DF,  p-value: 0.02477\n\n\n\n\\[\\widehat{\\text{LRUG}} = 0.634 + 0.024 \\text{WatrCont}\\]\n\n\nHow do I interpret \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) for this specific example?\nNote: the estimates \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\) will depend on the observed data! If I took a different sample of LRUG mites, I would probably have different estimated values.\n\nIsn’t that problematic? Can we asses how accurate are our estimates \\(\\hat{\\beta}_{0}\\) and \\(\\hat{\\beta}_{1}\\)?"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#assessing-accuracy-of-coefficient-estimates",
    "href": "slides/slides-02-linear-regression.html#assessing-accuracy-of-coefficient-estimates",
    "title": "Linear Regression",
    "section": "Assessing Accuracy of Coefficient Estimates",
    "text": "Assessing Accuracy of Coefficient Estimates\n\nStandard error (SE) of an estimator reflects how it varies under repeated sampling.\nFor simple linear regression: \\[\\text{SE}(\\hat{\\beta}_{0}) = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^{n}(x_{i} - \\bar{x})^2}\\right] \\qquad\n\\text{SE}(\\hat{\\beta}_{1}) = \\frac{\\sigma^2}{\\sum_{i=1}^{n}(x_{i} - \\bar{x})^2}\\]\nwhere \\(\\sigma^2 = \\text{Var}(\\epsilon)\\)\nTypically \\(\\sigma^2\\) is not known, but can be estimated from the data.\nEstimate \\(\\hat{\\sigma}\\) is residual standard error (RSE), given by: \\[\\hat{\\sigma}= \\text{RSE} = \\sqrt{\\frac{1}{n-2}\\text{RSS}}\\]\nWe use this estimate to calculate \\(\\text{SE}(\\hat{\\beta}_{0})\\) and \\(\\text{SE}(\\hat{\\beta}_{1})\\)"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#hypothesis-testing",
    "href": "slides/slides-02-linear-regression.html#hypothesis-testing",
    "title": "Linear Regression",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nHypothesis testing is a method of statistical inference to determine whether the data at hand sufficiently support a particular hypothesis\n\nHelps test the results of an experiment or survey to see if you have meaningful results\nHelps draw conclusions about a population parameter\n\nStandard errors can be used to perform hypothesis tests on the coefficients"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#hypothesis-testing-1",
    "href": "slides/slides-02-linear-regression.html#hypothesis-testing-1",
    "title": "Linear Regression",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nNotion of “null” versus “alternate” hypothesis\n\nNull hypothesis \\(H_{0}\\): there is no relationship between \\(X\\) and \\(Y\\)\nAlternative hypothesis \\(H_{A}\\): there is some relationship between \\(X\\) and \\(Y\\)\n\nMathematically, corresponds to testing \\[H_{0}: \\beta_{1} = 0 \\quad \\text{ vs. } \\quad H_{A}: \\beta_{1} \\neq 0\\]\nbecause if \\(H_{0}\\) true, then the model reduces to \\(Y = \\beta_{0} + \\epsilon\\) so there is no relationship\nTo test this null hypothesis, want to determine if \\(\\hat{\\beta}_{1}\\) is sufficiently far from zero\n\nHow much is ‘sufficiently far’? Depends on \\(\\text{SE}(\\hat{\\beta}_{1})\\)."
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#p-value",
    "href": "slides/slides-02-linear-regression.html#p-value",
    "title": "Linear Regression",
    "section": "p-value",
    "text": "p-value\n\nWith lots of hand-waving: can calculate a p-value, which is a probability that we observed the data we did, given that \\(H_{0}\\) is true. If the p-value is small, the observed data don’t seem to support \\(H_{0}\\)\n\nCompare \\(p\\)-value to a pre-determined rejection level \\(\\alpha\\) (often 0.05).\nIf \\(p\\)-value \\(< \\alpha\\), reject \\(H_{0}\\). Otherwise, fail to reject \\(H_{0}\\).\n\n\n\n\nsummary(m1)\n\n\nCall:\nlm(formula = abundance ~ WatrCont, data = mite_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.525  -8.033  -4.088   4.493  47.937 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.63410    4.51171   0.141   0.8886  \nWatrCont     0.02385    0.01039   2.296   0.0248 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.29 on 68 degrees of freedom\nMultiple R-squared:  0.07194,   Adjusted R-squared:  0.05829 \nF-statistic: 5.271 on 1 and 68 DF,  p-value: 0.02477\n\n\n\n\n\\(H_{0}: \\beta_{1} = 0\\) (there is no relationship between LRUG abundance and WatrCont)\n\nWhat is the p-value for this hypothesis?\nWhat is our decision in the mite example?"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#why-hypothesis-testing",
    "href": "slides/slides-02-linear-regression.html#why-hypothesis-testing",
    "title": "Linear Regression",
    "section": "Why hypothesis testing?",
    "text": "Why hypothesis testing?\n\nHypothesis testing can help us determine if there is a relationship between the predictor and the response variable!\n\n\nThis is the inference part of statistical learning\n\n\nIf there is a relationship, then it makes sense to interpret the strength of the relationship (i.e. interpret the value \\(\\hat{\\beta}_{1}\\))"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#multiple-linear-regression-1",
    "href": "slides/slides-02-linear-regression.html#multiple-linear-regression-1",
    "title": "Linear Regression",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nIn practice, we often have more than one predictor\nWith \\(p\\) predictors, the model is \\[Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\ldots + \\beta_{p}X_{p} +\\epsilon\\]\nInterpret \\(\\beta_{j}\\) as the average effect on \\(Y\\) for a one-unit increase in \\(X_{j}\\), holding all other predictors fixed/constant"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#mite-data-1",
    "href": "slides/slides-02-linear-regression.html#mite-data-1",
    "title": "Linear Regression",
    "section": "Mite data",
    "text": "Mite data\nRegressing abundance on both WatrCont and SubsDens:\n\nm2 <- lm(abundance ~ WatrCont + SubsDens, data = mite_dat)\nsummary(m2)\n\n\nCall:\nlm(formula = abundance ~ WatrCont + SubsDens, data = mite_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.192  -8.633  -1.385   6.866  44.245 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) 10.30549    5.48833   1.878  0.06477 . \nWatrCont     0.03444    0.01057   3.257  0.00177 **\nSubsDens    -0.35682    0.12604  -2.831  0.00612 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.7 on 67 degrees of freedom\nMultiple R-squared:  0.1711,    Adjusted R-squared:  0.1464 \nF-statistic: 6.915 on 2 and 67 DF,  p-value: 0.001861\n\n\n\n\\[\\widehat{abundance} = 10.306 + 0.034 \\text{WatrCont} -0.357 \\text{SubsDens}\\]\n\n\nHow do we interpret the estimated coefficients?\nHow do we interpret the p-values?\n\nThe p-value for \\(\\beta_{j}\\) corresponds to the test of \\(H_{0}: \\beta_{j} = 0\\) and \\(\\beta_{k} (k \\neq j)\\) unrestricted"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#model-fit",
    "href": "slides/slides-02-linear-regression.html#model-fit",
    "title": "Linear Regression",
    "section": "Model fit",
    "text": "Model fit\n\nHow well does our linear regression model “fit” the data it was trained on? How accurate is our model?\nResidual standard error (RSE) \\[\\text{RSE}  = \\sqrt{\\frac{1}{n-p-1} \\sum_{i=1}^{n} (y_{i} - \\hat{y}_{i})^2},\\]\nwhere \\(p\\) is the number of predictors, and \\(i\\) indexes the observations used to fit the model\nRSE is considered a measure of the lack of fit of the model\n\nMeasured in the units of \\(Y\\)"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#predictions",
    "href": "slides/slides-02-linear-regression.html#predictions",
    "title": "Linear Regression",
    "section": "Predictions",
    "text": "Predictions\n\nHow well does our linear regression model predict new responses for a given set of covariates?\nFor example, suppose I want to use our model to predict the abundance of LRUG mites at a new sampling location where the WatrCont is 400 g/L and the SubsDens is 30 g/L\nI will plug these values into our fitted model m2:\n\n\n\n\n\n\\[\\widehat{abundance} = 10.306 + 0.034 \\times 400 -0.357 \\times 30 = 13.3747829\\]\n\n\n\nnew_data <- data.frame(WatrCont = 400, SubsDens = 30)\npred_mite <- predict(m2, newdata = new_data)\n\n\n\nOkay…great! But should I trust these predictions?"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#prediction-performance",
    "href": "slides/slides-02-linear-regression.html#prediction-performance",
    "title": "Linear Regression",
    "section": "Prediction performance",
    "text": "Prediction performance\n\nWe could get a better sense of a model’s prediction performance by comparing the predicted responses to the true values\nWe should always compare prediction performance for “previously unseen” data (i.e. test data)\n\nThe model already “knows” the data used to fit it (i.e. the training data)\n\n\nDiscuss: what are some important criteria for the testing data?"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#qualitative-predictors-1",
    "href": "slides/slides-02-linear-regression.html#qualitative-predictors-1",
    "title": "Linear Regression",
    "section": "Qualitative predictors",
    "text": "Qualitative predictors\n\nThus far, we have assumed that all predictors in our linear model are quantitative. In practice, we often have categorical predictors\nOur mite data has the following categorical variables: Shrub, Substrate, and Topo\nLet’s begin with the simplest case: a categorical predictor with two categories/levels\n\ne.g. the Topo variable takes on the values “Blanket” or “Hummock” only"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#level-qualitative-predictor",
    "href": "slides/slides-02-linear-regression.html#level-qualitative-predictor",
    "title": "Linear Regression",
    "section": "2-level qualitative predictor",
    "text": "2-level qualitative predictor\n\nWe will create an indicator or dummy variable as follows: \\[\\text{TopoBlanket}_{i} = \\begin{cases} 1 & \\text{ if } \\color{blue}{\\text{Topo}_{i}} = \\text{Blanket} \\\\\n0 & \\text{ if } \\color{blue}{\\text{Topo}_{i}} = \\text{Hummock} \\end{cases}\\]\nSimple linear regression model for LRUG regressed on Topo: \\[\\text{LRUG}_{i} = \\beta_{0} + \\beta_{1}\\text{TopoBlanket}_{i} + \\epsilon_{i} = \\begin{cases}\n\\beta_{0} + \\epsilon_{i} & \\text{ if } \\color{blue}{\\text{Topo}_{i}} = \\text{Blanket} \\\\\n\\beta_{0} + \\beta_{1} + \\epsilon_{i} & \\text{ if } \\color{blue}{\\text{Topo}_{i}} = \\text{Hummock}  \\end{cases}\\]\n\nHow to interpret?"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#mite-data-2",
    "href": "slides/slides-02-linear-regression.html#mite-data-2",
    "title": "Linear Regression",
    "section": "Mite data",
    "text": "Mite data\n\nm3 <- lm(abundance ~ Topo, data = mite_dat)\nsummary(m3)\n\n\nCall:\nlm(formula = abundance ~ Topo, data = mite_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.318  -4.318  -2.154   4.473  41.682 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   15.318      1.658   9.238 1.26e-13 ***\nTopoHummock  -13.164      2.721  -4.838 7.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11 on 68 degrees of freedom\nMultiple R-squared:  0.2561,    Adjusted R-squared:  0.2452 \nF-statistic: 23.41 on 1 and 68 DF,  p-value: 7.851e-06\n\n\n\nFitted model is: \\[\\widehat{\\text{LRUG}} = 15.318 - 13.164 \\text{TopoHummock}\\]\nInterpret!"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#more-than-two-levels",
    "href": "slides/slides-02-linear-regression.html#more-than-two-levels",
    "title": "Linear Regression",
    "section": "More than two levels",
    "text": "More than two levels\n\nWith more than two levels, we simply create additional dummy variables: \\[\\begin{align*}\\text{Shrub}_{i,Few} &= \\begin{cases} 1 & \\text{ if } \\color{blue}{\\text{Shrub}_{i}} = \\text{Few} \\\\\n0 & \\text{ if } \\color{blue}{\\text{Shrub}_{i}} = \\text{not Few} \\end{cases} \\\\\n\\text{Shrub}_{i, Many} &= \\begin{cases} 1 & \\text{ if } \\color{blue}{\\text{Shrub}_{i}} = \\text{Many} \\\\\n0 & \\text{ if } \\color{blue}{\\text{Shrub}_{i}} = \\text{not Many} \\end{cases}\n\\end{align*}\\]\nResulting regression model for LRUG with only Shrub as predictor: \\[\\begin{align*} \\text{LRUG}_{i} &= \\beta_{0} + \\beta_{1} \\text{Shrub}_{i, Few} + \\beta_{2} \\text{Shrub}_{i, Many} + \\epsilon_{i}\\\\\n&\\approx \\begin{cases}  \\beta_{0} + \\beta_{1} & \\text{ if } \\color{blue}{\\text{Shrub}_{i}} = \\text{Few} \\\\\n\\beta_{0} + \\beta_{2} & \\text{ if } \\color{blue}{\\text{Shrub}_{i}} = \\text{Many} \\\\\n\\beta_{0} & \\text{ if } \\color{blue}{\\text{Shrub}_{i}} = \\text{None} \\end{cases}\\end{align*}\\]\n\n\n\nm4 <- lm(abundance ~ Shrub, data = mite_dat)\nsummary(m4)\n\n\nCall:\nlm(formula = abundance ~ Shrub, data = mite_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.895  -8.060  -2.760   6.635  40.105 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   10.923      1.405   7.775 6.12e-11 ***\nShrub.L       -9.288      2.505  -3.707 0.000427 ***\nShrub.Q       -1.460      2.359  -0.619 0.538164    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.64 on 67 degrees of freedom\nMultiple R-squared:  0.1791,    Adjusted R-squared:  0.1545 \nF-statistic: 7.306 on 2 and 67 DF,  p-value: 0.001348"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#remarks",
    "href": "slides/slides-02-linear-regression.html#remarks",
    "title": "Linear Regression",
    "section": "Remarks",
    "text": "Remarks\n\nFor a given categorical variable, there will always be one fewer dummy variables than levels\n\nLevel with no dummy variable is known as baseline. For model m4 on previous slide, the “None” category was the baseline level\n\nCan have multiple categorical variables in a single model:\n\n\n\nm5 <- lm(abundance ~ Topo + Shrub, data = mite_dat)\nsummary(m5)\n\n\nCall:\nlm(formula = abundance ~ Topo + Shrub, data = mite_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.895  -6.056   0.691   4.756  40.105 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   14.755      1.641   8.993 4.48e-13 ***\nTopoHummock  -11.253      2.997  -3.755 0.000369 ***\nShrub.L       -4.832      2.580  -1.873 0.065552 .  \nShrub.Q       -3.128      2.203  -1.420 0.160385    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.65 on 66 degrees of freedom\nMultiple R-squared:  0.3236,    Adjusted R-squared:  0.2928 \nF-statistic: 10.52 on 3 and 66 DF,  p-value: 9.593e-06\n\n\n\n\nLet’s write out the estimated regression model together\n\nWhat does \\(\\beta_{0}\\) represent?\nHow do we interpret the coefficients?"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#linear-model-is-restrictive",
    "href": "slides/slides-02-linear-regression.html#linear-model-is-restrictive",
    "title": "Linear Regression",
    "section": "Linear model is restrictive",
    "text": "Linear model is restrictive\n\nLinear model is widely used and works quite well, but has several highly restrictive assumptions\n\nRelationship between \\(X\\) and \\(Y\\) is additive\nRelationship between \\(X\\) and \\(Y\\) is linear\n\nThere are common approaches to loosen these assumptions. We will only discuss the first restriction here. Take a regression class for more!"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#interactions",
    "href": "slides/slides-02-linear-regression.html#interactions",
    "title": "Linear Regression",
    "section": "Interactions",
    "text": "Interactions\n\nAdditive assumption: the association between a predictor \\(X_{j}\\) and the response \\(Y\\) does not depend on the value of any other predictors\n\n\n\\[Y = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\epsilon\\]\nvs\n\\[Y = \\beta_{0} + \\beta_{1} X_{1} + \\beta_{2} X_{2} + \\beta_{3}\\color{orange}{X_{1}X_{2}} + \\epsilon\\]\n\n\nThis third predictor \\(\\color{orange}{X_{1}X_{2}}\\) is known as an interaction term\nThe total effect of \\(X_{1}\\) on \\(Y\\) also depends on the value of \\(X_{2}\\) through the interaction\nIn the above equation, \\(\\beta_{1}\\) and \\(\\beta_{2}\\) are called the “main effects” of \\(X_{1}\\) and \\(X_{2}\\) respectively"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#mite-data-3",
    "href": "slides/slides-02-linear-regression.html#mite-data-3",
    "title": "Linear Regression",
    "section": "Mite data",
    "text": "Mite data\n\nSuppose I want to regress LRUG abundance on SubsDens and WatrCont and their interaction:\n\n\n\nmod_int <- lm(abundance ~ SubsDens + WatrCont + \n                SubsDens * WatrCont,\n              data = mite_dat)\nsummary(mod_int)\n\n\n\n\n\n\nCall:\nlm(formula = abundance ~ SubsDens + WatrCont + SubsDens * WatrCont, \n    data = mite_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-24.012  -7.155  -1.896   5.011  44.539 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)  \n(Intercept)       -1.012e+01  1.369e+01  -0.740   0.4621  \nSubsDens           1.323e-01  3.256e-01   0.406   0.6859  \nWatrCont           8.667e-02  3.379e-02   2.565   0.0126 *\nSubsDens:WatrCont -1.210e-03  7.443e-04  -1.625   0.1088  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.56 on 66 degrees of freedom\nMultiple R-squared:  0.203, Adjusted R-squared:  0.1668 \nF-statistic: 5.604 on 3 and 66 DF,  p-value: 0.001744"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#mite-data-4",
    "href": "slides/slides-02-linear-regression.html#mite-data-4",
    "title": "Linear Regression",
    "section": "Mite data",
    "text": "Mite data\n\nFitted model: \\(\\widehat{LRUG} = -10.12 + 0.132 \\text{SubsDens} + 0.087 \\text{WatrCont} -0.001 \\text{SubsDens} \\times \\text{WatrCont}\\)\nInterpretations?\n\n\n\n\n\nEstimates suggest that a 1 g/L increase in the \\(\\color{blue}{\\text{WaterCont}}\\) of the soil is associated with an increased abundance of (0.087 + -0.001 \\(\\times \\color{blue}{\\text{SubsDens}}\\)) LRUG mites"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#interactions-with-categorical-variable",
    "href": "slides/slides-02-linear-regression.html#interactions-with-categorical-variable",
    "title": "Linear Regression",
    "section": "Interactions with categorical variable",
    "text": "Interactions with categorical variable\n\nCan also have interactions involving categorical variables!\nIn particular, the interaction between a quantitative and a categorical variable has nice interpretation\nConsider the effects of SubsDens and Topo and their interaction on the abundances: \\[\\begin{align*}\n\\text{LRUG}_{i} &\\approx \\beta_{0} + \\beta_{1} \\text{SubsDens}_{i} + \\beta_{2}\\text{TopoBlanket}_{i} +  \\beta_{3}  \\text{SubsDens}_{i} \\times \\text{TopoBlanket}_{i} \\\\\n& = \\begin{cases}\n\\beta_{0} + \\beta_{1}\\text{SubsDens}_{i} & \\text{ if } \\color{blue}{\\text{Topo}_{i}} = \\text{Blanket} \\\\\n(\\beta_{0} + \\beta_{2}) + (\\beta_{1} + \\beta_{3}) \\text{SubsDens}_{i} & \\text{ if } \\color{blue}{\\text{Topo}_{i}} = \\text{Hummock}  \\end{cases}\n\\end{align*}\\]\n\n\n\n\n\nCall:\nlm(formula = abundance ~ SubsDens + Topo + SubsDens * Topo, data = mite_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.059  -4.764  -1.186   3.229  39.191 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           34.7881     5.3697   6.479 1.35e-08 ***\nSubsDens              -0.4695     0.1242  -3.781 0.000338 ***\nTopoHummock          -29.4584     9.0331  -3.261 0.001757 ** \nSubsDens:TopoHummock   0.3803     0.2323   1.637 0.106406    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.11 on 66 degrees of freedom\nMultiple R-squared:  0.3901,    Adjusted R-squared:  0.3624 \nF-statistic: 14.07 on 3 and 66 DF,  p-value: 3.425e-07"
  },
  {
    "objectID": "slides/slides-02-linear-regression.html#fitted-regression-lines",
    "href": "slides/slides-02-linear-regression.html#fitted-regression-lines",
    "title": "Linear Regression",
    "section": "Fitted regression lines",
    "text": "Fitted regression lines"
  },
  {
    "objectID": "live-code/live-code-04.html",
    "href": "live-code/live-code-04.html",
    "title": "Live code 04:",
    "section": "",
    "text": "library(tidyverse)\nlibrary(vegan) # install this in your Console: install.packages(\"vegan\")\ndata(mite)\ndata(mite.env)\nmite_dat <- mite.env %>%\n  add_column(abundance = mite$LRUG)\n\n\n# SLR\nm1 <- lm(abundance ~ WatrCont, data = mite_dat)\nsummary(m1)\n\n\nCall:\nlm(formula = abundance ~ WatrCont, data = mite_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.525  -8.033  -4.088   4.493  47.937 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.63410    4.51171   0.141   0.8886  \nWatrCont     0.02385    0.01039   2.296   0.0248 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.29 on 68 degrees of freedom\nMultiple R-squared:  0.07194,   Adjusted R-squared:  0.05829 \nF-statistic: 5.271 on 1 and 68 DF,  p-value: 0.02477\n\n# MLR\nm2 <- lm(abundance ~ WatrCont + SubsDens, data = mite_dat)\nsummary(m2)\n\n\nCall:\nlm(formula = abundance ~ WatrCont + SubsDens, data = mite_dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-20.192  -8.633  -1.385   6.866  44.245 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept) 10.30549    5.48833   1.878  0.06477 . \nWatrCont     0.03444    0.01057   3.257  0.00177 **\nSubsDens    -0.35682    0.12604  -2.831  0.00612 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.7 on 67 degrees of freedom\nMultiple R-squared:  0.1711,    Adjusted R-squared:  0.1464 \nF-statistic: 6.915 on 2 and 67 DF,  p-value: 0.001861\n\n\n\nnew_dat <- data.frame(WatrCont = 400:405, SubsDens = 30:35)\npreds <- predict(m2, newdata = new_dat)\npreds\n\n       1        2        3        4        5        6 \n13.37478 13.05239 12.73000 12.40761 12.08523 11.76284"
  },
  {
    "objectID": "slides/slides-03-knn-regression.html#k-nearest-neighbors-1",
    "href": "slides/slides-03-knn-regression.html#k-nearest-neighbors-1",
    "title": "KNN Regression",
    "section": "K-nearest neighbors",
    "text": "K-nearest neighbors\n\nK-nearest neighbors (KNN) is a nonparametric supervised learning method\nIntuitive and simple\nRelies on the assumption: observations with similar predictors will have similar responses\nWorks for both regression and classification (we will start with regression)"
  },
  {
    "objectID": "slides/slides-03-knn-regression.html#algorithm-in-words",
    "href": "slides/slides-03-knn-regression.html#algorithm-in-words",
    "title": "KNN Regression",
    "section": "Algorithm (in words)",
    "text": "Algorithm (in words)\n\nChoose a positive integer \\(K\\), and have your data split into a train set and test set\nFor a given test observation with predictor \\(x_{0}\\):\n\nIdentify the \\(K\\) points in the train data that are closest (in predictor space) to \\(x_{0}\\). Call this set of neighbors \\(\\mathcal{N}_{0}\\).\nPredict \\(\\hat{y}_{0}\\) to be the average of the responses in the neighbor set, i.e. \\[\\hat{y}_{0} = \\frac{1}{K} \\sum_{i \\in \\mathcal{N}_{0}} y_{i}\\]"
  },
  {
    "objectID": "slides/slides-03-knn-regression.html#example",
    "href": "slides/slides-03-knn-regression.html#example",
    "title": "KNN Regression",
    "section": "Example",
    "text": "Example\n\n\n\n\nOn the next slide, you will see plot with a bunch of colored points\nEach point is plotted in predictor space \\((x1, x2)\\), and is colored according to the value of its response \\(y\\)\nI have a new point (\\(\\color{blue}{*}\\)) and its covariates/predictors, but I need to make a prediction for its response"
  },
  {
    "objectID": "labs/lab-02-linear-regression.html",
    "href": "labs/lab-02-linear-regression.html",
    "title": "Lab 02: Linear regression",
    "section": "",
    "text": "Note\n\n\n\nThroughout this document, you will see text in different colors. The text in maroon/red denotes the “deliverable” (i.e. what I will be looking for you to code/answer/address in your submission.\n\n\n\nIntroduction\nMoneyball tells the success story of the Oakland A baseball team in 2002. In 2001, the team was extremely poor, and so the General Manager named Billy Beane needed ideas on how to improve the team with limited financial resources.\nBilly Beane and his colleague Paul DePodesta did some analysis and explored models to assemble a competitive baseball team. Beane hypothesized that some skills of a baseball player were overvalued, whereas others undervalued. If they could detect the undervalued skills, they could find good players at a bargain (i.e. cheaper) contract. We will re-create their findings here.\nLoad in the data using the following code (if you get an error, make sure you are set to the correct file directory):\n\nbaseball <- read.csv(\"data/baseball.csv\")\n\nFor terminology, according to Wikipedia, “a run is scored when a player advances around first, second and third base and returns safely to home plate, touching the bases in that order, before three outs are recorded…The object of the game is for a team to score more runs than its opponent.”\nEach observation represents a team in a given year. The data dictionary is as follows:\n\nTeam: MLB team\nLeague: National League (NL) or American League (AL)\nYear: Year\nRS: Total runs scored\nRA: Total runs allowed (the amount of runs that score against a pitcher)\nW: Total wins in the season\nOBP: On-base percentage (how frequently a batter reaches base per plate appearance)\nSLG: Slugging percentage (total bases divided by at bats)\nBA: Batting average (total hit divided by total at-bats)\nPlayoffs: If the team made it to the playoffs (1) or did not (0)\nOOBP: Opponent’s on-base percentage\nOSLG: Opponent’s slugging percentage\n\n\n\nPart 1: EDA and Data Wrangling\n\nData wrangling\nThe data provided to you have data ranging from 1962 to 2012. To re-create this famous analysis, we need to pretend it’s the year 2002 and thus we only have data through 2001.\n\nCreate a new data frame called moneyball with the appropriate subset of the baseball data.\n\n\n\nEDA\nThe goal of any MLB team (I think) is to make it to the playoffs. Billy Beane determined that a team needs to win at least 95 games to make it to the playoffs.\n\nUsing some appropriate EDA, demonstrate how you think Billy Beane arrived at the number 95.\n\n\n\nMore data wrangling\nSo, Billy Beane needed some way to understand what influences/determines the number of wins a team had in a given season. It was determined that the run differential was an important metric, calculated as the overall runs scored minus the runs allowed. A positive run differential means the team scores more than it allows (this is good).\n\nModify your moneyball data frame to add a new variable called RD that is the run differential.\n\n\n\nMore EDA\n\nCreate a scatterplot of a team’s wins versus its run differential in each season. Does there appear to be a linear relationship?\n\n\n\n\n\n\n\nCommit reminder\n\n\n\n\n\nThis would be a good time to knit, commit, and push your changes to GitHub!\n\n\n\n\n\n\nPart 2: Model for wins\n\nFit the model\n\nFit a simple linear regression model with a team’s wins as the response variable and the run differential as the predictor. Call this model mod_wins. Interpret the coefficients.\nRecall that we need at least 95 wins to enter the playoffs. Based on your model, how large of a run differential do we need to get into the playoffs?\n\n\n\n\n\n\n\nCommit reminder\n\n\n\n\n\nThis would be a good time to knit, commit, and push your changes to GitHub!\n\n\n\n\n\n\nPart 3: Components of run differential\nRecall that the runs scored and the runs allowed determine the run differential. So, we also need to understand which variables impact both of these components and how.\n\nModel for runs scored\nThe Oakland A’s discovered that a team’s on-base percentage (OBP), the slugging percentage (SLG), and the batting average (BA) were important for determining how many runs are scored (RS).\n\nFit a linear regression model called mod_rs1 for the runs scored as the response regressed on these three predictors. Is a team’s batting average important for explaining its runs scored? Why or why not?\n\nThe Oakland A’s determined that a team’s batting average was overvalued. Because of this, the Oakland A’s decided to not consider batting average.\n\nUse your fitted model mod_rs1 to explain how they came to this conclusion. Then fit another linear regression model called mod_rs2 for runs scored regressed only on on-base percentage and the slugging percentage.\n\n\n\nModel for runs allowed\nThe Oakland A’s found that the runs allowed (RA) were influenced by the opponents on-base percentage (OOBP) and the opponents slugging percentage (OSLG).\n\nFit a multiple linear regression model called mod_ra for this relationship.\n\n\n\n\n\n\n\nCommit reminder\n\n\n\n\n\nThis would be a good time to knit, commit, and push your changes to GitHub!\n\n\n\n\n\n\nPart 4: Putting it all together\nRecap: for the upcoming 2002 baseball season, we need at least 95 wins to enter the playoffs. We fit a model (mod_wins) for a team’s wins based on its run differential. So, we need to predict the run differential for our team in the upcoming season. To do this, we can predict the runs scored and runs allowed for our new team given some statistics.\n\nCreate new team\nPaul DePodesta ultimately formulated a team of players with the following statistics:\n\nOBP: 0.339\nSLG: 0.430\nOOBP: 0.307\nOSLG: 0.373\n\n\nCreate a new data frame called pauls_team that contains these four statistics of the new team (i.e. pauls_team should have one row and four columns).\n\n\n\nPredictions\n\nUsing your models mod_rs2 and mod_ra and the predict() function, predict the runs scored and runs allowed for pauls_team. Based on these two predictions, what is the predicted run differential?\nBased on the predicted run differential, what is the predicted number of wins for our team in the upcoming season? Should we expect to enter the playoffs?\n\n\n\n\n\n\n\nCommit reminder\n\n\n\n\n\nThis would be a good time to knit, commit, and push your changes to GitHub!\n\n\n\n\n\n\nPart 5: Prediction performance\nBilly Beane and Paul DePodesta ultimately decided to remove batting average from their model for runs scored because they had limited financial resources and wanted to find the skills that were undervalued. However, it could be the case that knowing the batting average of a player is important to explaining the runs scored, and also predicting the runs scored. After all, we had to make predictions for the 2002 season, so we would like a model that predicts the runs scored well. Let’s examine this now to see if the team would have made better predictions of runs scored if they had kept the batting average in the model. We will predict for the upcoming year 2002.\n\nData preparation\n\nCreate a new data frame called baseball2002 that contains the observations from the year 2002 in the original baseball data.\nThen, create a new a variable called RS_true that holds the vector of true runs scored in 2002.\n\n\n\nPredict\n\nObtain predictions of runs scored for the baseball2002 data using both mod_rs1 and mod_rs2 (i.e. you should have two vectors of predictions).\n\nPlease do not report/display/print the vectors of predictions! Just store them using an appropriate variable name.\n\n\nPrediction error\n\nLastly, calculate and report the root mean squared error (RMSE) for the predictions from both of the models.\nBased on your results, do you think the Oakland A’s were correct to remove batting average from their model for runs scored? Why or why not?\n\n\n\n\n\n\n\n\n\nBrief comprehension questions\nPlease answer each of the following questions. Each question only requires a 1-2 sentence response at most!\n\nFor this analysis, did we have train and test datasets? If so, what was the train set and what was the test set?\nFor this analysis, did we ask questions concerning prediction? If so, where?\nFor this analysis, did we ask questions concerning inference? If so, where?\n\n\n\nSubmission\nWhen you’re finished, knit to PDF one last time and upload the PDF to Canvas. Commit and push your code back to GitHub one last time."
  },
  {
    "objectID": "slides/slides-03-knn-regression.html#considerations",
    "href": "slides/slides-03-knn-regression.html#considerations",
    "title": "KNN Regression",
    "section": "Considerations",
    "text": "Considerations\n\nHow do we determine who the neighbors \\(\\mathcal{N}_{0}\\) should be? On the previous slide, it may have seemed intuitive.\n\ni.e. how do we quantify “closeness”?\n\nWhich \\(K\\) should we use?"
  },
  {
    "objectID": "slides/slides-03-knn-regression.html#euclidean-distance",
    "href": "slides/slides-03-knn-regression.html#euclidean-distance",
    "title": "KNN Regression",
    "section": "Euclidean distance",
    "text": "Euclidean distance\n\nOne of the most common ways to measure distance between points is with Euclidean distance\nOn a number line (one-dimension), the distance between two points \\(a\\) and \\(b\\) is simply the absolute value of their difference\n\nLet \\(d(a,b)\\) denote the Euclidean distance between \\(a\\) and $b$. Then in 1-D, \\(d(a,b) = |a-b|\\).\n\nIn 2-D (think lon-lat coordinate system), the two points are \\(\\mathbf{a} = (a_{1}, a_{2})\\) and \\(\\mathbf{b} = (b_{1}, b_{2})\\) with Euclidean distance \\[d(\\mathbf{a}, \\mathbf{b}) = \\sqrt{(a_{1} - b_{1}) ^2 + (a_{2} - b_{2})^2}\\]\nImportant: the “two-dimensions” refers to the number of coordinates in each point, not the fact that we are calculating a distance between two points"
  },
  {
    "objectID": "slides/slides-03-knn-regression.html#euclidean-distance-cont.",
    "href": "slides/slides-03-knn-regression.html#euclidean-distance-cont.",
    "title": "KNN Regression",
    "section": "Euclidean distance (cont.)",
    "text": "Euclidean distance (cont.)\n\nGeneralizing to \\(p\\) dimensions: if \\(\\mathbf{a} = (a_{1}, a_{2}, \\ldots, a_{p})\\) and \\(\\mathbf{b} = (b_{1}, b_{2}, \\ldots, b_{p})\\), then \\[d(\\mathbf{a}, \\mathbf{b}) = \\sqrt{(a_{1} - b_{1}) ^2 + (a_{2} - b_{2})^2 + \\ldots (a_{p} - b_{p})^2} = \\sqrt{\\sum_{j=1}^{p}(a_{j} - b_{j})^2}\\]\nE.g. let \\(\\mathbf{a} = (1, 0, 3)\\) and \\(\\mathbf{b} = (-1, 2, 2)\\). What is \\(d(\\mathbf{a}, \\mathbf{b})\\)?"
  },
  {
    "objectID": "slides/slides-03-knn-regression.html#euclidean-distance-1",
    "href": "slides/slides-03-knn-regression.html#euclidean-distance-1",
    "title": "KNN Regression",
    "section": "Euclidean distance",
    "text": "Euclidean distance\n\nAnother common distance metric is Manhattan distance\n\nNamed after the grid-system of Manhattan’s roads\n\nThe Manhattan distance between two points \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) in \\(p\\)-dimensions is \\[d_{m}(\\mathbf{a}, \\mathbf{b}) = |a_{1} - b_{1}| + |a_{2}  - b_{2}| + \\ldots +|a_{p}  - b_{p}| = \\sum_{j=1}^{p} |a_{j}  - b_{j}|\\]\nE.g. let \\(\\mathbf{a} = (1, 0, 3)\\) and \\(\\mathbf{b} = (-1, 2, 2)\\). What is \\(d_{m}(\\mathbf{a}, \\mathbf{b})\\)?"
  },
  {
    "objectID": "slides/slides-03-knn-regression.html#distance-in-knn",
    "href": "slides/slides-03-knn-regression.html#distance-in-knn",
    "title": "KNN Regression",
    "section": "Distance in KNN",
    "text": "Distance in KNN\n\nWe want to find the closest neighbor(s) in the train set to a given test point \\(\\mathbf{x}_{0}\\), such that we can make a prediction \\(\\hat{y}_{0}\\).\n\nImportant: what would the points \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) be in KNN?"
  },
  {
    "objectID": "slides/slides-03-knn-regression.html#how-much-does-k-matter",
    "href": "slides/slides-03-knn-regression.html#how-much-does-k-matter",
    "title": "KNN Regression",
    "section": "How much does \\(K\\) matter?",
    "text": "How much does \\(K\\) matter?\n\nIt can matter a lot!\nAs we saw previously, you will get different predicted \\(\\hat{y}_{0}\\) for different choices of \\(K\\) in KNN regression\nDiscuss:\n\nWhat does \\(K=1\\) mean? Do you think \\(K = 1\\) is a good choice?\nGenerally, do you think it’s better to choose a small \\(K\\) or big \\(K\\)?"
  },
  {
    "objectID": "slides/slides-03-knn-regression.html#example-cont.",
    "href": "slides/slides-03-knn-regression.html#example-cont.",
    "title": "KNN Regression",
    "section": "Example (cont.)",
    "text": "Example (cont.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing \\(K= 3\\), predicted \\(\\hat{y}_{0} = \\frac{1}{3}(8.073 + 8.838 + 9.17) = 8.694\\)\n\n\n\n\n\nUsing \\(K= 4\\), predicted \\(\\hat{y}_{0} = \\frac{1}{4}(8.073 + 8.838 + 9.17 + 9.541) = 8.906\\)"
  },
  {
    "objectID": "slides/slides-03-knn-regression.html#mite-data",
    "href": "slides/slides-03-knn-regression.html#mite-data",
    "title": "KNN Regression",
    "section": "Mite data",
    "text": "Mite data\n\nset.seed(1)\nn <- nrow(mite_dat)\ntest_ids <- sample(1:n, 2)\ntrain_dat <- mite_dat[-test_ids,]\ntest_dat <- mite_dat[test_ids,]\ntest_dat\n\n   SubsDens WatrCont Substrate Shrub    Topo abundance\n68    29.24   590.11   Sphagn1  None Blanket         0\n39    64.75   691.79   Sphagn2   Few Blanket        18"
  },
  {
    "objectID": "slides/slides-03-knn-regression.html#mite-data-preparation",
    "href": "slides/slides-03-knn-regression.html#mite-data-preparation",
    "title": "KNN Regression",
    "section": "Mite data: preparation",
    "text": "Mite data: preparation\n\nThe following code divides my data into train and test sets.\n\nMake sure you understand what each line of code is doing! If you don’t, please ask!\n\n\n\nset.seed(6)\nn <- nrow(mite_dat)\ntest_ids <- sample(1:n, 2)\ntrain_dat <- mite_dat[-test_ids,]\ntest_dat <- mite_dat[test_ids,]\ntest_dat\n\n   SubsDens WatrCont Substrate Shrub    Topo abundance\n53    26.83   414.65 Interface  None Blanket        25\n10    32.14   220.73   Sphagn1  Many Hummock         0"
  },
  {
    "objectID": "slides/slides-03-knn-regression.html#mite-data-knn",
    "href": "slides/slides-03-knn-regression.html#mite-data-knn",
    "title": "KNN Regression",
    "section": "Mite data: KNN",
    "text": "Mite data: KNN"
  },
  {
    "objectID": "slides/slides-03-knn-regression.html#scaling",
    "href": "slides/slides-03-knn-regression.html#scaling",
    "title": "KNN Regression",
    "section": "Scaling",
    "text": "Scaling\n\nRMSE: 3.3082389"
  },
  {
    "objectID": "slides/slides-03-knn-regression.html#mite-data-knn-continued",
    "href": "slides/slides-03-knn-regression.html#mite-data-knn-continued",
    "title": "KNN Regression",
    "section": "Mite data: KNN continued",
    "text": "Mite data: KNN continued\n\n\n\nRunning KNN with \\(K = 3\\) and using Euclidean distance, I identify the following neighbor sets for each test point:\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicted abundance \\(\\hat{y}\\) and true abundance \\(y\\) for both test points, for a test RMSE of 9.428.\n\n\n\n\n# A tibble: 2 × 3\n  test_pt y_hat y_true\n    <int> <dbl>  <int>\n1       1  11.7     25\n2       2   0        0\n\n\n\n\n\nDiscuss: it seems like we did poorly for the first test observation. Does its neighbor set “make sense”?"
  }
]