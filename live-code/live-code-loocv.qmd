---
title: "Live code:"
date: "March 9, 2023"
description: "LOOCV"
editor: visual
categories: "Live code"
format:
  html: 
    code-fold: false
    code-line-numbers: true
draft: false
editor_options: 
  chunk_output_type: console
---

### Data

```{r message = F}
library(tidyverse)
library(vegan)
data(mite)
data(mite.env)
mite_dat <- mite.env %>%
  add_column(abundance = mite$LRUG)
```

### LOOCV

```{r}
#| code-line-numbers: true
n <- nrow(mite_dat)
rmses <- rep(NA, n)
for(i in 1:n){
  train_dat <- mite_dat[-i,]
  test_dat <- mite_dat[i,]
  mod <- lm(abundance ~ WatrCont, data = train_dat)
  pred <- predict(mod, newdata = test_dat)
  rmses[i] <- sqrt((test_dat$abundance - pred)^2)
}
loocv_err <- mean(rmses)
loocv_err
```

How does this compare to when I take a usual validation set approach?

```{r}
set.seed(2)
train_ids <- sample(n, 0.7 * n)
train_dat <- mite_dat[train_ids,]
test_dat <- mite_dat[-train_ids,]
mod <- lm(abundance ~ WatrCont , data = train_dat)
pred <- predict(mod, newdata = test_dat)
rmse_val <- sqrt(mean((test_dat$abundance - pred)^2))
rmse_val
```

Also, if you run with different seeds, you will get different estimated RMSEs!

```{r}
#| echo: false
#| eval: false
n <- nrow(mite_dat)
k <- 5
set.seed(7)
# the following creates a list
fold_ids <- split(sample(1:n), ceiling(seq_along(1:n)/(n/k)))
true_y <- mite_dat$abundance
errs <- rep(NA, k)
for(i in 1:k){
  # to access list elements, use double brackets
  test_fold_ids <- fold_ids[[i]]
  mod <- lm(abundance ~ WatrCont, data = mite_dat[-test_fold_ids,])
  pred <- predict(mod, newdata = mite_dat[test_fold_ids,])
  errs[i] <- get_rmse(true_y[test_fold_ids], pred) 
}
kfold_err <- mean(errs)
```
