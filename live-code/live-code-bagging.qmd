---
title: "Live code:"
date: "March 30, 2023"
description: "Bagging trees"
editor: visual
categories: "Live code"
format:
  html: 
    code-fold: false
    code-line-numbers: true
draft: true
---

```{r setup, message = F}
library(tidyverse)
library(vegan)
library(randomForest)
data(mite)
data(mite.env)
mite_dat <- mite.env %>%
  add_column(abundance = mite$LRUG)
```

## Introduction

We will use the `R` package called `randomForest` is fit bagged decision trees and random forests. Go ahead and install this package in your console!

As we will see in the slides, bagged trees and random forests are very similar and can be fit using the same function: `randomForest()`. The difference lies in the specification of the `mtry` argument, as we will see below.

## Bagging

The syntax for bagged regression trees is the same as in regular regression trees: `response ~ predictors`.

In bagged regression trees, the *only* parameter that the modeler needs to choose is the number of bootstrap samples $B$ to obtain (and therefore the number of trees to fit). This is denoted as the `ntree` argument

However, we will need to specify the additional `mtry` argument to specify we want to fit a bagged model rather than a random forest. For bagged trees, we set `mtry` equal to the number of predictors we have.

In the following code, I fit `B = 10` regression trees, and specify `mtry = 5`.

```{r}
set.seed(2)
n <- nrow(mite_dat)
train_ids <- sample(1:n, 0.8*n)
bag_mod <- randomForest(abundance ~ . , data = mite_dat[train_ids,],
                    ntree = 10, 
                    mtry = ncol(mite_dat) - 1)
```

To make predictions for the test set, we will use the familiar `predict()` function:

```{r}
preds <- predict(bag_mod, newdata = mite_dat[-train_ids,])
preds
```

### Out-of-bag error

The nice thing about bootstrapping is that typically \~1/3 of observations are left out in each sample (and therefore, in each one of the `B` trees). So, we don't necessarily need to explicitly specify a test/train split!

In the following code, a fit a bagged model using all of the available observations:

```{r}
set.seed(5)
bag_all <- randomForest(abundance ~ . , data = mite_dat,
                    ntree = 10, 
                    mtry = ncol(mite_dat) - 1)
```

The `randomForest()` function will automatically create a vector of predicted values for the input data based on the out of bag (OOB) samples; i.e. whenever observation $i$ is OOB (not included in the bootstrap sample) for tree $b$, we can treat $i$ as a test observation and obtain a prediction for it. This is accessed through the `predicted` component of the fitted model:

```{r}
bag_all$predicted
```

Do you notice anything strange in these predictions?

### Importance measure

In order to obtain a measure of how "important" each predictor is by accessing the `importance` component. For regression tasks, this corresponds to the total amount that MSE decreases due to splits over a predictor, averaged over `B`:

```{r}
bag_all$importance
```

```{r}
importance(bag_all)
```

We can use the `varImpPlot()` function to visualize the importance:

```{r}
varImpPlot(bag_all)
```
